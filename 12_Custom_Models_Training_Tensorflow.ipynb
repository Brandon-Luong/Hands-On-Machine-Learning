{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.0 64-bit ('venv')",
      "metadata": {
        "interpreter": {
          "hash": "7b58ff03a81b7b220740b362f01ec4719380627cca970f4b363191d14b8c12ac"
        }
      }
    },
    "colab": {
      "name": "12_Custom_Models_Training_Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqFk51RzfBnP"
      },
      "source": [
        "# Chapter 12: Custom Models and Training with Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-1Isky4fFpv"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQmRvcLcfBnZ"
      },
      "source": [
        "## 12.1 A Quick Tour of Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijuZeMOQfBnZ"
      },
      "source": [
        "- Similar to NumPy but with GPU support.\n",
        "- Supports distributed computing.\n",
        "- Includes a just-in-time (JIT) compiler that allows it to optimize computations for speed and memory usage.\n",
        "- Computation graphs can be exported to a portable format.\n",
        "- Implements autodiff and provides some excellent optimizers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZYn4bGZfBna"
      },
      "source": [
        "## 12.2 Using TensorFlow like NumPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoRB4g_3fBnb"
      },
      "source": [
        "**TensorFlow** - API revolves around **tensors**, which flow from operation to operation.\n",
        "\n",
        "**Tensor** - Very similar to NumPy `ndarray`: it is usually a multidimensional array, but can also hold a scalar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQZ7LoZtfBnb"
      },
      "source": [
        "### 12.2.1 Tensors and Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWa7OKGnfBnc"
      },
      "source": [
        "Create a tensor with `tf.constant()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9ZUB7InfWo5",
        "outputId": "60f7a29c-7d8f-4ccb-832a-010a51bee4db"
      },
      "source": [
        "tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
              "array([[1., 2., 3.],\n",
              "       [4., 5., 6.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDJu_j1yfi2y",
        "outputId": "f65f2643-1550-4286-9a28-067787147dfe"
      },
      "source": [
        "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\r\n",
        "t.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxDQz3KbfqqX",
        "outputId": "014cb2ae-a925-4a0e-a94d-1a75f0b991fe"
      },
      "source": [
        "t.dtype"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tf.float32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aafiawrQfvMX",
        "outputId": "137f069a-27ec-400e-a197-ed0085dd3f06"
      },
      "source": [
        "# Indexing similar to NumPy\r\n",
        "t[:, 1:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[2., 3.],\n",
              "       [5., 6.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8bqo_06f5Bv",
        "outputId": "d2d9c3d3-8196-453a-d518-ef27b1b25621"
      },
      "source": [
        "t[..., 1, tf.newaxis] # ... = Access all unspecified elements"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
              "array([[2.],\n",
              "       [5.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HNsBP2xhsxP",
        "outputId": "cde54d41-0781-49c2-819c-4cdcb060676d"
      },
      "source": [
        "t + 10\r\n",
        "\r\n",
        "# Python calls t.__add__(10)\r\n",
        "# Which calls tf.add(t, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
              "array([[11., 12., 13.],\n",
              "       [14., 15., 16.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ-2kb0ghxKX",
        "outputId": "404716a7-a524-42b7-abbc-b75efd5914dc"
      },
      "source": [
        "tf.square(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
              "array([[ 1.,  4.,  9.],\n",
              "       [16., 25., 36.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBaTsnxVhz9G",
        "outputId": "9a7923d3-c3a2-4048-f2ae-e37b93c8c147"
      },
      "source": [
        "t @ tf.transpose(t)\r\n",
        "\r\n",
        "# TensorFlow creates a new tensor object for transpose\r\n",
        "# Cannot do NumPy's t.T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
              "array([[14., 32.],\n",
              "       [32., 77.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVON2Ym1iznJ"
      },
      "source": [
        "> #### Keras' Low-Level API\r\n",
        "\r\n",
        "> Keras API has its own low-level API, located in `keras.backend`. In `tf.keras`, these functions generally just call the corresponding TensorFlow operations. But if you want to write code that will be portable to other Keras implementations, you should use these Keras functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWtXUgDnjYWz"
      },
      "source": [
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wP6d9V1jak3",
        "outputId": "abe63a8b-b5b7-4da0-8440-d83bf9367ac7"
      },
      "source": [
        "K = keras.backend\r\n",
        "K.square(K.transpose(t)) + 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
              "array([[11., 26.],\n",
              "       [14., 35.],\n",
              "       [19., 46.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpDVqkf5fBnc"
      },
      "source": [
        "### 12.2.2 Tensors and NumPy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNYbyTOQj27E"
      },
      "source": [
        "You can create a tensor from a NumPy array, and vice versa. You can even apply TensorFlow operations to NumPy arrays and NumPy operations to tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq517vdOj_Ul",
        "outputId": "1d280b6d-771c-46e1-940c-f88c34c689ee"
      },
      "source": [
        "a = np.array([2., 4., 5.])\r\n",
        "tf.constant(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wbZVSWOkEt9",
        "outputId": "4ffab252-fecc-452e-e2ba-992202e776c3"
      },
      "source": [
        "t.numpy() # or np.array(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 2., 3.],\n",
              "       [4., 5., 6.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkBQivkGkN8_",
        "outputId": "9da9530b-5668-4bbc-d42c-f1a8b28265e6"
      },
      "source": [
        "tf.square(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXe4Nd9hkRr9",
        "outputId": "14f9b7a3-9db5-4c28-e82c-ecc7d46484fb"
      },
      "source": [
        "np.square(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.,  4.,  9.],\n",
              "       [16., 25., 36.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bTXYLupfBnc"
      },
      "source": [
        "### 12.2.3 Type Conversions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgYBRSi_ksNi"
      },
      "source": [
        "Type conversions can significantly hurt performance. To avoid this, TensorFlow does not perform any type conversions automatically; it just raises an exception if you try to execute an operation on tensors with incompatible types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "Yg3qXvqilA3-",
        "outputId": "c404878a-e985-4605-bc69-0d5d4a30d274"
      },
      "source": [
        "tf.constant(2.) + tf.constant(40) # Cannot add float and integer tensors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-1026a7e18cf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    470\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "xp_tY5JdlIlR",
        "outputId": "a0a2d5df-6847-4fdd-ed8e-4ba95b8a465a"
      },
      "source": [
        "tf.constant(2.) + tf.constant(40., dtype=tf.float64) # Cannot add 32-bit float and 64-bit float tensors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-11603e4e2c5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    470\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnBJ8x1ZldgZ",
        "outputId": "32573bda-18f6-4c4c-ee90-006cd68e189e"
      },
      "source": [
        "t2 = tf.constant(40., dtype=tf.float64)\r\n",
        "tf.constant(2.0) + tf.cast(t2, tf.float32) # Use tf.cast() to convert types"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CQTc8ljfBnd"
      },
      "source": [
        "### 12.2.4 Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUx-mS0Ul3Z2"
      },
      "source": [
        "`tf.Tensor` values are immutable: you cannot modify them.\r\n",
        "\r\n",
        "Not helpful as weights in neural networks since they need to be tweaked by backpropagation.\r\n",
        "\r\n",
        "Use `tf.Variable`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-KBPMu8mHFN",
        "outputId": "27ccfc54-9ef8-43dd-af2c-d5567ce08095"
      },
      "source": [
        "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\r\n",
        "v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
              "array([[1., 2., 3.],\n",
              "       [4., 5., 6.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9MTBYyWmYfh"
      },
      "source": [
        "A `tf.Variable` acts much like a `tf.Tensor` but it can also be modified in place using the `assign()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6Jwr9JQmiib",
        "outputId": "7c32c6d7-9475-4f5a-ed2d-b990d7862032"
      },
      "source": [
        "v.assign(2 * v) # Mutates v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
              "array([[ 2.,  4.,  6.],\n",
              "       [ 8., 10., 12.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbJBh4MHmokr",
        "outputId": "c6c565e7-ed99-465d-ab90-ebf228f2dfb7"
      },
      "source": [
        "v[0, 1].assign(42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
              "array([[ 2., 42.,  6.],\n",
              "       [ 8., 10., 12.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v7S-uIim3vn",
        "outputId": "fd72b9e6-00a8-4b06-9755-aa818d18386e"
      },
      "source": [
        "v[:, 2].assign([0., 1.])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
              "array([[ 2., 42.,  0.],\n",
              "       [ 8., 10.,  1.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA9cQjG8m_Yb",
        "outputId": "feef7ee6-6fcd-465b-9ec4-f96386578672"
      },
      "source": [
        "# Assign/update specific indices with specific values\r\n",
        "v.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
              "array([[100.,  42.,   0.],\n",
              "       [  8.,  10., 200.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNeNs9xyfBnd"
      },
      "source": [
        "### 12.2.5 Other Data Structures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrMSqAcQnkkb"
      },
      "source": [
        "**Sparse tensors** (`tf.SparseTensor`): Efficiently represent tensors containing mostly 0s.\r\n",
        "\r\n",
        "**Tensor arrays** (`tf.TensorArray`): Lists of tensors. All tensors contained must have the same shape and data type.\r\n",
        "\r\n",
        "**Ragged tensors** (`tf.RaggedTensor`): Represent static lists of lists of tensors, where every tensor has the same shape and data type.\r\n",
        "\r\n",
        "**String tensors**: Regular tensors of type `tf.string`.\r\n",
        "- These represent byte strings, not Unicode strings.\r\n",
        "- `tf.string` is atomic, meaning that its length does not appear in the tensor's shape.\r\n",
        "- Once you convert it to a Unicode tensor, then the length appears in the shape.\r\n",
        "\r\n",
        "**Sets**: Represented as regular tensors (or sparse tensors).\r\n",
        "- `tf.constant([[1, 2], [3, 4]])` represents 2 sets [1, 2] and [3, 4].\r\n",
        "\r\n",
        "**Queues**: Store tensors across multiple steps, in `tf.queue` package.\r\n",
        "- First In, First Out (FIFO) queues, \"`FIFOQueue`\"\r\n",
        "- Queues that can prioritize some items, \"`PriorityQueue`\"\r\n",
        "- Shuffle the items, \"`RandomShuffleQueue`\"\r\n",
        "- Batch items of different shapes by padding, \"`PaddingFIFOQueue`\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDoC3edvfBne"
      },
      "source": [
        "## 12.3 Customizing Models and Training Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ1RXMoufBne"
      },
      "source": [
        "### 12.3.1 Custom Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVzJDZQmFcsw"
      },
      "source": [
        "Let's imagine implementing the Huber loss.\r\n",
        "\r\n",
        "> Note: Always try to use vectorized implementation for better performance. To benefit from TensorFlow's graph feature, you should only use TensorFlow operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M2Pq-GYHKZa",
        "outputId": "565e61bd-644c-449c-ff11-a3964cded7dd"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\r\n",
        "\r\n",
        "from sklearn.datasets import fetch_california_housing\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "\r\n",
        "housing = fetch_california_housing()\r\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\r\n",
        "    housing.data, housing.target.reshape(-1, 1), random_state=42)\r\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\r\n",
        "    X_train_full, y_train_full, random_state=42)\r\n",
        "\r\n",
        "scaler = StandardScaler()\r\n",
        "X_train_scaled = scaler.fit_transform(X_train)\r\n",
        "X_valid_scaled = scaler.transform(X_valid)\r\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIB1GHXQFodr"
      },
      "source": [
        "def huber_fn(y_true, y_pred):\r\n",
        "    error = y_true - y_pred\r\n",
        "    is_small_error = tf.abs(error) < 1\r\n",
        "    squared_loss = tf.square(error) / 2\r\n",
        "    linear_loss = tf.abs(error) - 0.5\r\n",
        "    return tf.where(is_small_error, squared_loss, linear_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRKX_UNTHa_r"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\r\n",
        "\r\n",
        "input_shape = X_train.shape[1:]\r\n",
        "\r\n",
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\r\n",
        "                       input_shape=input_shape),\r\n",
        "    keras.layers.Dense(1),\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsILyC58GmMc",
        "outputId": "f2239627-57eb-4d3f-8a06-6e0900b1bf9f"
      },
      "source": [
        "model.compile(loss=huber_fn, optimizer=\"nadam\")\r\n",
        "# From textbook notebook\r\n",
        "model.fit(X_train_scaled, y_train, epochs=2,\r\n",
        "          validation_data=(X_valid_scaled, y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "363/363 [==============================] - 2s 2ms/step - loss: 1.1191 - val_loss: 0.2455\n",
            "Epoch 2/2\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.2165 - val_loss: 0.2034\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2a3a0e01d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VErFHsQjfBne"
      },
      "source": [
        "### 12.3.2 Saving and Loading Models That Contain Custom Components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBPuS06iH_JX"
      },
      "source": [
        "When you load a model containing custom objects, you need to map the names to the objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBgvUTxPIGuS"
      },
      "source": [
        "# From textbook notebook\r\n",
        "model.save(\"my_model_with_a_custom_loss.h5\")\r\n",
        "\r\n",
        "model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\r\n",
        "                                custom_objects={\"huber_fn\": huber_fn})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aLHSRTeIfLb"
      },
      "source": [
        "# Function that creates a configured loss function\r\n",
        "def create_huber(threshold=1.0):\r\n",
        "    def huber_fn(y_true, y_pred):\r\n",
        "        error = y_true - y_pred\r\n",
        "        is_small_error = tf.abs(error) < threshold\r\n",
        "        squared_loss = tf.square(error) / 2\r\n",
        "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\r\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\r\n",
        "    return huber_fn\r\n",
        "\r\n",
        "model.compile(loss=create_huber(2.0), optimizer=\"nadam\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1OAKgTsJQ_Z"
      },
      "source": [
        "When you save the model, the `threshold` will not be saved. This means that you will have to specify the `threshold` value when loading the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDvphvL9JdGP"
      },
      "source": [
        "# From textbook notebook\r\n",
        "model.save(\"my_model_with_a_custom_loss_threshold_2.h5\")\r\n",
        "\r\n",
        "model = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2.h5\",\r\n",
        "                                custom_objects={\"huber_fn\": create_huber(2.0)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgyhQ8VJJ966"
      },
      "source": [
        "By creating a subclass of `keras.losses.Loss` and implementing its `get_config()` method, you can solve this problem of having to specify the `threshold` value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctac1ckqKR3L"
      },
      "source": [
        "class HuberLoss(keras.losses.Loss):\r\n",
        "    def __init__(self, threshold=1.0, **kwargs):\r\n",
        "        self.threshold = threshold\r\n",
        "        super().__init__(**kwargs)\r\n",
        "    def call(self, y_true, y_pred):\r\n",
        "        error = y_true - y_pred\r\n",
        "        is_small_error = tf.abs(error) < self.threshold\r\n",
        "        squared_loss = tf.square(error) / 2\r\n",
        "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2\r\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)\r\n",
        "    def get_config(self):\r\n",
        "        base_config = super().get_config()\r\n",
        "        return {**base_config, \"threshold\": self.threshold}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbrJNSTsMHZF"
      },
      "source": [
        "Code explanation:\r\n",
        "\r\n",
        "1. Constructor (`__init__`) accepts `**kwargs` and passes them to the parent constructor (`super().__init__`), which handles standard hyperparameters.\r\n",
        "    - Note: `**kwargs` stands for unpacking (`**`) the keyword arguments dictionary (`kwargs`).\r\n",
        "\r\n",
        "2. The `call()` method takes the labels and predictions, computes all the instance losses, and returns them.\r\n",
        "    - Exact same as `huber_fn` from above.\r\n",
        "\r\n",
        "3. The `get_config()` method returns a dictionary mapping each hyperparameter name to its value.\r\n",
        "    - First calls the parent class's `get_config()` method (`super().get_config()`).\r\n",
        "    - Then adds the new hyperparameters to this dictionary.\r\n",
        "    - Note: `**base_config` unpacks the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnzW6x6xPzw1"
      },
      "source": [
        "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7EeeXNFP-ob"
      },
      "source": [
        "# From textbook notebook\r\n",
        "model.save(\"my_model_with_a_custom_loss_class.h5\")\r\n",
        "\r\n",
        "model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\",\r\n",
        "                                custom_objects={\"HuberLoss\": HuberLoss})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2bDNkm8fBnf"
      },
      "source": [
        "### 12.3.3 Custom Activation Functions, Initializers, Regularizers, and Constraints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdSs7Cnoj4tH"
      },
      "source": [
        "# Custom Keras functions\r\n",
        "\r\n",
        "def my_softplus(z): # return value is just tf.nn.softplus(z)\r\n",
        "    return tf.math.log(tf.exp(z) + 1.0)\r\n",
        "\r\n",
        "def my_glorot_initializer(shape, dtype=tf.float32):\r\n",
        "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\r\n",
        "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\r\n",
        "\r\n",
        "def my_l1_regularizer(weights):\r\n",
        "    return tf.reduce_sum(tf.abs(0.01 * weights))\r\n",
        "\r\n",
        "def my_positive_weights(weights): # return value is just tf.nn.relu(weights)\r\n",
        "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZdFOKoPk7-C"
      },
      "source": [
        "# Custom functions can then be used normally\r\n",
        "layer = keras.layers.Dense(30, activation=my_softplus,\r\n",
        "                           kernel_initializer=my_glorot_initializer,\r\n",
        "                           kernel_regularizer=my_l1_regularizer,\r\n",
        "                           kernel_constraint=my_positive_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuUnS-nMlhf9"
      },
      "source": [
        "If a function has hyperparameters that need to be saved along with the model, then you will want to subclass the appropriate class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy0LpoIHmWlQ"
      },
      "source": [
        "# l1 regularization that saves its factor hyperparameter\r\n",
        "# No calling parent constructor, super.__init__()\r\n",
        "# Not defined in parent class\r\n",
        "class MyL1Regularizer(keras.regularizers.Regularizer):\r\n",
        "    def __init__(self, factor):\r\n",
        "        self.factor = factor\r\n",
        "    def __call__(self, weights):\r\n",
        "        return tf.reduce_sum(tf.abs(self.factor * weights))\r\n",
        "    def get_config(self):\r\n",
        "        return {\"factor\": self.factor}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R2DdGgBo01M"
      },
      "source": [
        "> Note: You must implement the `call()` method for losses, layers, activation functions, and models, or `__call__()` for regularizers, initializers, and constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq9-ad3pfBnf"
      },
      "source": [
        "### 12.3.4 Custom Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCl-w0gHqHrD"
      },
      "source": [
        "Losses and metrics are conceptually not the same thing.\r\n",
        "\r\n",
        "Losses (eg. cross entropy) are:\r\n",
        "- Used by Gradient Descent to *train* a model.\r\n",
        "- They must be differentiable (at least where they are evaluated).\r\n",
        "- Their gradients should not be 0 everywhere.\r\n",
        "- Okay if not easily interpretable by humans.\r\n",
        "\r\n",
        "Metrics (eg. accuracy) are:\r\n",
        "- Used to *evaluate* a model.\r\n",
        "- Can be non-differentiable.\r\n",
        "- Can have 0 gradients everywhere.\r\n",
        "- Must be more easily interpretable.\r\n",
        "\r\n",
        "Defining a custom metric function is exactly the same as defining a custom loss function. \r\n",
        "\r\n",
        "We can use the Huber loss function as a metric (though MAE or MSE is preferred)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpq0fK6yrtAP"
      },
      "source": [
        "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp0wR6g0skBH"
      },
      "source": [
        "> Recall: In Chapter 3, precision is the number of true positives divided by the number of positive predictions (true positives + false positives).\r\n",
        "\r\n",
        "For each batch during training, Keras will compute this metric and keep track of its mean since the beginning of the epoch. This can be incorrect depending on the batch vs. overall.\r\n",
        "\r\n",
        "`keras.metrics.Precision` class can keep track of the number of true positives and false positives and can compute their ratio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU_NghJ7tW-q",
        "outputId": "a206789d-d644-479a-8afd-d9faa8fcab7f"
      },
      "source": [
        "precision = keras.metrics.Precision() # Create Precision object\r\n",
        "# Pass labels and predictions of 1st batch\r\n",
        "# 5 positive predictions, 4 correct\r\n",
        "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCJm4Nactmzc",
        "outputId": "5fe71ffa-3122-4022-8020-effa11e4bb3e"
      },
      "source": [
        "# Pass labels and predictions of 2nd batch\r\n",
        "# 3 positive predictions, 0 correct\r\n",
        "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BS7u4MfucD2"
      },
      "source": [
        "**Streaming metric (stateful metric)**: A metric that is gradually updated, batch after batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dgl0nN_up3e",
        "outputId": "b27e4394-6180-48d7-d063-37b9aa6668b4"
      },
      "source": [
        "# Get the current value of the metric.\r\n",
        "precision.result()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlFQlechuutb",
        "outputId": "dede754c-7daa-4fdb-9a0d-a2e3814d7b1a"
      },
      "source": [
        "# Look at its variables (number of true/false positives)\r\n",
        "precision.variables"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
              " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4MVfl-Xu6Ra"
      },
      "source": [
        "# Reset these variables\r\n",
        "precision.reset_states() # both variables get reset to 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U206FfRvGMH"
      },
      "source": [
        "If you need to create such a streaming metric, create a subclass of `keras.metrics.Metric` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suvdWv3SvQbb"
      },
      "source": [
        "# Keeps track of total Huber loss\r\n",
        "# Keeps track of number of instances seen so far\r\n",
        "# When asked for result, returns the ratio, which is the mean Huber loss\r\n",
        "\r\n",
        "class HuberMetric(keras.metrics.Metric):\r\n",
        "    def __init__(self, threshold=1.0, **kwargs):\r\n",
        "        super().__init__(**kwargs) # handles base args (eg. dtype)\r\n",
        "        self.threshold = threshold\r\n",
        "        self.huber_fn = create_huber(threshold)\r\n",
        "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\r\n",
        "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\r\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\r\n",
        "        metric = self.huber_fn(y_true, y_pred)\r\n",
        "        self.total.assign_add(tf.reduce_sum(metric))\r\n",
        "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\r\n",
        "    def result(self):\r\n",
        "        return self.total / self.count\r\n",
        "    def get_config(self):\r\n",
        "        base_config = super().get_config()\r\n",
        "        return {**base_config, \"treshold\": self.threshold}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ2J8l3EwkxO"
      },
      "source": [
        "Code explanation:\r\n",
        "\r\n",
        "1. The constructor uses the `add_weight()` method to create the variables needed to keep track of the metric's state over multiple batches (sum of all Huber losses, `total`, and number of instances seen so far, `count`).\r\n",
        "    - Alternatively, can create variables manually since Keras tracks any `tf.Variable` that is set as an attribute.\r\n",
        "\r\n",
        "2. The `updated_state()` method is called when you use an instance of this class as a function. It updates the variables, given the labels and predictions for 1 batch.\r\n",
        "\r\n",
        "3. The `result()` method computes and returns the final result, in this case the mean Huber metric over all instances.\r\n",
        "    - When you use the metric as a function, the `update_state()` method gets called first.\r\n",
        "    - Then the `result()` method is called, and its output is returned.\r\n",
        "\r\n",
        "4. The `get_config()` method ensures the `threshold` gets saved along with the model.\r\n",
        "\r\n",
        "> **Not in code**: The `reset_states()` method resets all variables to 0.0 and can be overridden if needed.\r\n",
        "\r\n",
        "In general, Keras calls the simple function metric (not custom) for each batch and keeps track of the mean during each epoch. But some metric, like precision, cannot be averaged over batches and so must implement a custom streaming metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PW26BylfBnf"
      },
      "source": [
        "### 12.3.5 Custom Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxYInbEwFRy6"
      },
      "source": [
        "Custom layers are useful if you want to build an exotic layer with no default TensorFlow implementation or treat blocks of layers as a single layer.\r\n",
        "\r\n",
        "If you want to create a custom layer without any weights, the simplest option is to write a function and wrap it in a `keras.layers.Lambda` layer.\r\n",
        "\r\n",
        "This custom layer can then be used like any other layer, using the Sequential, Function, or Subclassing API. It can also be used as an activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wlp5CpjzFfEu"
      },
      "source": [
        "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYhcFz1XGW3O"
      },
      "source": [
        "To build a custom stateful layer (ie. a layer with weights), you need to create a subclass of the `keras.layers.Layer` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz1jePxqGkrW"
      },
      "source": [
        "# Simplified version of the Dense layer\r\n",
        "class MyDense(keras.layers.Layer):\r\n",
        "    def __init__(self, units, activation=None, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "        self.units = units\r\n",
        "        self.activation = keras.activations.get(activation)\r\n",
        "    \r\n",
        "    def build(self, batch_input_shape):\r\n",
        "        self.kernel = self.add_weight(\r\n",
        "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\r\n",
        "            initializer=\"glorot_normal\")\r\n",
        "        self.bias = self.add_weight(\r\n",
        "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\r\n",
        "        super().build(batch_input_shape) # must be at the end\r\n",
        "    \r\n",
        "    def call(self, X):\r\n",
        "        return self.activation(X @ self.kernel + self.bias)\r\n",
        "    \r\n",
        "    def compute_output_shape(self, batch_input_shape):\r\n",
        "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\r\n",
        "    \r\n",
        "    def get_config(self):\r\n",
        "        base_config = super().get_config()\r\n",
        "        return {**base_config, \"units\": self.units,\r\n",
        "                \"activation\": keras.activations.serialize(self.activation)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxoNvcBFH77O"
      },
      "source": [
        "Code explanation:\r\n",
        "\r\n",
        "1. The constructor takes all the hyperparameters as arguments (eg. `units` and `activation`), and `**kwargs` argument.\r\n",
        "    - It calls the parent constructor and passes unpacked `kwargs` (`super().__init__(**kwargs)`), which takes care of standard arguments.\r\n",
        "    - Saves hyperparameters as attributes.\r\n",
        "    - Converts `activation` argument to appropriate activation function (`keras.activations.get()`).\r\n",
        "\r\n",
        "2. The `build()` method creates the layer's variables by calling the `add_weight()` method for each weight.\r\n",
        "    - Pass the shape of this layer's inputs to `build()`, which is necessary to create some of the weights.\r\n",
        "    - We need to know the number of neurons in the previous layer in order to create the connection weights matrix.\r\n",
        "    - `\"kernel\"` corresponds to the size of the last dimension of the inputs.\r\n",
        "    - Only at the end, call parent's `build` method (`super().build()`) to tell Keras that the layer is built (ie. sets `self.built=True`).\r\n",
        "\r\n",
        "3. The `call()` method performs the desired operations.\r\n",
        "    - Compute the matrix multiplication of inputs `X` and layer's kernel.\r\n",
        "    - Add the bias vector.\r\n",
        "    - Apply activation function to result.\r\n",
        "    - Gives output of the layer.\r\n",
        "\r\n",
        "4. The `compute_output_shape()` method returns the shape of this layer's outputs.\r\n",
        "    - Same shape as inputs, except last dimension is replaced with the number of neurons in the layer.\r\n",
        "    - In `tf.keras`, shapes are instances of `tf.TensorShape` class can can be converted to Python lists using `as_list()`.\r\n",
        "\r\n",
        "5. The `get_config()` method saves the hyperparameter values.\r\n",
        "    - The activation function's full configuration is saved by calling `keras.activations.serialize()`.\r\n",
        "\r\n",
        "> Note: You can generally omit `compute_output_shape()` method, as tf.keras automatically infers the output shape, except when the layer is dynamic.\r\n",
        "\r\n",
        "To create a layer with multiple inputs (eg. `Concatenate`):\r\n",
        "1. The argument to `call()` method should be a tuple containing all the inputs.\r\n",
        "2. The argument to `compute_output_shape()` method should be a tuple containing each input's batch shape.\r\n",
        "\r\n",
        "To create a layer with multiple outputs:\r\n",
        "1. The `call()` method should return the list of outputs.\r\n",
        "2. The `compute_output_shape()` method should return the list of batch output shapes (1 per output)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfIafo3VN-0B"
      },
      "source": [
        "# Takes 2 inputs, returns 3 outputs\r\n",
        "class MyMultiLayer(keras.layers.Layer):\r\n",
        "    def call(self, X):\r\n",
        "        X1, X2 = X\r\n",
        "        return [X1 + X2, X1 * X2, X1 / X2]\r\n",
        "    \r\n",
        "    def compute_output_shape(self, batch_input_shape):\r\n",
        "        b1, b2 = batch_input_shape\r\n",
        "        return [b1, b1, b1] # should probably handle broadcasting rules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l56oP4lOlzS"
      },
      "source": [
        "This layer can now be used like any other layer - only using Functional and Subclassing API, as Sequential only accepts 1 input and output.\r\n",
        "\r\n",
        "If your layer needs to have a different behavior during training and during testing (eg. uses `Dropout` or `BatchNormalization` layers), then you must add a `training` argument to the `call()` method and use this argument to decide what to do."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qry25RD8PJpL"
      },
      "source": [
        "# keras.layers.GaussianNoise does the same thing\r\n",
        "\r\n",
        "# Adds Gaussian noise during training (for regularization)\r\n",
        "# Does nothing during testing\r\n",
        "class MyGaussianNoise(keras.layers.Layer):\r\n",
        "    def __init__(self, stddev, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "        self.stddev = stddev\r\n",
        "    \r\n",
        "    def call(self, X, training=None):\r\n",
        "        if training:\r\n",
        "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\r\n",
        "            return X + noise\r\n",
        "        else:\r\n",
        "            return X\r\n",
        "        \r\n",
        "    def compute_output_shape(self, batch_input_shape):\r\n",
        "        return batch_input_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALxvTop3fBng"
      },
      "source": [
        "### 12.3.6 Custom Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFEiNFi74ICU"
      },
      "source": [
        "To create custom models: subclass the `keras.Model` class, create layers and variables in the constructor, and implement the `call()` method to do whatever you want the model to do.\r\n",
        "\r\n",
        "Suppose we want to build a custom model similar to *Figure 12-3*:\r\n",
        "1. Input layer goes through 1st dense layer\r\n",
        "2. Then through a **residual block**, which is composed of:\r\n",
        "    - 2 dense layers\r\n",
        "    - An addition operation\r\n",
        "    - Concatenating the inputs to the output using the (+) operation\r\n",
        "3. Through the residual block 3 more times\r\n",
        "4. Into another residual block\r\n",
        "5. Finally a dense output layer\r\n",
        "\r\n",
        "To create this model, first create a `ResidualBlock` layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy6sEYSJ4SNU"
      },
      "source": [
        "# Create ResidualBlock layer\r\n",
        "class ResidualBlock(keras.layers.Layer):\r\n",
        "    def __init__(self, n_layers, n_neurons, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\r\n",
        "                                          kernel_initializer=\"he_normal\")\r\n",
        "                       for _ in range(n_layers)]\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        Z = inputs\r\n",
        "        for layer in self.hidden:\r\n",
        "            Z = layer(Z)\r\n",
        "        return inputs + Z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC9PTJxu6g0w"
      },
      "source": [
        "Keras automatically detects that the `hidden` attribute contains trackable objects (layers in this case), so their variables are automatically added to this layer's list of variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6A2-9tW7XKV"
      },
      "source": [
        "# Create ResidualRegressor model\r\n",
        "class ResidualRegressor(keras.Model):\r\n",
        "    def __init__(self, output_dim, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\r\n",
        "                                          kernel_initializer=\"he_normal\")\r\n",
        "        self.block1 = ResidualBlock(2, 30)\r\n",
        "        self.block2 = ResidualBlock(2, 30)\r\n",
        "        self.out = keras.layers.Dense(output_dim)\r\n",
        "    \r\n",
        "    def call(self, inputs):\r\n",
        "        Z = self.hidden1(inputs)\r\n",
        "        for _ in range(1 + 3):\r\n",
        "            Z = self.block1(Z)\r\n",
        "        Z = self.block2(Z)\r\n",
        "        return self.out(Z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ls13xvz18PvR"
      },
      "source": [
        "We create the layers in the constructor and then use them in the `call()` method.\r\n",
        "\r\n",
        "To save the model and load it, you must implement the `get_config()` method in both the `ResidualBlock` class and the `ResidualRegressor` class. Alternatively, save and load the weights using `save_weights()` and `load_weights()` methods.\r\n",
        "\r\n",
        "`Layer` class (superclass)  \r\n",
        "$\\downarrow$  \r\n",
        "`Model` class (subclass of `Layer`)\r\n",
        "\r\n",
        "> Best practices: Subclass `Layer` class for internal components of your model (ie. layers or reusable blocks of layers). Subclass `Model` class for the model itself (ie. the object you will train)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOWKydGtfBng"
      },
      "source": [
        "### 12.3.7 Losses and Metrics Based on Model Internals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKzCWY7U99Bu"
      },
      "source": [
        "So far custom losses and metrics were based on the labels and predictions.\r\n",
        "\r\n",
        "But sometimes you want them based on other parts of the model, such as weights or activations of its hidden layers - useful for regularization or to monitor some internal aspect of the model.\r\n",
        "\r\n",
        "To define a custom loss based on model internals, compute it based on any part of the model you want, then pass the result to the `add_loss()` method.\r\n",
        "\r\n",
        "Suppose we want to build a custom regression MLP model composed of 5 hidden layers, and 1 output layer. It will have an auxiliary output on top of the upper hidden layer, with an associated loss called the **reconstruction loss**: the mean squared difference between the reconstruction and the inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XcG0EDb_IjU"
      },
      "source": [
        "class ReconstructingRegressor(keras.Model):\r\n",
        "    def __init__(self, output_dim, **kwargs):\r\n",
        "        super().__init__(**kwargs)\r\n",
        "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\r\n",
        "                                          kernel_initializer=\"lecun_normal\")\r\n",
        "                       for _ in range(5)]\r\n",
        "        self.out = keras.layers.Dense(output_dim)\r\n",
        "    \r\n",
        "    def build(self, batch_input_shape):\r\n",
        "        n_inputs = batch_input_shape[-1]\r\n",
        "        self.reconstruct = keras.layers.Dense(n_inputs)\r\n",
        "        super().build(batch_input_shape)\r\n",
        "    \r\n",
        "    def call(self, inputs):\r\n",
        "        Z = inputs\r\n",
        "        for layer in self.hidden:\r\n",
        "            Z = layer(Z)\r\n",
        "        reconstruction = self.reconstruct(Z)\r\n",
        "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\r\n",
        "        self.add_loss(0.05 * recon_loss)\r\n",
        "        return self.out(Z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iHYN2C2ALJJ"
      },
      "source": [
        "Code explanation:\r\n",
        "\r\n",
        "1. The constructor creates the DNN with 5 dense hidden layers and 1 dense output layer.\r\n",
        "\r\n",
        "2. The `build()` method creates an extra dense layer which will be used to reconstruct the inputs of the model.\r\n",
        "    - Must be created here because its number of units must be equal to the number of inputs and is unknown before `build()` is called.\r\n",
        "\r\n",
        "3. The `call()` method:\r\n",
        "    - Processes the inputs through all 5 hidden layers.\r\n",
        "    - Passes the results through reconstruction layer, producing the reconstruction.\r\n",
        "    - Computes the reconstruction loss.\r\n",
        "    - Adds loss to model's list of losses using `add_loss()` method.\r\n",
        "        - Scale down the reconstruction loss by multiplying by 0.05 (a hyperparameter you can tune).\r\n",
        "        - Ensures that the reconstruction loss does not dominate the main loss.\r\n",
        "    - Finally passes the output of the hidden layers to the output layer and returns its output.\r\n",
        "\r\n",
        "Similarly, you can add a custom metric based on model internals as long as the result is the output of a metric object. For example, create a `keras.metrics.Mean` object in the constructor, call it in `call()` method, passing the `recon_loss` and add it to the model by calling `add_metric()` method. This will display both the mean loss and the mean reconstruction error over each epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZTMV0_NfBng"
      },
      "source": [
        "### 12.3.8 Computing Gradients Using Autodiff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj-wggUavQav"
      },
      "source": [
        "def f(w1, w2):\r\n",
        "    return 3*w1**2 + 2*w1*w2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_dZ0arVv121"
      },
      "source": [
        "$\\frac{\\partial f}{\\partial w1} = 6*w1 + 2*w2$ \r\n",
        "\r\n",
        "$\\frac{\\partial f}{\\partial w2} = 2*w1$.\r\n",
        "\r\n",
        "So at $(w1, w2) = (5,3)$, the gradient vector is $(\\frac{\\partial f}{\\partial w1}, \\frac{\\partial f}{\\partial w2}) = (36, 10)$.\r\n",
        "\r\n",
        "For a neural network, the function would be much more complex and finding the partials by hand is almost an impossible task.\r\n",
        "\r\n",
        "One solution could be to compute an approximation of each partial derivative by measuring how much the function's output changes when the corresponding parameter is tweaked."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shAk8OvXxJ_R",
        "outputId": "7b4e00ff-9b32-4893-a49f-55dfa9bce599"
      },
      "source": [
        "w1, w2 = 5, 3\r\n",
        "eps = 1e-6\r\n",
        "(f(w1 + eps, w2) - f(w1, w2)) / eps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36.000003007075065"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRi2MVKnxYgO",
        "outputId": "48c3d0e4-64d3-4473-8048-4d56fb71952c"
      },
      "source": [
        "(f(w1, w2 + eps) - f(w1, w2)) / eps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.000000003174137"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQS_cRQTxk8G"
      },
      "source": [
        "Works well but needing to call `f()` at least once per parameter is not suitable for large neural networks. Instead use autodiff."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C3YPn8Wx7g-",
        "outputId": "42c41076-9b71-4d71-943e-bb78fe42ad8b"
      },
      "source": [
        "w1, w2 = tf.Variable(5.), tf.Variable(3.)\r\n",
        "with tf.GradientTape() as tape: # Records every operation that involves a variable\r\n",
        "    z = f(w1, w2)\r\n",
        "\r\n",
        "gradients = tape.gradient(z, [w1, w2])\r\n",
        "gradients"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "T15QELiDysiA",
        "outputId": "7c8495d9-daaa-44a5-b3b4-d5cf277636a8"
      },
      "source": [
        "# tape erased after calling its gradient() method\r\n",
        "# Exception if gradient() called twice\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "    z = f(w1, w2)\r\n",
        "\r\n",
        "dz_dw1 = tape.gradient(z, w1) # => tensor 36.0\r\n",
        "dz_dw2 = tape.gradient(z, w2) # RuntimeError!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-d08852a12f55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdz_dw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# => tensor 36.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdz_dw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# RuntimeError!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1025\u001b[0m     \"\"\"\n\u001b[1;32m   1026\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m       raise RuntimeError(\"A non-persistent GradientTape can only be used to\"\n\u001b[0m\u001b[1;32m   1028\u001b[0m                          \"compute one set of gradients (or jacobians)\")\n\u001b[1;32m   1029\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: A non-persistent GradientTape can only be used tocompute one set of gradients (or jacobians)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSAEM4gBzYB4"
      },
      "source": [
        "# To call gradient() more than once\r\n",
        "with tf.GradientTape(persistent=True) as tape:\r\n",
        "    z = f(w1, w2)\r\n",
        "\r\n",
        "dz_dw1 = tape.gradient(z, w1) # => tensor 36.0\r\n",
        "dz_dw2 = tape.gradient(z, w2) # => tensore 10.0, works fine now!\r\n",
        "del tape # Delete when finished to free resources"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oo1Wlp6O0E8o",
        "outputId": "b87398d4-87f6-4df1-f582-b81443beb648"
      },
      "source": [
        "# tape only tracks variable operations\r\n",
        "# Result is None, otherwise\r\n",
        "c1, c2 = tf.constant(5.), tf.constant(3.)\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "    z = f(c1, c2)\r\n",
        "\r\n",
        "gradients = tape.gradient(z, [c1, c2]) # returns [None, None]\r\n",
        "gradients"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3HUsNkO0imi",
        "outputId": "6f93a68c-b8c8-43fc-ad9d-e7bd04616970"
      },
      "source": [
        "# Force tape to watch any tensor and track their operations\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "    tape.watch(c1)\r\n",
        "    tape.watch(c2)\r\n",
        "    z = f(c1, c2)\r\n",
        "\r\n",
        "gradients = tape.gradient(z, [c1, c2]) # returns [tensor 36., tensor 10.]\r\n",
        "gradients"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km4NN5ca1YQz"
      },
      "source": [
        "To compute the gradients of a vector containing multiple losses, TensorFlow will compute the gradients of the vector's sum.\r\n",
        "\r\n",
        "To get the individual gradients (eg. the gradients of each loss with regard to the model parameters), call the tape's `jacobian()` method: it will perform reverse-mode autodiff once for each loss in the vector.\r\n",
        "\r\n",
        "In some cases use `tf.stop_gradient()` function to stop gradients from backpropagating through some part of the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WePKgDZt2Ku2",
        "outputId": "2e1bbd32-5bc9-4859-e001-daa60f0b569f"
      },
      "source": [
        "def f(w1, w2):\r\n",
        "    return 3*w1**2 + tf.stop_gradient(2*w1*w2)\r\n",
        "\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "    z = f(w1, w2) # same result as without stop_gradient()\r\n",
        "\r\n",
        "gradients = tape.gradient(z, [w1, w2]) # => returns [tensor 30., None]\r\n",
        "gradients"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaLlAoML2qkf",
        "outputId": "275bc4a9-55e7-4c08-9f25-e789c6cd8554"
      },
      "source": [
        "# Numerical issues when computing gradients for large inputs\r\n",
        "x = tf.Variable([100.])\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "    z = my_softplus(x)\r\n",
        "\r\n",
        "tape.gradient(z, [x]) # result is NaN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI_rYnIm3SQb"
      },
      "source": [
        "Computing gradients using `my_softplus()` function leads to numerical difficulties because due to floating-point precision errors, autodiff ends up computing infinity divided by infinity (returning NaN).\r\n",
        "\r\n",
        "The derivative of softplus function is $ 1 / (1 + 1 / \\text{exp}(x)) $. \r\n",
        "\r\n",
        "So decorate with `@tf.custom_gradient` to tell TensorFlow to use this stable function when computing the gradients of `my_softplus()` function and making it return both its normal output and the function that computes the derivatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV-IX-dW4e3V"
      },
      "source": [
        "@tf.custom_gradient\r\n",
        "def my_better_softplus(z):\r\n",
        "    exp = tf.exp(z)\r\n",
        "    def my_softplus_gradients(grad):\r\n",
        "        return grad / (1 + 1 / exp)\r\n",
        "    return tf.math.log(exp + 1), my_softplus_gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE2ThU-8fBng"
      },
      "source": [
        "### 12.3.9 Custom Training Loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3ooGP4fMO3K"
      },
      "source": [
        "Since `fit()` method only uses one optimizer (the one used when the model is compiled), `fit()` may not be flexible enough in some cases (eg. 1 optimizer for wide path, 1 optimizer for deep path) and would require writing a custom loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC0-IDJ4Mr3q"
      },
      "source": [
        "# Build a simple model\r\n",
        "l2_reg = keras.regularizers.l2(0.05)\r\n",
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\r\n",
        "                       kernel_regularizer=l2_reg)\r\n",
        "])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ixGGmDyNAz1"
      },
      "source": [
        "def random_batch(X, y, batch_size=32):\r\n",
        "    \"\"\"Randomly sample a batch of instances from the training set.\"\"\"\r\n",
        "    idx = np.random.randint(len(X), size=batch_size)\r\n",
        "    return X[idx], y[idx]\r\n",
        "\r\n",
        "def print_status_bar(iteration, total, loss, metrics=None):\r\n",
        "    \"\"\"Displays the training status, including the number of steps,\r\n",
        "    the total number of steps, the mean loss since the start of the epoch,\r\n",
        "    and other metrics.\"\"\"\r\n",
        "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\r\n",
        "                         for m in [loss] + (metrics or [])])\r\n",
        "    end = \"\" if iteration < total else \"\\n\"\r\n",
        "    print(\"\\r{}/{} - \".format(iteration, total) + metrics,\r\n",
        "          end=end)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn1WZHyyOZny"
      },
      "source": [
        "> Code Notes:\r\n",
        "\r\n",
        "> - `{:.4f}` format a float with 4 digits after the decimal point.\r\n",
        "\r\n",
        "> - $\\backslash$ r (carriage return) along with `end=\"\"` ensures stats bar gets printed on the same line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiDkuhEwP2jq"
      },
      "source": [
        "n_epochs = 5\r\n",
        "batch_size = 32\r\n",
        "n_steps = len(X_train) // batch_size # X_train is on housing set\r\n",
        "optimizer = keras.optimizers.Nadam(lr=0.01)\r\n",
        "loss_fn = keras.losses.mean_squared_error\r\n",
        "mean_loss = keras.metrics.Mean()\r\n",
        "metrics = [keras.metrics.MeanAbsoluteError()]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSkN01WvRfce",
        "outputId": "2b586115-9258-4c56-9f60-9519f0ef1eb9"
      },
      "source": [
        "# Build custom loop\r\n",
        "for epoch in range(1, n_epochs + 1):\r\n",
        "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\r\n",
        "    for step in range(1, n_steps + 1):\r\n",
        "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            y_pred = model(X_batch, training=True)\r\n",
        "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\r\n",
        "            loss = tf.add_n([main_loss] + model.losses)\r\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\r\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
        "        mean_loss(loss)\r\n",
        "        for metric in metrics:\r\n",
        "            metric(y_batch, y_pred)\r\n",
        "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\r\n",
        "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\r\n",
        "    for metric in [mean_loss] + metrics:\r\n",
        "        metric.reset_states()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "11610/11610 - mean: 2.3334 - mean_absolute_error: 1.0091\n",
            "Epoch 2/5\n",
            "11610/11610 - mean: 1.0631 - mean_absolute_error: 0.7386\n",
            "Epoch 3/5\n",
            "11610/11610 - mean: 1.0606 - mean_absolute_error: 0.7398\n",
            "Epoch 4/5\n",
            "11610/11610 - mean: 1.0299 - mean_absolute_error: 0.7259\n",
            "Epoch 5/5\n",
            "11610/11610 - mean: 1.0689 - mean_absolute_error: 0.7429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpDltxuAS6W6"
      },
      "source": [
        "Code explanation:\r\n",
        "\r\n",
        "1. (Lines 2 & 4): 2 nested loops for the epochs and the batches within an epoch.\r\n",
        "\r\n",
        "2. (Line 5): Sample a random batch from the training set.\r\n",
        "\r\n",
        "3. (Lines 6-9): Inside `tf.GradientTape()` block,\r\n",
        "    - (Line 7): Make a prediction for one batch\r\n",
        "    - (Line 8): Calculate the main loss. Since `loss_fn` contains `mean_squared_error()` which returns one loss per instance, compute the mean over the batch using `tf.reduce_mean()`.\r\n",
        "    - (Line 9): Total loss is main loss + other losses (eg. regularization loss). Regularization is already reduced to a single scalar, so use `tf.add_n()`, which sums multiple tensors of the same shape and data type.\r\n",
        "\r\n",
        "4. (Line 10): Ask tape to compute the gradient of the loss with regard to each **trainable** variable (not all variables!).\r\n",
        "\r\n",
        "5. (Line 11): Apply to optimizer to perform a Gradient Descent step.\r\n",
        "\r\n",
        "6. (Line 12): Update mean loss.\r\n",
        "\r\n",
        "7. (Lines 13-14): Update metrics over the current epoch.\r\n",
        "\r\n",
        "8. (Line 15): Display status bar.\r\n",
        "\r\n",
        "9. (Line 16): At end of each epoch, display the status bar again to make it look complete and print a line feed.\r\n",
        "\r\n",
        "10. (Lines 17-18): Reset the states of the mean loss and the metrics.\r\n",
        "\r\n",
        "If you add weight constraints to your model (eg. by setting `kernel_constraint` or `bias_constraint` when creating a layer), you should update the training loop to apply these constraints just after `apply_gradients()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsO2mY09XMKD"
      },
      "source": [
        "for variable in model.variables:\r\n",
        "    if variable.constraint is not None:\r\n",
        "        variable.assign(variable.constraint(variable))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlWidwJSXbRu"
      },
      "source": [
        "> Note: This training loop does not handle layers that behave differently during training and testing (eg. `BatchNormalization` or `Dropout`). You need to call the model with `training=True` and make sure it propagates this to every layer that needs it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePzxEHGHfBnh"
      },
      "source": [
        "## 12.4 TensorFlow Functions and Graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2YbhsqUfBnh"
      },
      "source": [
        "### 12.4.1 AutoGraph and Tracing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrLFCV3vfBnh"
      },
      "source": [
        "### 12.4.2 TF Function Rules"
      ]
    }
  ]
}