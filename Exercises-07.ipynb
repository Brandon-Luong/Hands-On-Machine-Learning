{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "54b71c1b326a101c02c54fb24502b2663857f505f7b72a06051ccceba068a4ff"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 7: Ensemble Learning and Random Forests Exercises"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.\n",
    "\n",
    "> If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results?\n",
    " \n",
    "> If so, how? If not, why?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Yes, we can combine these models and get better results by aggregating the predictions of each classifier and then taking the class that has the most votes or highest probability."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.\n",
    "\n",
    "> What is the difference between hard and soft voting classifiers?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Hard voting classifier predicts the class with the majority of votes.  \n",
    "\n",
    "Whereas soft voting classifier predicts the class with the highest probability, averaged over all classifiers.  \n",
    "Soft voting performs better than hard voting because it gives more weight to highly confident votes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.\n",
    "\n",
    "> Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers?\n",
    "\n",
    "> What about pasting ensembles, boosting ensembles, Random Forests, or stacking ensembles?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Yes, it is possible to speed up bagging (with replacement) ensembles by distributing across multiple servers because the predictors are trained in parallel.\n",
    "\n",
    "Pasting (without replacement) ensembles can be sped up because they are also trained in parallel.\n",
    "\n",
    "Random Forests can also be speed up because they are generally trained using the bagging method.\n",
    "\n",
    "Boosting ensembles cannot be sped up because they cannot be parallelized. Each subsequent predictor is based off the predecessor's errors.\n",
    "\n",
    "Stacking ensembles also cannot be sped up because again they cannot be parallelized. A blender is the culmination of the predictions from each layer of prior predictions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.\n",
    "\n",
    "> What is the benefit of out-of-bag evaluation?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Since out-of-bag instances are never seen by the predictor, out-of-bag evaluation is handy because you can evaluate on that set rather than making a validation set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.\n",
    "\n",
    "> What makes Extra-Trees more random than regular Random Forests?\n",
    "\n",
    "> How can this extra randomness help?\n",
    "\n",
    "> Are Extra-Trees slower or faster than regular Random Forests?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Extra-Trees randomize the thresholds for each feature rather than search for the best possible thresholds, as what Random Forests do.\n",
    "\n",
    "The extra randomness helps because it trades more bias for a lower variance.\n",
    "\n",
    "Extra-Trees are faster than regular Random Forests in training time, but performance can vary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.\n",
    "\n",
    "> If your AdaBoost ensemble underfits the training data, which hyperparameters should you tweak and how?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "If AdaBoost ensemble is underfitting the training data, increase the number of estimators and decrease the regularization of the base estimator."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.\n",
    "\n",
    "> If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "If Gradient Boosting ensemble overfits the training set, I should increase the learning rate because the learning rate shrinks the contribution of each tree. Since it's overfitting, each tree needs to contribute less."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.\n",
    "\n",
    "> 1. Load the MNIST data (introduced in Chapter 3), and split it into a training set, a validation set, and a test set  \n",
    "(e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing).\n",
    "\n",
    "> 2. Then train various classifiers, such as a Random Forest classifer, an Extra-Trees classifier, and an SVM classifier.\n",
    "\n",
    "> 3. Next, try to combine them into an ensemble that outperforms each individual classifier on the validation set, using soft or hard voting.\n",
    "\n",
    "> 4. Once you have found one, try it on the test set.\n",
    "\n",
    "> 5. How much better does it perform compared to the individual classifiers?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 9.\n",
    "\n",
    "> 1. Run the individual classifiers from the previous exercise to make predictions on the validation set. \n",
    "\n",
    "> 2. Create a new training set with the resulting predictions:  \n",
    "\n",
    ">>    - Each training instance is a vector containing the set of predictions from all your classifiers for an image\n",
    ">>    - The target is the image's class\n",
    "\n",
    "> 3. Train a classifier on this new training set.\n",
    "\n",
    "> 4. Congratulations, you have just trained a blender, and together with the classifiers it forms a stacking ensemble!\n",
    "\n",
    "> 5. Now evaluate the ensemble on the test set.\n",
    "\n",
    "> 6. For each image in the test set, make predictions with all your classifiers.\n",
    "\n",
    "> 7. Feed the predictions to the blender to get the ensemble's predictions.\n",
    "\n",
    "> 8. How does it compare to the voting classifier you trained earlier?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}