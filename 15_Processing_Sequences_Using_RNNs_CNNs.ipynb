{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "15_Processing_Sequences_Using_RNNs_CNNs.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIXSKqRpxA_0"
      },
      "source": [
        "# Chapter 15: Processing Sequences Using RNNs and CNNs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaC4rbnKgoT3"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmzv8sf4xA_4"
      },
      "source": [
        "## 15.1 Recurrent Neurons and Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soVjMn3Ny64G"
      },
      "source": [
        "A **recurrent neural network (RNN)** looks very much like a feedforward neural network, except it also has connections pointing backward. \n",
        "\n",
        "**Unrolling the network through time** - The same recurrent neuron represented once per time step.\n",
        "\n",
        "Extending to a layer of recurrent neurons, each neuron receives the input vector $\\mathbf{x}_{(t)}$ and the output vector from the previous time step $\\mathbf{y}_{(t-1)}$.\n",
        "\n",
        "Considering the whole recurrent layer, the weight vectors can be placed in two weight matrices $\\mathbf{W}_x$ and $\\mathbf{W}_y$.\n",
        "\n",
        "*Equation 15-1. Output of a recurrent layer for a single instance:*\n",
        "$$ \\mathbf{y}_{(t)} = \\phi(\\mathbf{W}_x^T \\mathbf{x}_{(t)} \n",
        "+ \\mathbf{W}_y^T \\mathbf{y}_{(t-1)} + \\mathbf{b})$$\n",
        "\n",
        "*Equation 15-2. Outputs of a layer of recurrent neurons for all instances in a mini-batch:\n",
        "$$\n",
        "\\mathbf{Y}_{(t)} = \\phi(\\mathbf{X}_{(t)} \\mathbf{W}_x  \n",
        "+ \\mathbf{Y}_{(t-1)} \\mathbf{W}_y + \\mathbf{b}) $$\n",
        "\n",
        "$$ \n",
        "\\mathbf{Y}_{(t)} = \\phi([\\mathbf{X}_{(t)} \\mathbf{Y}_{(t-1)}] \\mathbf{W} + \\mathbf{b}) \\text{, with } \\mathbf{W} = \n",
        "\\begin{bmatrix}\n",
        "\\mathbf{W}_x \\\\\n",
        "\\mathbf{W}_y\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "In this equation:\n",
        "- $\\mathbf{Y}_{(t)}$ is an $m \\times n_{neurons} $ matrix containing the layer's outputs at time step *t* for each instance in the mini-batch.\n",
        "- $\\mathbf{X}_{(t)}$ is an $m \\times n_{inputs}$ matrix containing the inputs for all instances.\n",
        "- $\\mathbf{W}_x $ is an $n_{inputs} \\times n_{neurons}$ matrix containing the connection weights for the inputs of the current time step.\n",
        "- $\\mathbf{W}_y $ is an $n_{neurons} \\times n_{neurons}$ matrix containing the connection weights for the outputs of the previous time step.\n",
        "- $\\mathbf{b}$ is a vector of size $n_{neurons}$ containing each neuron's bias term.\n",
        "- The weight matrices $\\mathbf{W}_x $ and $\\mathbf{W}_y$ are concatenated vertically into a single weight matrix $\\mathbf{W}$ of shape $(n_{inputs} + n_{neurons}) \\times n_{neurons}$.\n",
        "- The notation $[\\mathbf{X}_{(t)} \\mathbf{Y}_{(t-1)}]$ represents the horizontal concatentation of the matrices $\\mathbf{X}_{(t)}$ and $\\mathbf{Y}_{(t-1)}$.\n",
        "\n",
        "> Note: $\\mathbf{Y}_{(t)}$ is a function of all the inputs since time $t=0$ (ie. $\\mathbf{X}_{(0)}, \\mathbf{X}_{(1)}... \\mathbf{X}_{(t)}$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ9gQzYtxA_5"
      },
      "source": [
        "### 15.1.1 Memory Cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCWx2kaKDbK0"
      },
      "source": [
        "Since the output of a recurrent neuron at time step $t$ is a function of all the inputs from the previous time steps, it has a form of *memory*.\n",
        "\n",
        "**Memory cell (a cell)** - A part of a neural network that preserves some state across time steps.\n",
        "\n",
        "A single recurrent neuron, or a layer of recurrent neurons, is a very basic cell.\n",
        "\n",
        "In general a cell's state at time step $t$, denoted $\\mathbf{h}_{(t)}$ (the \"h\" stands for \"hidden\"), is a function of some inputs at that time step and its state at the previous time step: $\\mathbf{h}_{(t)} = f(\\mathbf{h}_{(t-1)}, \\mathbf{x}_{(t)})$. Its output at time step $t$ denoted $\\mathbf{y}_{(t)}$ is also a function of the previous state and the current inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AsvaGLlxA_5"
      },
      "source": [
        "### 15.1.2 Input and Output Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRngCIt-EZgP"
      },
      "source": [
        "**Sequence-to-sequence network** - An RNN that can simultaneously take a sequence of inputs and produce a sequence of outputs (**see top-left Figure 15-4 in book**). \n",
        "\n",
        "- Predicting time series such as stock prices: feed it prices over the last *N* days, and it must output the prices shifted by one day into the future.\n",
        "\n",
        "**Sequence-to-vector network** - Feed the network a sequence of inputs and ignore all outputs except for the last one (**see top-right Figure 15-4 in book**).\n",
        "\n",
        "- You can feed the network a sequence of words corresponding to a movie review, and the network would output a sentiment score.\n",
        "\n",
        "**Vector-to-sequence network** - Feed the network the same input vector over and over at each time step and let it output a sequence (**see bottom-left Figure 15-4 in book**).\n",
        "\n",
        "- The input can be an image (or the output of a CNN) and the output could be a caption for that image.\n",
        "\n",
        "**Encoder-Decoder** - A two-step model consisting of a sequence-to-vector network (**encoder**) followed by vector-to-sequence network (**decoder**) (**see bottom-right Figure 15-4 in book**).\n",
        "\n",
        "- Translating a sentence from one language to another. Feed the network a sentence in one language, the encoder would convert this sentence into a single vector representation, and then the decoder would decode this vector into a sentence in another language.\n",
        "- Works much better than translating with a single sequence-to-sequence RNN: the last words of a sentence can affect the first words of the translation, so you need to wait until you have seen the whole sentence before translating it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FalKPugAxA_6"
      },
      "source": [
        "## 15.2 Training RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSSAPZstbtmN"
      },
      "source": [
        "**Backpropagation through time (BPTT)** - To train an RNN, unroll it through time and then use regular backpropagation (**see Figure 15-5 in book**).\n",
        "\n",
        "General outline to train RNN:\n",
        "1. Compute forward pass through the unrolled network.\n",
        "2. Evaluate the output sequence using a cost function $C(\\mathbf{Y}_{(0)}, \\mathbf{Y}_{(1)},..., \\mathbf{Y}_{(T)})$, where $T$ is the max time step.\n",
        "3. Propagate the gradients of that cost function backward through the unrolled network.\n",
        "    - Note: The gradients flow backward through all the outputs used by the cost function.\n",
        "4. Update model parameters using the gradients computed during BPTT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFkbHKr0xA_6"
      },
      "source": [
        "## 15.3 Forecasting a Time Series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYx_RK_ie9cC"
      },
      "source": [
        "**Time series** - A sequence of one or more values per time step.\n",
        "\n",
        "**Univariate/Multivariate time series** - A single value or multiple values per time step.\n",
        "\n",
        "**Forecasting** - Predicting future values.\n",
        "\n",
        "**Imputation** - Fill in the blanks: to predict (or \"postdict\") missing values from the past."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfPgrPnkgSdY"
      },
      "source": [
        "# Generates Figure 15-6 time series\n",
        "def generate_time_series(batch_size, n_steps):\n",
        "    \"\"\"Creates as many time series as requested by batch_size,\n",
        "    each with length n_steps, and one value per time step in each series.\n",
        "    \n",
        "    Returns a NumPy array of shape [batch size, time steps, 1], where\n",
        "    each series is the sum of 2 sine waves of fixed amplitudes\n",
        "    but random frequencies and phases, plus a bit of noise.\"\"\"\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) # wave 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5) # + noise\n",
        "    return series[..., np.newaxis].astype(np.float32)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLNoCMc4ioOW"
      },
      "source": [
        "> Note: When dealing with time series, the input features are generally represented as 3D arrays of shape _[batch size, time steps, dimensionality]_, where *dimensionality* is 1 for univariate time series and more for multivariate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuW6R9rAjBXI"
      },
      "source": [
        "\"\"\"Create training set, validation set, and test set using the function.\n",
        "\n",
        "X_train contains 7,000 time series (shape [7000, 50, 1]),\n",
        "X_valid contains 2,000 (7,000th-8,999th time series),\n",
        "X_test contains 1,000 (9,000-9,999th time series).\n",
        "Since we want to forecast a single value for each series, \n",
        "the targets are column vectors (y_train has shape [7000, 1]).\"\"\"\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)\n",
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWy-mqYLxA_7"
      },
      "source": [
        "### 15.3.1 Baseline Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiMachZKgrHR"
      },
      "source": [
        "It is a good idea to have a few baseline metrics to make sure the model isn't doing worse than the basic models.\n",
        "\n",
        "One approach is:\n",
        "- **Naive forecasting** - Predict the last value in each series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZrKL5OEhKnD",
        "outputId": "83d03334-c64d-4dc8-8120-4ee930631751"
      },
      "source": [
        "y_pred = X_valid[:, -1]\n",
        "np.mean(keras.losses.mean_squared_error(y_valid, y_pred))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.020132534"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fax-whFviP5k"
      },
      "source": [
        "Another approach is:\n",
        "- A fully connected network.\n",
        "\n",
        "The MSE of the fully connected network is about 0.004, much better than the naive approach with 0.020."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnN7x_vaihu7"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[50, 1]), # Expects a flat list of features\n",
        "    keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPWOY8vikXqy",
        "outputId": "f2f4ff64-e3a8-4c8b-9c15-f70fb8e18348"
      },
      "source": [
        "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "219/219 [==============================] - 1s 2ms/step - loss: 0.1120 - val_loss: 0.0216\n",
            "Epoch 2/20\n",
            "219/219 [==============================] - 0s 2ms/step - loss: 0.0173 - val_loss: 0.0138\n",
            "Epoch 3/20\n",
            "219/219 [==============================] - 0s 2ms/step - loss: 0.0123 - val_loss: 0.0106\n",
            "Epoch 4/20\n",
            "219/219 [==============================] - 0s 1ms/step - loss: 0.0099 - val_loss: 0.0090\n",
            "Epoch 5/20\n",
            "219/219 [==============================] - 0s 1ms/step - loss: 0.0087 - val_loss: 0.0081\n",
            "Epoch 6/20\n",
            "219/219 [==============================] - 0s 1ms/step - loss: 0.0079 - val_loss: 0.0072\n",
            "Epoch 7/20\n",
            "219/219 [==============================] - 0s 2ms/step - loss: 0.0072 - val_loss: 0.0066\n",
            "Epoch 8/20\n",
            "219/219 [==============================] - 0s 1ms/step - loss: 0.0067 - val_loss: 0.0062\n",
            "Epoch 9/20\n",
            "219/219 [==============================] - 0s 1ms/step - loss: 0.0062 - val_loss: 0.0058\n",
            "Epoch 10/20\n",
            "219/219 [==============================] - 0s 1ms/step - loss: 0.0058 - val_loss: 0.0054\n",
            "Epoch 11/20\n",
            "219/219 [==============================] - 0s 1ms/step - loss: 0.0055 - val_loss: 0.0051\n",
            "Epoch 12/20\n",
            "219/219 [==============================] - 0s 1ms/step - loss: 0.0053 - val_loss: 0.0049\n",
            "Epoch 13/20\n",
            "219/219 [==============================] - 0s 2ms/step - loss: 0.0050 - val_loss: 0.0047\n",
            "Epoch 14/20\n",
            "219/219 [==============================] - 0s 1ms/step - loss: 0.0049 - val_loss: 0.0046\n",
            "Epoch 15/20\n",
            "219/219 [==============================] - 0s 1ms/step - loss: 0.0047 - val_loss: 0.0044\n",
            "Epoch 16/20\n",
            "219/219 [==============================] - 0s 1ms/step - loss: 0.0046 - val_loss: 0.0042\n",
            "Epoch 17/20\n",
            "219/219 [==============================] - 0s 1ms/step - loss: 0.0045 - val_loss: 0.0042\n",
            "Epoch 18/20\n",
            "219/219 [==============================] - 0s 1ms/step - loss: 0.0044 - val_loss: 0.0041\n",
            "Epoch 19/20\n",
            "219/219 [==============================] - 0s 1ms/step - loss: 0.0043 - val_loss: 0.0039\n",
            "Epoch 20/20\n",
            "219/219 [==============================] - 0s 1ms/step - loss: 0.0042 - val_loss: 0.0039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOyQ3rIfmHuT",
        "outputId": "05d16cfc-c547-4e9c-f098-e83427234690"
      },
      "source": [
        "model.evaluate(X_valid, y_valid)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 0s 1ms/step - loss: 0.0039\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.003884839126840234"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5aNo0dnxA_7"
      },
      "source": [
        "### 15.3.2 Implementing a Simple RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrYWv6o5xA_8"
      },
      "source": [
        "### 15.3.3 Deep RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDzAx_-4xA_9"
      },
      "source": [
        "### 15.3.4 Forecasting Several Time Steps Ahead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMaIv1PkxA_9"
      },
      "source": [
        "## 15.4 Handling Long Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR81kzkYxA_-"
      },
      "source": [
        "### 15.4.1 Fighting the Unstable Gradients Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbxZDDFixA_-"
      },
      "source": [
        "### 15.4.2 Tackling the Short-Term Memory Problem"
      ]
    }
  ]
}