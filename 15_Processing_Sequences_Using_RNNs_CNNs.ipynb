{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": 3
    },
    "orig_nbformat": 2,
    "colab": {
      "name": "15_Processing_Sequences_Using_RNNs_CNNs.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIXSKqRpxA_0"
      },
      "source": [
        "# Chapter 15: Processing Sequences Using RNNs and CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmzv8sf4xA_4"
      },
      "source": [
        "## 15.1 Recurrent Neurons and Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soVjMn3Ny64G"
      },
      "source": [
        "A **recurrent neural network (RNN)** looks very much like a feedforward neural network, except it also has connections pointing backward. \n",
        "\n",
        "**Unrolling the network through time** - The same recurrent neuron represented once per time step.\n",
        "\n",
        "Extending to a layer of recurrent neurons, each neuron receives the input vector $\\mathbf{x}_{(t)}$ and the output vector from the previous time step $\\mathbf{y}_{(t-1)}$.\n",
        "\n",
        "Considering the whole recurrent layer, the weight vectors can be placed in two weight matrices $\\mathbf{W}_x$ and $\\mathbf{W}_y$.\n",
        "\n",
        "*Equation 15-1. Output of a recurrent layer for a single instance:*\n",
        "$$ \\mathbf{y}_{(t)} = \\phi(\\mathbf{W}_x^T \\mathbf{x}_{(t)} \n",
        "+ \\mathbf{W}_y^T \\mathbf{y}_{(t-1)} + \\mathbf{b})$$\n",
        "\n",
        "*Equation 15-2. Outputs of a layer of recurrent neurons for all instances in a mini-batch:\n",
        "$$\n",
        "\\mathbf{Y}_{(t)} = \\phi(\\mathbf{X}_{(t)} \\mathbf{W}_x  \n",
        "+ \\mathbf{Y}_{(t-1)} \\mathbf{W}_y + \\mathbf{b}) $$\n",
        "\n",
        "$$ \n",
        "\\mathbf{Y}_{(t)} = \\phi([\\mathbf{X}_{(t)} \\mathbf{Y}_{(t-1)}] \\mathbf{W} + \\mathbf{b}) \\text{, with } \\mathbf{W} = \n",
        "\\begin{bmatrix}\n",
        "\\mathbf{W}_x \\\\\n",
        "\\mathbf{W}_y\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "In this equation:\n",
        "- $\\mathbf{Y}_{(t)}$ is an $m \\times n_{neurons} $ matrix containing the layer's outputs at time step *t* for each instance in the mini-batch.\n",
        "- $\\mathbf{X}_{(t)}$ is an $m \\times n_{inputs}$ matrix containing the inputs for all instances.\n",
        "- $\\mathbf{W}_x $ is an $n_{inputs} \\times n_{neurons}$ matrix containing the connection weights for the inputs of the current time step.\n",
        "- $\\mathbf{W}_y $ is an $n_{neurons} \\times n_{neurons}$ matrix containing the connection weights for the outputs of the previous time step.\n",
        "- $\\mathbf{b}$ is a vector of size $n_{neurons}$ containing each neuron's bias term.\n",
        "- The weight matrices $\\mathbf{W}_x $ and $\\mathbf{W}_y$ are concatenated vertically into a single weight matrix $\\mathbf{W}$ of shape $(n_{inputs} + n_{neurons}) \\times n_{neurons}$.\n",
        "- The notation $[\\mathbf{X}_{(t)} \\mathbf{Y}_{(t-1)}]$ represents the horizontal concatentation of the matrices $\\mathbf{X}_{(t)}$ and $\\mathbf{Y}_{(t-1)}$.\n",
        "\n",
        "> Note: $\\mathbf{Y}_{(t)}$ is a function of all the inputs since time $t=0$ (ie. $\\mathbf{X}_{(0)}, \\mathbf{X}_{(1)}... \\mathbf{X}_{(t)}$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ9gQzYtxA_5"
      },
      "source": [
        "### 15.1.1 Memory Cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCWx2kaKDbK0"
      },
      "source": [
        "Since the output of a recurrent neuron at time step $t$ is a function of all the inputs from the previous time steps, it has a form of *memory*.\n",
        "\n",
        "**Memory cell (a cell)** - A part of a neural network that preserves some state across time steps.\n",
        "\n",
        "A single recurrent neuron, or a layer of recurrent neurons, is a very basic cell.\n",
        "\n",
        "In general a cell's state at time step $t$, denoted $\\mathbf{h}_{(t)}$ (the \"h\" stands for \"hidden\"), is a function of some inputs at that time step and its state at the previous time step: $\\mathbf{h}_{(t)} = f(\\mathbf{h}_{(t-1)}, \\mathbf{x}_{(t)})$. Its output at time step $t$ denoted $\\mathbf{y}_{(t)}$ is also a function of the previous state and the current inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AsvaGLlxA_5"
      },
      "source": [
        "### 15.1.2 Input and Output Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRngCIt-EZgP"
      },
      "source": [
        "**Sequence-to-sequence network** - An RNN that can simultaneously take a sequence of inputs and produce a sequence of outputs (**see top-left Figure 15-4 in book**). \n",
        "\n",
        "- Predicting time series such as stock prices: feed it prices over the last *N* days, and it must output the prices shifted by one day into the future.\n",
        "\n",
        "**Sequence-to-vector network** - Feed the network a sequence of inputs and ignore all outputs except for the last one (**see top-right Figure 15-4 in book**).\n",
        "\n",
        "- You can feed the network a sequence of words corresponding to a movie review, and the network would output a sentiment score.\n",
        "\n",
        "**Vector-to-sequence network** - Feed the network the same input vector over and over at each time step and let it output a sequence (**see bottom-left Figure 15-4 in book**).\n",
        "\n",
        "- The input can be an image (or the output of a CNN) and the output could be a caption for that image.\n",
        "\n",
        "**Encoder-Decoder** - A two-step model consisting of a sequence-to-vector network (**encoder**) followed by vector-to-sequence network (**decoder**) (**see bottom-right Figure 15-4 in book**).\n",
        "\n",
        "- Translating a sentence from one language to another. Feed the network a sentence in one language, the encoder would convert this sentence into a single vector representation, and then the decoder would decode this vector into a sentence in another language.\n",
        "- Works much better than translating with a single sequence-to-sequence RNN: the last words of a sentence can affect the first words of the translation, so you need to wait until you have seen the whole sentence before translating it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FalKPugAxA_6"
      },
      "source": [
        "## 15.2 Training RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSSAPZstbtmN"
      },
      "source": [
        "**Backpropagation through time (BPTT)** - To train an RNN, unroll it through time and then use regular backpropagation (**see Figure 15-5 in book**).\n",
        "\n",
        "General outline to train RNN:\n",
        "1. Compute forward pass through the unrolled network.\n",
        "2. Evaluate the output sequence using a cost function $C(\\mathbf{Y}_{(0)}, \\mathbf{Y}_{(1)},..., \\mathbf{Y}_{(T)})$, where $T$ is the max time step.\n",
        "3. Propagate the gradients of that cost function backward through the unrolled network.\n",
        "    - Note: The gradients flow backward through all the outputs used by the cost function.\n",
        "4. Update model parameters using the gradients computed during BPTT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFkbHKr0xA_6"
      },
      "source": [
        "## 15.3 Forecasting a Time Series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWy-mqYLxA_7"
      },
      "source": [
        "### 15.3.1 Baseline Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5aNo0dnxA_7"
      },
      "source": [
        "### 15.3.2 Implementing a Simple RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrYWv6o5xA_8"
      },
      "source": [
        "### 15.3.3 Deep RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDzAx_-4xA_9"
      },
      "source": [
        "### 15.3.4 Forecasting Several Time Steps Ahead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMaIv1PkxA_9"
      },
      "source": [
        "## 15.4 Handling Long Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR81kzkYxA_-"
      },
      "source": [
        "### 15.4.1 Fighting the Unstable Gradients Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbxZDDFixA_-"
      },
      "source": [
        "### 15.4.2 Tackling the Short-Term Memory Problem"
      ]
    }
  ]
}