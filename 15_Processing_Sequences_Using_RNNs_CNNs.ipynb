{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": 3
    },
    "orig_nbformat": 2,
    "colab": {
      "name": "15_Processing_Sequences_Using_RNNs_CNNs.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIXSKqRpxA_0"
      },
      "source": [
        "# Chapter 15: Processing Sequences Using RNNs and CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmzv8sf4xA_4"
      },
      "source": [
        "## 15.1 Recurrent Neurons and Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soVjMn3Ny64G"
      },
      "source": [
        "A **recurrent neural network (RNN)** looks very much like a feedforward neural network, except it also has connections pointing backward. \n",
        "\n",
        "**Unrolling the network through time** - The same recurrent neuron represented once per time step.\n",
        "\n",
        "Extending to a layer of recurrent neurons, each neuron receives the input vector $\\mathbf{x}_{(t)}$ and the output vector from the previous time step $\\mathbf{y}_{(t-1)}$.\n",
        "\n",
        "Considering the whole recurrent layer, the weight vectors can be placed in two weight matrices $\\mathbf{W}_x$ and $\\mathbf{W}_y$.\n",
        "\n",
        "*Equation 15-1. Output of a recurrent layer for a single instance:*\n",
        "$$ \\mathbf{y}_{(t)} = \\phi(\\mathbf{W}_x^T \\mathbf{x}_{(t)} \n",
        "+ \\mathbf{W}_y^T \\mathbf{y}_{(t-1)} + \\mathbf{b})$$\n",
        "\n",
        "*Equation 15-2. Outputs of a layer of recurrent neurons for all instances in a mini-batch:\n",
        "$$\n",
        "\\mathbf{Y}_{(t)} = \\phi(\\mathbf{X}_{(t)} \\mathbf{W}_x  \n",
        "+ \\mathbf{Y}_{(t-1)} \\mathbf{W}_y + \\mathbf{b}) $$\n",
        "\n",
        "$$ \n",
        "\\mathbf{Y}_{(t)} = \\phi([\\mathbf{X}_{(t)} \\mathbf{Y}_{(t-1)}] \\mathbf{W} + \\mathbf{b}) \\text{, with } \\mathbf{W} = \n",
        "\\begin{bmatrix}\n",
        "\\mathbf{W}_x \\\\\n",
        "\\mathbf{W}_y\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "In this equation:\n",
        "- $\\mathbf{Y}_{(t)}$ is an $m \\times n_{neurons} $ matrix containing the layer's outputs at time step *t* for each instance in the mini-batch.\n",
        "- $\\mathbf{X}_{(t)}$ is an $m \\times n_{inputs}$ matrix containing the inputs for all instances.\n",
        "- $\\mathbf{W}_x $ is an $n_{inputs} \\times n_{neurons}$ matrix containing the connection weights for the inputs of the current time step.\n",
        "- $\\mathbf{W}_y $ is an $n_{neurons} \\times n_{neurons}$ matrix containing the connection weights for the outputs of the previous time step.\n",
        "- $\\mathbf{b}$ is a vector of size $n_{neurons}$ containing each neuron's bias term.\n",
        "- The weight matrices $\\mathbf{W}_x $ and $\\mathbf{W}_y$ are concatenated vertically into a single weight matrix $\\mathbf{W}$ of shape $(n_{inputs} + n_{neurons}) \\times n_{neurons}$.\n",
        "- The notation $[\\mathbf{X}_{(t)} \\mathbf{Y}_{(t-1)}]$ represents the horizontal concatentation of the matrices $\\mathbf{X}_{(t)}$ and $\\mathbf{Y}_{(t-1)}$.\n",
        "\n",
        "> Note: $\\mathbf{Y}_{(t)}$ is a function of all the inputs since time $t=0$ (ie. $\\mathbf{X}_{(0)}, \\mathbf{X}_{(1)}... \\mathbf{X}_{(t)}$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ9gQzYtxA_5"
      },
      "source": [
        "### 15.1.1 Memory Cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCWx2kaKDbK0"
      },
      "source": [
        "Since the output of a recurrent neuron at time step $t$ is a function of all the inputs from the previous time steps, it has a form of *memory*.\n",
        "\n",
        "**Memory cell (a cell)** - A part of a neural network that preserves some state across time steps.\n",
        "\n",
        "A single recurrent neuron, or a layer of recurrent neurons, is a very basic cell.\n",
        "\n",
        "In general a cell's state at time step $t$, denoted $\\mathbf{h}_{(t)}$ (the \"h\" stands for \"hidden\"), is a function of some inputs at that time step and its state at the previous time step: $\\mathbf{h}_{(t)} = f(\\mathbf{h}_{(t-1)}, \\mathbf{x}_{(t)})$. Its output at time step $t$ denoted $\\mathbf{y}_{(t)}$ is also a function of the previous state and the current inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AsvaGLlxA_5"
      },
      "source": [
        "### 15.1.2 Input and Output Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FalKPugAxA_6"
      },
      "source": [
        "## 15.2 Training RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFkbHKr0xA_6"
      },
      "source": [
        "## 15.3 Forecasting a Time Series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWy-mqYLxA_7"
      },
      "source": [
        "### 15.3.1 Baseline Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5aNo0dnxA_7"
      },
      "source": [
        "### 15.3.2 Implementing a Simple RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrYWv6o5xA_8"
      },
      "source": [
        "### 15.3.3 Deep RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDzAx_-4xA_9"
      },
      "source": [
        "### 15.3.4 Forecasting Several Time Steps Ahead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMaIv1PkxA_9"
      },
      "source": [
        "## 15.4 Handling Long Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR81kzkYxA_-"
      },
      "source": [
        "### 15.4.1 Fighting the Unstable Gradients Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbxZDDFixA_-"
      },
      "source": [
        "### 15.4.2 Tackling the Short-Term Memory Problem"
      ]
    }
  ]
}