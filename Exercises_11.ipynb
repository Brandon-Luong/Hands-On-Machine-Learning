{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "7b58ff03a81b7b220740b362f01ec4719380627cca970f4b363191d14b8c12ac"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 11: Training Deep Neural Networks Exercises"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.\n",
    "\n",
    "> Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.\n",
    "\n",
    "> Is it OK to initialize the bias terms to 0?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.\n",
    "\n",
    "> Name three advantages of the SELU activation function over ReLU."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.\n",
    "\n",
    "> In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.\n",
    "\n",
    "> What may happen if you set the `momentum` hyperparameter too close to 1 (eg. 0.99999) when using an SGD optimizer?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.\n",
    "\n",
    "> Name three ways you can produce a sparse model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.\n",
    "\n",
    "> Does dropout slow down training?\n",
    "\n",
    "> Does it slow down inference (ie. making predictions on new instances)?\n",
    "\n",
    "> What about MC Dropout?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.\n",
    "\n",
    "> Practice training a deep neural network on the CIFAR10 image dataset.\n",
    "\n",
    "> a. Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function.\n",
    "\n",
    "> b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset.\n",
    "\n",
    ">> - You can load it with `keras.datasets.cifar10.load_data()`.\n",
    ">> - The dataset is composed of 60,000 32x32-pixel color images (50,000 for training, 10,000 for testing) with 10 classes.\n",
    ">> - So you'll need a softmax output layer with 10 neurons.\n",
    ">> - Remember to search for the right learning rate each time you change the model's architecture or hyperparameters.\n",
    "\n",
    "> c. Now try adding Batch Normalization and compare the learning curves:\n",
    "\n",
    ">> - Is it converging faster than before?\n",
    ">> - Does it produce a better model?\n",
    ">> - How does it affect training speed?\n",
    "\n",
    "> d. Try replacing Batch Normalization with SELU, and make the necessary adjustments to ensure the network self-normalizes (ie. standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "\n",
    "> e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.\n",
    "\n",
    "> f. Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}