{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "7b58ff03a81b7b220740b362f01ec4719380627cca970f4b363191d14b8c12ac"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 11: Training Deep Neural Networks Exercises"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.\n",
    "\n",
    "> Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "It's not okay to initialize all the weights with the same value even if the value is randomly selected from He initialization because it will still be unable to break the symmetry. If all the weights are the same value, the gradients would all be the same during backpropagation and the neural network becomes equivalent to one with only a single layer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.\n",
    "\n",
    "> Is it OK to initialize the bias terms to 0?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "It's okay to initialize the bias terms to 0 as it makes no difference."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.\n",
    "\n",
    "> Name three advantages of the SELU activation function over ReLU."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- The network will self-normalize, preserving a mean of 0 and standard deviation of 1, which solves the vanishing/exploding gradients problem.\n",
    "\n",
    "- It is a scaled variant of ELU activation function so it can take negative values when $z<0$, which allows an average output closer to 0 and alleviates the vanishing gradients problem.\n",
    "\n",
    "- Also from ELU, it has a nonzero gradient for $z<0$, which avoids the dead neurons problem (dying ReLUs)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.\n",
    "\n",
    "> In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In general, $ \\text{SELU} > \\text{ELU} > \\text{leaky ReLU (and its variants)} > \\text{ReLU} > \\tanh > \\text{logistic}$ and $\\text{softmax}$.\n",
    "\n",
    "SELU:\n",
    "- Best performance but needs to meet certain conditions.\n",
    "- Input features must be standardized (mean 0, standard deviation 1).\n",
    "- Every hidden layer's weights must be LeCun normal initialization.\n",
    "- Network's architecture must be sequential.\n",
    "- All layers must be dense.\n",
    "\n",
    "leaky ReLU (and its variants):\n",
    "- If you care about runtime latency.\n",
    "\n",
    "ReLU:\n",
    "- If you care about speed.\n",
    "- Most widely used so many libraries have optimizations for it.\n",
    "\n",
    "tanh:\n",
    "- Want output to fall within certain values.\n",
    "- Has a range of -1 to 1.\n",
    "\n",
    "logistic:\n",
    "- Want output to be strictly positive.\n",
    "- Has a range of 0 to 1.\n",
    "\n",
    "softmax:\n",
    "- Mostly output for multiclass classification.\n",
    "- Estimated probabilities between 0 and 1.\n",
    "- All probabilities add up to 1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.\n",
    "\n",
    "> What may happen if you set the `momentum` hyperparameter too close to 1 (eg. 0.99999) when using an SGD optimizer?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "If the `momentum` hyperparameter is set too close to 1, such as 0.99999, it means it has barely any friction. Without any friction, the optimizer will overshoot the optimum and oscillate for a very long time, possibly never even converging."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.\n",
    "\n",
    "> Name three ways you can produce a sparse model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- Train the model as usual then get rid of the tiny weights (set them to 0).\n",
    "- Apply strong $\\ell_1$ regularization during training.\n",
    "- Use TensorFlow Model Optimization Toolkit (TF-MOT) to iteratively remove connections during training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.\n",
    "\n",
    "> Does dropout slow down training?\n",
    "\n",
    "> Does it slow down inference (ie. making predictions on new instances)?\n",
    "\n",
    "> What about MC Dropout?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Dropout does slow down training since at every training step, every neuron has a chance to ignored and thus slow down convergence to a solution.\n",
    "\n",
    "Dropout does not slow down inference because it is only active during training.\n",
    "\n",
    "For MC Dropout, training time is slowed down similar to regular dropout. But MC Dropout runs during inference, which means that doubling the number of Monte Carlo samples will double the inference time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.\n",
    "\n",
    "> Practice training a deep neural network on the CIFAR10 image dataset.\n",
    "\n",
    "> a. Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function.\n",
    "\n",
    "> b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset.\n",
    "\n",
    ">> - You can load it with `keras.datasets.cifar10.load_data()`.\n",
    ">> - The dataset is composed of 60,000 32x32-pixel color images (50,000 for training, 10,000 for testing) with 10 classes.\n",
    ">> - So you'll need a softmax output layer with 10 neurons.\n",
    ">> - Remember to search for the right learning rate each time you change the model's architecture or hyperparameters.\n",
    "\n",
    "> c. Now try adding Batch Normalization and compare the learning curves:\n",
    "\n",
    ">> - Is it converging faster than before?\n",
    ">> - Does it produce a better model?\n",
    ">> - How does it affect training speed?\n",
    "\n",
    "> d. Try replacing Batch Normalization with SELU, and make the necessary adjustments to ensure the network self-normalizes (ie. standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "\n",
    "> e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.\n",
    "\n",
    "> f. Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}