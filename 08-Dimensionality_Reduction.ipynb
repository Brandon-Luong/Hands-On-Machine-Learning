{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "7b58ff03a81b7b220740b362f01ec4719380627cca970f4b363191d14b8c12ac"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 8: Dimensionality Reduction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*Curse of dimensionality* - Each training instance has thousands or millions of features, which makes training extremely slow and much harder to find a good solution.\n",
    "\n",
    "> Note: Reducing dimensionality does cause some information loss (similar to image compression). So if training is too slow, you should first try to train your system with the original data before considering using dimensionality reduction.\n",
    "\n",
    "Dimensionality reduction is also useful for data visualization (DataViz). If data is reduced to 2-3 dimensions, you can plot it for any additional patterns or insight. Also great for presentations to non-data scientists."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.1 The Curse of Dimensionality"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Average distance in N-Dimension Comparison:\n",
    "\n",
    "- 2D (unit square): 0.52\n",
    "- 3D (unit cube): 0.66\n",
    "- 1,000,000-D (unit hypercube): 408.25\n",
    "\n",
    "Conclusions:\n",
    "\n",
    "- There's just plenty of space in high dimensions.\n",
    "- High-dimension datasets are at risk of being very sparse (ie. most training instances would be far away from each other).\n",
    "- The more dimensions the training set has, the greater risk of overfitting.\n",
    "\n",
    "> Recall: To solve high bias, add more and/or complex features."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.2 Main Approaches for Dimensionality Reduction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.2.1 Projection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Not all training instances are spread uniformly across all dimensions; some are constant and some are highly correlated with each other.  \n",
    "=> All training instances lie within or close to a much lower-dimensional subspace.\n",
    "\n",
    "Suppose all training instance of 3-D dataset lie close to a plane. If we project each instance onto the plane, we have successfully reduced from 3-D to 2-D.\n",
    "\n",
    "But sometimes it's not the best idea if the dataset \"twists or turns\" because in doing so, the 2-D projections may overlap with each other instead of \"unrolling\" each section.\n",
    "\n",
    "> Note: See Figures 8-4 and 8-5 in the book for pictorial reference."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.2.2 Manifold Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally resembles a d-dimensional hyperplane.  \n",
    "In Swiss roll example, d=2 & n=3, so it resembles a 2-D hyperplane rolled up in 3-D.\n",
    "\n",
    "Key assumptions for **Manifold Learning**:\n",
    "\n",
    "1. *Manifold assumption (manifold hypothesis)* - Most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. Constraints tend to squeeze the dataset into a lower-dimensional manifold.\n",
    "\n",
    "2. The task at hand (eg. classification, regression) will be simpler if expressed in the lower-dimensional space of the manifold.\n",
    "\n",
    "> Note: Assumption #2 doesn't always hold. Sometimes, going from 3-D to 2-D results in complex -> simple decision boundary. Other times it results in simple -> complex. See Figure 8-6 in book for pictorial reference."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.3 PCA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.3.1 Preserving the Variance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.3.2 Principal Components"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.3.3 Projecting Down to d Dimensions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.3.4 Using Scikit-Learn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.3.5 Explained Variance Ratio"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.3.6 Choosing the Right Number of Dimensions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.3.7 PCA for Compression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.3.8 Randomized PCA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.3.9 Incremental PCA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.4 Kernel PCA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.4.1 Selecting a Kernel and Tuning Hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.5 LLE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.6 Other Dimensionality Reduction Techniques"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}