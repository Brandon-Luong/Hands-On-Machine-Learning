{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "7b58ff03a81b7b220740b362f01ec4719380627cca970f4b363191d14b8c12ac"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 8: Dimensionality Reduction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "*Curse of dimensionality* - Each training instance has thousands or millions of features, which makes training extremely slow and much harder to find a good solution.\n",
    "\n",
    "> Note: Reducing dimensionality does cause some information loss (similar to image compression). So if training is too slow, you should first try to train your system with the original data before considering using dimensionality reduction.\n",
    "\n",
    "Dimensionality reduction is also useful for data visualization (DataViz). If data is reduced to 2-3 dimensions, you can plot it for any additional patterns or insight. Also great for presentations to non-data scientists."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.1 The Curse of Dimensionality"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Average distance in N-Dimension Comparison:\n",
    "\n",
    "- 2D (unit square): 0.52\n",
    "- 3D (unit cube): 0.66\n",
    "- 1,000,000-D (unit hypercube): 408.25\n",
    "\n",
    "Conclusions:\n",
    "\n",
    "- There's just plenty of space in high dimensions.\n",
    "- High-dimension datasets are at risk of being very sparse (ie. most training instances would be far away from each other).\n",
    "- The more dimensions the training set has, the greater risk of overfitting.\n",
    "\n",
    "> Recall: To solve high bias, add more and/or complex features."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.2 Main Approaches for Dimensionality Reduction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.2.1 Projection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Not all training instances are spread uniformly across all dimensions; some are constant and some are highly correlated with each other.  \n",
    "=> All training instances lie within or close to a much lower-dimensional subspace.\n",
    "\n",
    "Suppose all training instance of 3-D dataset lie close to a plane. If we project each instance onto the plane, we have successfully reduced from 3-D to 2-D.\n",
    "\n",
    "But sometimes it's not the best idea if the dataset \"twists or turns\" because in doing so, the 2-D projections may overlap with each other instead of \"unrolling\" each section.\n",
    "\n",
    "> Note: See Figures 8-4 and 8-5 in the book for pictorial reference."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.2.2 Manifold Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally resembles a d-dimensional hyperplane.  \n",
    "In Swiss roll example, d=2 & n=3, so it resembles a 2-D hyperplane rolled up in 3-D.\n",
    "\n",
    "Key assumptions for **Manifold Learning**:\n",
    "\n",
    "1. *Manifold assumption (manifold hypothesis)* - Most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. Constraints tend to squeeze the dataset into a lower-dimensional manifold.\n",
    "\n",
    "2. The task at hand (eg. classification, regression) will be simpler if expressed in the lower-dimensional space of the manifold.\n",
    "\n",
    "> Note: Assumption #2 doesn't always hold. Sometimes, going from 3-D to 2-D results in complex -> simple decision boundary. Other times it results in simple -> complex. See Figure 8-6 in book for pictorial reference."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.3 PCA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*Principal Component Analysis (PCA)* - The most popular dimensionality reduction algorithm, that identifies the hyperplane closest to the data and then projects the data onto it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.3.1 Preserving the Variance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "When projecting down onto a lower-dimensional subspace, it is best to select the axis that preserves the maximum amount of variance, as it will most likely lose the least information.\n",
    "\n",
    "> Recall: Variance is the spread of the dataset. So maximizing its variance preserves the looks or characteristics of the original dataset.\n",
    "\n",
    "It can  also be thought of as selecting an axis that minimizes the mean squared distance between the original dataset and its projection onto that axis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.3.2 Principal Components"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The $i^{th}$ axis is called the *$i^{th}$ principal component (PC)*.  \n",
    "To find the principal components, use Singular Value Decomposition (SVD) to decompose the training set matrix **X** into $ \\mathbf{U} \\times \\mathbf{\\sum} \\times \\mathbf{V^T} $ , where $ \\mathbf{V} $ contains the unit vectors that define the principal components."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM ACCOMPANYING NOTEBOOK\n",
    "\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X - X.mean(axis=0) # Center dataset around origin\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0] # First PC\n",
    "c2 = Vt.T[:, 1] # Second PC"
   ]
  },
  {
   "source": [
    "> Note: PCA assumes the dataset is centered around the origin. Scikit-Learn's `PCA` does this for you. Make sure to center the data beforehand, otherwise."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.3.3 Projecting Down to d Dimensions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "You can reduce the dimensionality of the dataset down to *d* dimensions by projecting it onto the hyperplane defined by the first *d* principal components.\n",
    "\n",
    "To do this, take the training set and matrix multiply it with the matrix containing the first *d* columns of $ \\mathbf{V}$. Other words,\n",
    "\n",
    "*Equation 8-2. Projecting the training set down to d dimensions*\n",
    "\n",
    "$$ \\mathbf{X}_{d-proj} = \\mathbf{X W}_d $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Vt.T[:, 2]\n",
    "X2D = X_centered @ W2 # (@ or matmul) is preferred over .dot for \n",
    "                      # matrix multiplication"
   ]
  },
  {
   "source": [
    "### 8.3.4 Using Scikit-Learn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Scikit-Learn's `PCA` class uses SVD decomposition to implement PCA as well as centering the data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "source": [
    "`components_` attribute holds the transpose of $\\mathbf{W}_d$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.93636116, -0.29854881, -0.18465208],\n",
       "       [ 0.34027485, -0.90119108, -0.2684542 ]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-0.93636116, -0.29854881, -0.18465208])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "pca.components_.T[:, 0] # First PC"
   ]
  },
  {
   "source": [
    "### 8.3.5 Explained Variance Ratio"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The *explained variance ratio* indicates the proportion of the dataset's variance lies along each principal component, via `explained_variance_ratio` variable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.84248607, 0.14631839])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "source": [
    "84.2% lies along the first PC.  \n",
    "14.6% lies along the second PC.  \n",
    "<1.2% lies along the third PC => probably carries little information."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.3.6 Choosing the Right Number of Dimensions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Alternatively, instead of choosing a particular number of dimensions to reduce to, choose the number of dimensions that add up to a sufficiently large proportion of the variance (eg. 95%)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM TEXTBOOK NOTEBOOK\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.target = mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM TEXTBOOK NOTEBOOK\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "d"
   ]
  },
  {
   "source": [
    "Now we can set `n_components=d` and run PCA again.\n",
    "\n",
    "Another (better) option is to set `n_components=` float between 0.0 and 1.0, indicating the ratio of variance to preserve."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before (52500, 784)\nAfter being reduced (52500, 154)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "print('Before', X_train.shape)\n",
    "print('After being reduced', X_reduced.shape)"
   ]
  },
  {
   "source": [
    "Another option is to plot the explained variance as a function of number of dimensions (ie. plot `cumsum`). There will usually be an elbow in the curve, and so pick the number of dimensions at which the curve stops growing.\n",
    "\n",
    "> Note: See Figure 8-8 in book for pictorial reference."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.3.7 PCA for Compression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "As we saw, PCA reduced MNIST dataset from 784 -> 154 features (now less than 20% of original size).\n",
    "\n",
    "It is possible to decompress the dataset back to 784 dimensions but it won't be a perfect reconstruction due to information loss from the compression operation.\n",
    "\n",
    "*Reconstuction error* - The mean squared distance between the original data and the reconstructed data (compressed -> decompressed)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "source": [
    "> Note: See Figure 8-9 in book for comparison between original images and reconstructed images.\n",
    "\n",
    "*Equation 8-3. PCA inverse transformation, back to original d-dimensions*\n",
    "\n",
    "$$ \\mathbf{X}_{recovered} = \\mathbf{X}_{d-proj} \\mathbf{W}_d^T $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.3.8 Randomized PCA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "If you set `svd_solver=\"randomized\"`, Scikit-Learn uses a stochastic algorithm called *Randomized PCA* that quickly finds an approximation of the first *d* principal components.\n",
    "\n",
    "Computational complexity:  \n",
    "$ O(m \\times d^2) + O(d^3) $ instead of $ O(m \\times n^2) + O(n^3) $, where $d<n$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "source": [
    "By default, `svd_solver=\"auto\"`, meaning Scikit-Learn uses randomized PCA if m or n > 500 and d < 80% of m or n. It uses full SVD otherwise.\n",
    "\n",
    "Set `svd_solver=\"full\"` to force Scikit-Learn to use full SVD."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 8.3.9 Incremental PCA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Our current PCA approach requires whole training set to fit in memory. *Incremental PCA (IPCA)* algorithms allow you to split the training set into mini-batches and feed an IPCA algorithm one mini-batch at a time.\n",
    "\n",
    "- NumPy's `array_split()` splits the MNIST dataset into 100 mini-batches.  \n",
    "- Scikit-Learn's `IncrementalPCA` reduces the dimensionality to 154.  \n",
    "- `partial_fit()` can be called for each mini-batch instead of `fit()` for whole training set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "    \n",
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "source": [
    "Alternatively you can use NumPy's `memmap` class which allow you to manipulate a large array stored in a binary file on disk as if it were entirely in memory (ie. the class loads only the data it needs in memory when it needs it).\n",
    "\n",
    "Using this approach, you can call the usual `fit()` method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM TEXTBOOK NOTEBOOK\n",
    "\n",
    "filename = \"my_mnist.data\"\n",
    "m, n = X_train.shape\n",
    "\n",
    "X_mm = np.memmap(filename, dtype='float32', mode='write', shape=(m, n))\n",
    "X_mm[:] = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "IncrementalPCA(batch_size=525, n_components=154)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
    "\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "source": [
    "## 8.4 Kernel PCA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> Recall: Support Vector Machine's kernel trick in which it implicitly maps instances into a very high dimensional space, enabling nonlinear classification and regression.\n",
    "\n",
    "*Kernel PCA (kPCA)* - Applying the same trick as with SVM's kernel trick to perform complex nonlinear projections for dimensionality reduction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM TEXTBOOK NOTEBOOK\n",
    "\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "y = t > 6.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "source": [
    "### 8.4.1 Selecting a Kernel and Tuning Hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "kPCA is an unsupervised learning algorithm, so there is no obvious performance measure to help select the best kernel and hyperparameter values.\n",
    "\n",
    "But since dimensionality reduction is often used as preparation step before supervised learning, you can still use grid search to select the best kernel and hyperparameters that leads to best performance on the supervised task."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    (\"kpca\", KernelPCA(n_components=2)),\n",
    "    (\"log_reg\", LogisticRegression())\n",
    "])\n",
    "param_grid = [{\n",
    "    \"kpca__gamma\": np.linspace(0.03, 0.05, 10), # NOTE: double underscore __\n",
    "    \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    "}]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "source": [
    "Another approach is to select the kernel and hyperparameters that yield the lowest reconstruction error.\n",
    "\n",
    "This is much harder than with linear PCA. The kernel trick transformation is equivalent to using the *feature map* $ \\varphi$ to map the training set to an $ \\infty$-dimensional feature space, then projecting the transformed training set down to 2D using linear PCA.\n",
    "\n",
    "> Note: If we reverse linear PCA step, the reconstructed instance is in the $ \\infty$-dimensional feature space, not in the original space, and therefore cannot compute the reconstruction error.\n",
    "\n",
    "But it is possible to find a point (called *\"reconstruction pre-image\"*) in the original space that would map close to the reconstructed point. Then measure its squared distance from the original instance, and select the best kernels/hyperparameters that minimize this.\n",
    "\n",
    "We can do this by training a supervised regression model, with the projected instances as the training set and the original instances as the targets and using `fit_inverse_transform=True`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.0433,\n",
    "                    fit_inverse_transform=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7.642296115403016e-27"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "mean_squared_error(X, X_preimage)"
   ]
  },
  {
   "source": [
    "## 8.5 LLE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.6 Other Dimensionality Reduction Techniques"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}