{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "54b71c1b326a101c02c54fb24502b2663857f505f7b72a06051ccceba068a4ff"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 4: Training Models Exercises"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.\n",
    "\n",
    "> Which Linear Regression training algorithm can you use if you have a training set with millions of features?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Use Gradient Descent algorithm for training sets with millions of features. It's slower for many training examples but still performs well on many features."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.\n",
    "\n",
    "> Suppose the features in your training set have very different scales.\n",
    "\n",
    "> Which algorithms might suffer from this, and how?\n",
    "\n",
    "> What can you do about it?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Gradient Descent algorithms may suffer from this by having to take many steps down a \"flat valley.\" Also any regularization algorithms such as Ridge Regression as they are sensitive to scale of input features.\n",
    "\n",
    "Make sure all features have a similar scale by using Scikit-Learn's `StandardScaler` so each feature has mean, $\\mu=0$ and variance, $\\sigma^2=1$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.\n",
    "\n",
    "> Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "When training a Logistic Regression model, Gradient Descent can never get stuck at a local minimum as its cost function is convex. Therefore, there is no local minimum, only its global minimum."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.\n",
    "\n",
    "> Do all Gradient Descent algorithms lead to the same model, provided you let them run long enough?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Not all Gradient Descent algorithms lead to the same model. One important factor is its learning rate; if the learning rate hyperparameter is too large, the model may actually pass the minimum and diverge.\n",
    "\n",
    "Batch Gradient Descent will always find a local minimum given enough time, but may miss the global minimum. On the other hand, Stochastic Gradient Descent can jump out of the local min and get closer to the global min. but never stops at the global minimum due to randomness."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.\n",
    "\n",
    "> Suppose you use Batch Gradient Descent and you plot the validation error at every epoch.\n",
    "\n",
    "> If you notice that the validation error consistently goes up, what is likely going on?\n",
    "\n",
    "> How can you fix this?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "If the validation error consistently goes up, it means it is overfitting the training data too much. It can be fixed by decreasing the degrees of freedom or constraining the weights with regularization."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.\n",
    "\n",
    "> Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "No, it is not a good idea to immediately stop Mini-batch Gradient Descent when the validation error goes up, because due to its randomness, the error may fluctuate up and down. Only stop when the error stops decreasing after a while."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.\n",
    "\n",
    "> Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest?  \n",
    "\n",
    "> Which will actually converge?  \n",
    "\n",
    "> How can you make the others converge as well?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Stochastic Gradient Descent will reach the vicinity of the optimal solution the fastest.\n",
    "\n",
    "Batch Gradient Descent is the only one that will converge.\n",
    "\n",
    "Stochastic and Mini-Batch Gradient Descent can converge if the learning schedule is gradually reduced."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.\n",
    "\n",
    "> Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error.  \n",
    "\n",
    ">What is happening?  \n",
    "\n",
    ">What are three ways to solve this?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The model is performing much better on the training set than the validation set, meaning overfitting.\n",
    "\n",
    "Three ways to solve this are:\n",
    "- Add more training data\n",
    "- Reduce the model's complexity by constraining its degrees of freedom\n",
    "- Add regularization to the weights"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 9.\n",
    "\n",
    "> Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high.  \n",
    "\n",
    "> Would you say that the model suffers from high bias or high variance?  \n",
    "\n",
    "> Should you increase the regularization hyperparameter $\\alpha$ or reduce it?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The model suffers from high bias, and so the regularization hyperparameter $\\alpha$ should be reduced."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 10.\n",
    "\n",
    "> Why would you want to use:\n",
    "\n",
    ">> a. Ridge Regression instead of plain Linear Regression (ie, without any regularization?)  \n",
    ">>\n",
    ">> b. Lasso instead of Ridge Regression?  \n",
    ">>\n",
    ">> c. Elastic Net instead of Lasso?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "a. Ridge Regression should be used instead of plain Linear Regression because it is always better to have a little bit of regularization than to have none.\n",
    "\n",
    "b. Lasso should be used instead of Ridge when there are only a few useful features.\n",
    "\n",
    "c. Elastic Net should be used instead of Lasso because Lasso can behave erratically when the number of features is greater than the training instances or when the features are strongly correlated."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 11.\n",
    "\n",
    "> Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.\n",
    "\n",
    "> Should you implement two Logistic Regression classifiers or one Softmax Regression classifier?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Implement one Softmax Regression classifier because it has multiclass output, and the classes (location and time of day) are mutually exclusive - outdoor or indoor, daytime or nighttime."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 12.\n",
    "\n",
    "> Implement Batch Gradient Descent with early stopping for Softmax Regression (without using Scikit-Learn)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}