{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "7b58ff03a81b7b220740b362f01ec4719380627cca970f4b363191d14b8c12ac"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 10: Introduction to Artificial Neural Networks with Keras Exercises"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.\n",
    "\n",
    "> The TensorFlow Playground is a handy neural network simulator built by the TensorFlow team. \n",
    "\n",
    "> In this exercise, you will train several binary classifiers in just a few clicks, and tweak the model's architecture and its hyperparameters to gain some intuition on how neural networks work and what their hyperparameters do. Take some time to explore the following:\n",
    "\n",
    ">> a. The patterns learned by a neural net.\n",
    ">>\n",
    ">>> - Try training the default neural network by clicking the Run button (top left).\n",
    ">>>\n",
    ">>> - Notice how it quickly finds a good solution for the classification task.\n",
    ">>>\n",
    ">>> - The neurons in the first hidden layer have learned simple patterns, while the neurons in the second hidden layer have learned to combine the simple patterns of the first hidden layer into more complex patterns.\n",
    ">>>\n",
    ">>> - In general, the more layers there are, the more complex the patterns can be.\n",
    ">>\n",
    ">> b. Activation functions.\n",
    ">>\n",
    ">>> - Try replacing the tanh activation function with a ReLU activation function, and train the network again.\n",
    ">>>\n",
    ">>> - Notice that it finds a solution even faster, but this time the boundaries are linear. This is due to the shape of the ReLU function.\n",
    ">>\n",
    ">> c. The risk of local minima.\n",
    ">>\n",
    ">>> - Modify the network architecture to have just one hidden layer with three neurons.\n",
    ">>>\n",
    ">>> - Train it multiple times (to reset the network weights, click the Reset button next to the Play button).\n",
    ">>>\n",
    ">>> - Notice that the training time varies a lot, and sometimes it even gets stuck in a local minimum.\n",
    ">>\n",
    ">> d. What happens when neural nets are too small.\n",
    ">>\n",
    ">>> - Remove one neuron to keep just two.\n",
    ">>>\n",
    ">>> - Notice that the neural network is now incapable of finding a good solution, even if you try multiple times.\n",
    ">>>\n",
    ">>> - The model has too few parameters and systematically underfits the training set.\n",
    ">>\n",
    ">> e. What happens when neural nets are large enough.\n",
    ">>\n",
    ">>> - Set the number of neurons to eight, and train the network several times.\n",
    ">>>\n",
    ">>> - Notice that it is now consistently fast and never gets stuck.\n",
    ">>>\n",
    ">>> - This highlights an important finding in neural network theory: large neural networks almost never get stuck in local minima, and even when they do these local optima are almost as good as the global optimum.\n",
    ">>>\n",
    ">>> - However, they can still get stuck on long plateaus for a long time.\n",
    ">>\n",
    ">> f. The risk of vanishing gradients in deep networks.\n",
    ">>\n",
    ">>> - Select the spiral dataset (the bottom-right dataset under \"DATA\"), and change the network architecture to have four hidden layers with eight neurons each.\n",
    ">>>\n",
    ">>> - Notice that training takes much longer and often gets stuck on plateaus for long periods of time.\n",
    ">>>\n",
    ">>> - Also notice that the neurons in the highest layers (on the right) tend to evolve faster than the neurons in the lowest layers (on the left).\n",
    ">>>\n",
    ">>> - This problem, called the \"vanishing gradients\" problem, can be alleviated with better weight initialization and other techniques, better optimizers (such as AdaGrad or Adam), or Batch Normalization (discussed in Chapter 11).\n",
    ">>\n",
    ">> g. Go further.\n",
    ">>\n",
    ">>> - Take an hour or so to play around with other parameters and get a feel for what they do, to build an intuitive understand about neural networks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "a.\n",
    "- I can see that for the 1st hidden layer, the patterns learned are linear decision boundaries.\n",
    "- For the 2nd hidden layer, the decision boundaries are more complicated.\n",
    "\n",
    "b.\n",
    "- Switching to ReLU, I can see that it converges much faster than tanh (lower number of epochs).\n",
    "- The boundaries are straight lines (a hexagon decision boundary) compared to tanh's circular decision boundary.\n",
    "\n",
    "c.\n",
    "- I noticed that ReLU tended to get stuck on a local minimum much more than using tanh.\n",
    "- When using tanh, it would get stuck on a local minimum for a little bit but eventually converge correctly.\n",
    "\n",
    "d.\n",
    "- I see that by decreasing the number of neurons to just 2, the neural network fails to find a good decision boundary (with test loss at about 0.24 vs. 0.005).\n",
    "- It is underfitting the training set.\n",
    "\n",
    "e.\n",
    "- With 8 neurons, the neural network never gets stuck at a local optimum and always converges.\n",
    "\n",
    "f.\n",
    "- With 4 hidden layers of 8 neurons each, training takes a significantly longer time.\n",
    "- I also see that it tends to get stuck on plateaus a lot more often.\n",
    "\n",
    "g.\n",
    "- I can see how the too large of a learning rate can cause the neural network to diverge.\n",
    "- And too small of a learning rate takes it forever to converge.\n",
    "- Despite having many layers (6), the neural network can still fail to converge if there's just not enough neurons (1 per layer)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.\n",
    "\n",
    "> - Draw an ANN using the original artificial neurons (like the ones in Figure 10-3 \"logic operations\") that computes $ A \\oplus B $ (where $\\oplus$ represents the XOR operation).\n",
    "\n",
    ">> Hint: $ A \\oplus B = (A \\wedge \\neg B \\lor (\\neg A \\wedge B)$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.\n",
    "\n",
    "> - Why is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (ie. a single layer of threshold logic units trained using the Perceptron training algorithm)?\n",
    "\n",
    "> - How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.\n",
    "\n",
    "> - Why was the logistic activation function a key ingredient in training the first MLPs?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.\n",
    "\n",
    "> - Name three popular activation functions.\n",
    "\n",
    "> - Can you draw them?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.\n",
    "\n",
    "> Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "\n",
    "> - What is the shape of the input matrix $\\mathbf{X}$?\n",
    "\n",
    "> - What are the shapes of the hidden layer's weight vector $\\mathbf{W}_h$ and its bias vector $\\mathbf{b}_h$?\n",
    "\n",
    "> - What are the shapes of the output layer's weight vector $\\mathbf{W}_o$ and its bias vector $\\mathbf{b}_o$?\n",
    "\n",
    "> - What is the shape of the network's output matrix $\\mathbf{Y}$?\n",
    "\n",
    "> - Write the equation that computes the network's output matrix $\\mathbf{Y}$ as a function of $\\mathbf{X}$, $\\mathbf{W}_h$, $\\mathbf{b}_h$, $\\mathbf{W}_o$, and $\\mathbf{b}_o$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.\n",
    "\n",
    "> - How many neurons do you need in the output layer if you want to classify email into spam or ham?\n",
    "\n",
    "> - What activation function should you use in the output layer?\n",
    "\n",
    "> - If instead you want to tackle MNIST, how many neurons do you need in the output layer?\n",
    "\n",
    "> - Which activation function should you use?\n",
    "\n",
    "> - What about for getting your network to predict housing prices, as in Chapter 2?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.\n",
    "\n",
    "> - What is backpropagation and how does it work?\n",
    "\n",
    "> - What is the difference between backpropagation and reverse-mode autodiff?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 9.\n",
    "\n",
    "> - Can you list all the hyperparameters you can tweak in a basic MLP?\n",
    "\n",
    "> - If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 10.\n",
    "\n",
    "> 1. Train a deep MLP on the MNIST dataset.\n",
    "\n",
    ">> You can load it using `keras.datasets.mnist.load_data()`.\n",
    "\n",
    "> 2. See if you can get over 98% precision.\n",
    "\n",
    "> 3. Try searching for the optimal learning rate by using the approach presented in this chapter.\n",
    "\n",
    ">> ie. by growing the learning rate exponentially, plotting the loss, and finding the point where the loss shoots up.\n",
    "\n",
    "> 4. Try adding all the bells and whistles - save checkpoints, use early stopping, and plot learning curves using TensorBoard."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}