{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "7b58ff03a81b7b220740b362f01ec4719380627cca970f4b363191d14b8c12ac"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 8: Dimensionality Reduction Exercises"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.\n",
    "\n",
    "> What are the main motivations for reducing a dataset's dimensionality?\n",
    "\n",
    "> What are the main drawbacks?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.\n",
    "\n",
    "> What is the curse of dimensionality?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.\n",
    "\n",
    "> Once a dataset's dimensionality has been reduced, is it possible to reverse the operation?\n",
    "\n",
    "> If so, how? If not, why?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.\n",
    "\n",
    "> Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.\n",
    "\n",
    "> Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%.\n",
    "\n",
    "> How many dimensions will the resulting dataset have?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.\n",
    "\n",
    "> In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA, or Kernel PCA?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.\n",
    "\n",
    "> How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.\n",
    "\n",
    "> Does it make any sense to chain two different dimensionality reduction algorithms?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 9.\n",
    "\n",
    "> 1. Load the MNIST dataset (introduced in Chapter 3) and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing).\n",
    "\n",
    "> 2. Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set.\n",
    "\n",
    "> 3. Next, use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%.\n",
    "\n",
    "> 4. Train a new Random Forest classifier on the reduced dataset and see how long it takes.\n",
    "\n",
    "> 5. Was training much faster?\n",
    "\n",
    "> 6. Next, evaluate the classifier on the test set.\n",
    "\n",
    "> 7. How does it compare to the previous classifier?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 10.\n",
    "\n",
    "> 1. Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the result using Matplotlib.\n",
    "\n",
    "> 2. You can use a scatterplot using 10 different colors to represent each image's target class.\n",
    "\n",
    "> 3. Alternatively, you can replace each dot in the scatterplot with the corresponding instance's class (a digit from 0 to 9).\n",
    "\n",
    "> 4. Or even plot scaled-down versions of the digit images themselves.\n",
    "    >> Note: If you plot all digits, the visualization will be too cluttered, so you should either:  \n",
    "    >> - Draw a random sample.\n",
    "    >>\n",
    "    >> - Or plot an instance only if no other instance has already been plotted at a close distance.\n",
    "\n",
    "> 5. You should get a nice visualization with well-separated clusters of digits.\n",
    "\n",
    "> 6. Try using other dimensionality reduction algorithms such as PCA, LLE, or MDS and compare the resulting visualizations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}