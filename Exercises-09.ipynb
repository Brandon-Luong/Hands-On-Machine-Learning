{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "7b58ff03a81b7b220740b362f01ec4719380627cca970f4b363191d14b8c12ac"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 9: Unsupervised Learning Techniques Exercises"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.\n",
    "\n",
    "> How would you define clustering?\n",
    "\n",
    "> Can you name a few clustering algorithms?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Clustering is the task of identifying similar instances and assigning them to clusters, or groups of similar instances.\n",
    "\n",
    "K-Means and DBSCAN are the two popular clustering algorithms."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.\n",
    "\n",
    "> What are some of the main applications of clustering algorithms?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Some of the main applications of clustering algorithms are:\n",
    "- Customer segmentation\n",
    "- Data analysis\n",
    "- Dimensionality reduction technique\n",
    "- Anomaly detection\n",
    "- Semi-supervised learning\n",
    "- Search engines\n",
    "- Image segmentation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.\n",
    "\n",
    "> Describe two techniques to select the right number of clusters when using K-Means."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To select the right number of clusters when using K-Means, we can use:\n",
    "- The model's **inertia**: the mean squared distance between each instance and its closest centroid.\n",
    "    - Plot the inertia as a function of the number of clusters *k*.\n",
    "    - Find the curve's \"elbow\" in which the inertia drop off changes from rapid to gradual decrease.\n",
    "    - The inertia will always decrease as *k* clusters increases, so you can't simply pick the one with lowest inertia.\n",
    "\n",
    "- The model's **silhouette score**: the mean silhouette coefficient over all the instances.\n",
    "    - Since the silhouette coefficient is a score telling how well an instance belongs to its own cluster, \n",
    "        - $\\approx +1$ => it's well in its own cluster, \n",
    "        - $\\approx 0 $ => it's close to a cluster boundary,\n",
    "        - $\\approx -1$ => it may have been assigned to wrong cluster.\n",
    "    - Plot the silhouette score as a function of number of clusters *k*.\n",
    "    - Pick the cluster number with the highest score \n",
    "        - => highest amount of instances belonging to its own cluster."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.\n",
    "\n",
    "> What is label propagation?\n",
    "\n",
    "> Why would you implement it, and how?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Label propagation is propagating the labels to all the other instances in the same cluster.\n",
    "\n",
    "This is useful for semi-supervised learning in which you have some labeled instances but many unlabeled instances. By using label propagation, you increase the number of labeled instances and improve the performance of the algorithm.\n",
    "\n",
    "To do this,\n",
    "\n",
    "1. Cluster the training set.\n",
    "2. For each cluster, find the instance closest to the centroid - the \"representative instance\".\n",
    "3. Manually label these representative instances (now 100% accurate).\n",
    "4. Label all other instances in the same cluster the same as representative instance.\n",
    "5. Or, only label a percentage of the instances as the same - reason being that instances at cluster boundaries are probably mislabeled into the wrong cluster."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.\n",
    "\n",
    "> Can you name two clustering algorithms that can scale to large datasets?\n",
    "\n",
    "> And two that look for regions of high density?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Two clustering algorithms that can scale to large datasets are:\n",
    "- Mini-batch K-Means:\n",
    "    - With caveat that data must have a clustering structure.\n",
    "    - Uses mini-batches, moving centroids slightly at each iteration.\n",
    "    - Capable of clustering huge datasets that don't fit into memory.\n",
    "    \n",
    "- BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies):\n",
    "    - With caveat that (n features < 20).\n",
    "    - Builds tree structure with just enough information to quickly assign each new instance to a cluster.\n",
    "    - Does not have to store all instances in the tree.\n",
    "    - Uses limited memory, while handling huge datasets.\n",
    "\n",
    "Two clustering algorithms that look for regions of high density are:\n",
    "- DBSCAN:\n",
    "    - Counts how many instances are located within a small distance $\\epsilon$.\n",
    "    - If an instance has more than the minimum instances in the $\\epsilon$-neighborhood, it is labeled a core instance. \n",
    "    - Any instance that is not a core instance or does not have any in its neighborhood is considered an anomaly.\n",
    "\n",
    "- Mean-Shift:\n",
    "    - Places a circle centered on each instance.\n",
    "    - For each circle, computes the mean of all instances located within it.\n",
    "    - Shifts the circle so that it's centered on the mean.\n",
    "    - It shifts in direction of higher density."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.\n",
    "\n",
    "> Can you think of a use case where active learning would be useful?\n",
    "\n",
    "> How would you implement it?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Active learning would be helpful in semi-supervised learning where there's some labeled instances but many unlabeled instances such as handwritten digits in MNIST dataset, assuming each digit is only labeled once. \n",
    "\n",
    "Since each digit can be written in many ways, you would train the model as usual. Then for the instances that have the lowest probability, have them manually labeled. And then repeat the process until the improvements are miniscule compared to the labeling effort."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.\n",
    "\n",
    "> What is the difference between anomaly detection and novelty detection?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The difference between anomaly and novelty detection is that novelty detection is assumed to be trained on a \"clean\" dataset with no outliers, whereas anomaly detection does not make this assumption."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.\n",
    "\n",
    "> What is a Gaussian mixture?\n",
    "\n",
    "> What tasks can you use it for?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A Gaussian mixture is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown.\n",
    "\n",
    "They can be used for density estimation, clustering, and anomaly detection."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 9.\n",
    "\n",
    "> Can you name two techniques to find the right number of clusters when using a Gaussian mixture model?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Two techniques to find the right number of clusters when using a Gaussian mixture model are:\n",
    "- Minimizing the theoretical information criterion, Bayesian (BIC) or Akaike (AIC):\n",
    "    - Must compute the maximum value of the likelihood function of the model.\n",
    "    - Plot the BIC/AIC as a function of number of clusters.\n",
    "    - Pick the cluster number that gives lowest BIC/AIC.\n",
    "\n",
    "- Running Bayesian Gaussian Mixture model to automatically find the number:\n",
    "    - Use `BayesianGaussianMixture` class.\n",
    "    - Must set number of initial clusters to be greater than ideal number.\n",
    "    - Gives weights of 0 to unnecessary clusters.\n",
    "    - Cluster parameters (weights, means, covariance matrices) are treated as latent random variables.\n",
    "    - Uses Bayes' theorem to update probability distribution."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 10.\n",
    "\n",
    "> The classic Olivetti faces dataset contains 400 grayscale 64x64-pixel images of faces. Each image is flattened to a 1D vector of size 4,096. 40 different people were photographed (10 times each).\n",
    "\n",
    "> And the usual task is to train a model that can predict which person is represented in each picture.\n",
    "\n",
    "> 1. Load the dataset using the `sklearn.datasets.fetch_olivetti_faces()` function.\n",
    "\n",
    "> 2. Then split it into a training set, a validation set, and a test set.\n",
    "\n",
    ">> Note: The dataset is already scaled between 0 and 1.\n",
    "\n",
    "> 3. Since the dataset is quite small, you probably want to use stratified sampling to ensure that there are the same number of images per person in each set.\n",
    "\n",
    "> 4. Next, cluster the images using K-Means.\n",
    "\n",
    "> 5. Ensure that you have a good number of clusters (using one of the techniques discussed in this chapter).\n",
    "\n",
    "> 6. Visualize the clusters.\n",
    "\n",
    "> Do you see similar faces in each cluster?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 11.\n",
    "\n",
    "> Continuing with the Olivetti faces dataset,\n",
    "\n",
    "> 1. Train a classifier to predict which person is represented in each picture.\n",
    "\n",
    "> 2. Evaluate it on the validation set.\n",
    "\n",
    "> 3. Next, use K-Means as a dimensionality reduction tool.\n",
    "\n",
    "> 4. Train a classifier on the reduced set.\n",
    "\n",
    "> 5. Search for the number of clusters that allows the classifier to get the best performance.\n",
    "\n",
    "> What performance can you reach?\n",
    "\n",
    "> What if you append the features from the reduced set to the original features (again, searching for the best number of clusters)?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 12.\n",
    "\n",
    "> 1. Train a Gaussian mixture model on the Olivetti faces dataset.\n",
    "\n",
    "> 2. To speed up the algorithm, you should probably reduce the dataset's dimensionality (eg. use PCA, preserving 99% of the variance).\n",
    "\n",
    "> 3. Use the model to generate some new faces (using the `sample()` method).\n",
    "\n",
    "> 4. Visualize them (if you used PCA, you will need to use its `inverse_transform()` method).\n",
    "\n",
    "> 5. Try to modify some images (eg. rotate, flip, darken).\n",
    "\n",
    "> 6. See if the model can detect the anomalies (ie. compare the output of the `score_samples()` method for normal images and for anomalies)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 13.\n",
    "\n",
    "> Some dimensionality reduction techniques can also be used for anomaly detection. For example,\n",
    "\n",
    "> 1. Take the Olivetti faces dataset and reduce it with PCA, preserving 99% of the variance.\n",
    "\n",
    "> 2. Then compute the reconstruction error for each image.\n",
    "\n",
    "> 3. Next, take some of the modified images you built in the previous exercise, and look at their reconstruction error.\n",
    "\n",
    "> 4. Notice how much large the reconstruction error is.\n",
    "\n",
    "> 5. If you plot a reconstructed image, you will see why: it tries to reconstruct a normal face."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}