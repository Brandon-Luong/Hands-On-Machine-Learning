{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "7b58ff03a81b7b220740b362f01ec4719380627cca970f4b363191d14b8c12ac"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 9: Unsupervised Learning Techniques Exercises"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.\n",
    "\n",
    "> How would you define clustering?\n",
    "\n",
    "> Can you name a few clustering algorithms?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.\n",
    "\n",
    "> What are some of the main applications of clustering algorithms?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.\n",
    "\n",
    "> Describe two techniques to select the right number of clusters when using K-Means."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.\n",
    "\n",
    "> What is label propagation?\n",
    "\n",
    "> Why would you implement it, and how?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.\n",
    "\n",
    "> Can you name two clustering algorithms that can scale to large datasets?\n",
    "\n",
    "> And two that look for regions of high density?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.\n",
    "\n",
    "> Can you think of a use case where active learning would be useful?\n",
    "\n",
    "> How would you implement it?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.\n",
    "\n",
    "What is the difference between anomaly detection and novelty detection?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 8.\n",
    "\n",
    "> What is a Gaussian mixture?\n",
    "\n",
    "> What tasks can you use it for?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 9.\n",
    "\n",
    "> Can you name two techniques to find the right number of clusters when using a Gaussian mixture model?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 10.\n",
    "\n",
    "> The classic Olivetti faces dataset contains 400 grayscale 64x64-pixel images of faces. Each image is flattened to a 1D vector of size 4,096. 40 different people were photographed (10 times each).\n",
    "\n",
    "> And the usual task is to train a model that can predict which person is represented in each picture.\n",
    "\n",
    "> 1. Load the dataset using the `sklearn.datasets.fetch_olivetti_faces()` function.\n",
    "\n",
    "> 2. Then split it into a training set, a validation set, and a test set.\n",
    "\n",
    ">> Note: The dataset is already scaled between 0 and 1.\n",
    "\n",
    "> 3. Since the dataset is quite small, you probably want to use stratified sampling to ensure that there are the same number of images per person in each set.\n",
    "\n",
    "> 4. Next, cluster the images using K-Means.\n",
    "\n",
    "> 5. Ensure that you have a good number of clusters (using one of the techniques discussed in this chapter).\n",
    "\n",
    "> 6. Visualize the clusters.\n",
    "\n",
    "> Do you see similar faces in each cluster?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 11.\n",
    "\n",
    "> Continuing with the Olivetti faces dataset,\n",
    "\n",
    "> 1. Train a classifier to predict which person is represented in each picture.\n",
    "\n",
    "> 2. Evaluate it on the validation set.\n",
    "\n",
    "> 3. Next, use K-Means as a dimensionality reduction tool.\n",
    "\n",
    "> 4. Train a classifier on the reduced set.\n",
    "\n",
    "> 5. Search for the number of clusters that allows the classifier to get the best performance.\n",
    "\n",
    "> What performance can you reach?\n",
    "\n",
    "> What if you append the features from the reduced set to the original features (again, searching for the best number of clusters)?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 12.\n",
    "\n",
    "> 1. Train a Gaussian mixture model on the Olivetti faces dataset.\n",
    "\n",
    "> 2. To speed up the algorithm, you should probably reduce the dataset's dimensionality (eg. use PCA, preserving 99% of the variance).\n",
    "\n",
    "> 3. Use the model to generate some new faces (using the `sample()` method).\n",
    "\n",
    "> 4. Visualize them (if you used PCA, you will need to use its `inverse_transform()` method).\n",
    "\n",
    "> 5. Try to modify some images (eg. rotate, flip, darken).\n",
    "\n",
    "> 6. See if the model can detect the anomalies (ie. compare the output of the `score_samples()` method for normal images and for anomalies)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 13.\n",
    "\n",
    "> Some dimensionality reduction techniques can also be used for anomaly detection. For example,\n",
    "\n",
    "> 1. Take the Olivetti faces dataset and reduce it with PCA, preserving 99% of the variance.\n",
    "\n",
    "> 2. Then compute the reconstruction error for each image.\n",
    "\n",
    "> 3. Next, take some of the modified images you built in the previous exercise, and look at their reconstruction error.\n",
    "\n",
    "> 4. Notice how much large the reconstruction error is.\n",
    "\n",
    "> 5. If you plot a reconstructed image, you will see why: it tries to reconstruct a normal face."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}