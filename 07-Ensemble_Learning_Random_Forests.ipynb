{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "54b71c1b326a101c02c54fb24502b2663857f505f7b72a06051ccceba068a4ff"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 7: Ensemble Learning and Random Forests"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*Ensemble* - A group of predictors  \n",
    "*Ensemble Learning* - Technique of aggregating the predictions of a group of predictors  \n",
    "*Ensemble method* - An Ensemble Learning algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.1 Voting Classifiers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Suppose you have a few classifiers, each with 80% accuracy. A simple way to make a better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes.\n",
    "\n",
    "*Hard voting* classifier - Majority-vote classifier\n",
    "\n",
    "> Note: Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms, increasing chances of making different errors and thus improving the ensemble's accuracy.\n",
    "\n",
    "Let's take a look at moons dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "Now look at each classifier's accuracy on test set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "source": [
    "*Soft voting* - Predict the class with the highest class **probability**, averaged over all the individual classifiers.  \n",
    "It gives more weight to highly confident votes => higher performance than hard voting.\n",
    "\n",
    "> Note: Just replace `voting='hard'` with `voting='soft'` and ensure all classifiers can estimate class probabilities (eg. have `predict_proba()`)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.2 Bagging and Pasting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Another way is to train the same algorithm on different random subsets of training set.  \n",
    "\n",
    "*Bagging (bootstrap aggregating)* - When sampling is performed **with** replacement  \n",
    "*Pasting* - When sampling is performed **without** replacement\n",
    "\n",
    "The aggregation function is often the *statistical mode* - the most frequent prediction (classification) or average (regression).\n",
    "\n",
    "> Note: Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias and variance.\n",
    "\n",
    "Predictors for bagging/pasting can be trained in parallel => scales very well (ie. popular methods)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.2.1 Bagging and Pasting in Scikit-Learn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Scikit-Learn has `BaggingClassifier` and `BaggingRegressor`.\n",
    "\n",
    "> Note: `BaggingClassifier` automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "source": [
    "### 7.2.2 Out-of-Bag Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`BaggingClassifier` only samples about 63% of the training instances (when \n",
    "`bootstrap=True`, with replacement), so the remaining 37% are called *out-of-bag (oob)* instances.\n",
    "\n",
    "Set `oob_score=True` when creating a `BaggingClassifier` to request an automatic oob evaluation after training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "bag_clf.oob_decision_function_[:10] # Shorten output by taking 10 rows"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.4137931 , 0.5862069 ],\n",
       "       [0.37755102, 0.62244898],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00512821, 0.99487179],\n",
       "       [0.1185567 , 0.8814433 ],\n",
       "       [0.31188119, 0.68811881],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [0.98324022, 0.01675978],\n",
       "       [0.98543689, 0.01456311]])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "source": [
    "## 7.3 Random Patches and Random Subspaces"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`BaggingClassifier` can also sample the features too (`max_features` and `bootstrap_features`).\n",
    "\n",
    "*Random Patches* method - Sampling both training instances and features.  \n",
    "*Random Subspaces* method - Keeping all training instances (`bootstrap=False`, `max_samples=1.0`) and sampling the features (`bootstrap_features=True`, `max_features<1.0`)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.4 Random Forests"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A Random Forest is an ensemble of Decision Trees, generally trained with the bagging method. Use `RandomForestClassifier` (or `RandomForestRegressor`) instead of `BaggingClassifier` and `DecisionTreeClassifier` because the former is more optimized."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "source": [
    "Generally most of Decision Tree and Bagging hyperparameters are available to use - with some exceptions.\n",
    "\n",
    "Random Forests introduce extra randomness; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using these hyperparameters\n",
    "# Roughly equivalent to Random Forest Classifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "source": [
    "### 7.4.1 Extra-Trees"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*Extremely Randomized Trees (Extra-Trees)* ensemble - Using random thresholds for each feature rather than searching for the best possible thresholds.\n",
    "\n",
    "Use `ExtraTreesClassifier` (`ExtraTreesRegressor`) for Extra-Trees and they have the same hyperparameters as Random Forests.\n",
    "\n",
    "> Note: Training Extra-Trees is much faster because finding the best possible threshold is the most time-consuming task.\n",
    "\n",
    "> Note: Whether or not Extra-Trees perform better (eg. higher accuracy) than Random Forests is unclear. Best way is try both and compare their cross-validation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.4.2 Feature Importance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Scikit-Learn measures a feature's importance by looking at how much the tree nodes that use that feature reduce impurity (**recall: Gini impurity!**) on average (across all trees in the forest). It can be accessed using `feature_importances_`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sepal length (cm) 0.10149417945091706\nsepal width (cm) 0.02588642676804873\npetal length (cm) 0.4421831688641625\npetal width (cm) 0.4304362249168718\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "source": [
    "Random Forests are very handy to get a quick understanding of what features actually matter, in particular if you need to perform feature selection."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.5 Boosting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*Boosting (hypothesis boosting)* - Any Ensemble method that can combine several weak learners into a strong learner (ie. train predictors sequentially such that each corrects its predecessor).\n",
    "\n",
    "Two main popular methods of boosting:\n",
    "\n",
    "- *AdaBoost* (Adaptive Boosting)\n",
    "- *Gradient Boosting*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.5.1 AdaBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Adaboost pays a bit more attention to the training instances that the predecessor underfitted for each subsequent predictor.\n",
    "\n",
    "For example:\n",
    "\n",
    "1. Trains on a base classifier and makes its prediction. \n",
    "2. Increase the relative weight of the misclassified training instances.\n",
    "3. Trains a second classifier using the updated weights and makes prediction.\n",
    "4. Repeat\n",
    "\n",
    "> Note: SVMs are generally not good base predictors for AdaBoost; they are slow and tend to be unstable with it.\n",
    "\n",
    "Similar to Gradient Descent (converging), but instead of tweaking a single predictor's parameters to minimize cost function, AdaBoost adds predictors to the ensemble, gradually making it better.\n",
    "\n",
    "> Note: It cannot be parallelized (predictor 1 -> predictor 2; 2 can only be trained after 1 is trained), and as a result, does not scale well as bagging or pasting.\n",
    "\n",
    "Scikit-Learn uses a multiclass version of AdaBoost called *SAMME (Stagewise Additive Modeling using a Multiclass Exponential loss function)*. When there's only 2 classes, it is equivalent to AdaBoost.\n",
    "\n",
    "If predictors have `predict_proba()` use `SAMME.R` (\"R\"=Real), which relies on class probabilities and performs better.\n",
    "\n",
    "A *Decision Stump* is a Decision Tree with `max_depth=1`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "> Note: If AdaBoost ensemble is overfitting the training set, reduce the number of estimators or more strongly regularize the base estimator."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.5.2 Gradient Boosting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Similar to AdaBoost, *Gradient Boosting* sequentially adds predictors to an ensemble and correcting its predecessor.\n",
    "\n",
    "But instead of tweaking the instance weights, it fits the new predictor to the *residual errors* made by the previous predictor.\n",
    "\n",
    "(Gradient Boosting + Decision Trees + Regression) is called *Gradient Tree Boosting* or *Graident Boosted Regression Trees (GBRT)*.\n",
    "\n",
    "Now an example using a noisy quadratic training set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: COPIED FROM ACCOMPANYING JUPYTER NOTEBOOK\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)\n",
    "\n",
    "# Now train a second DecisionTreeRegressor on residual errors from first\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)\n",
    "\n",
    "# Now train a third regressor on residual errors from second\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)\n",
    "\n",
    "# NOTE: COPIED \"X_new\" FROM ACCOMPANYING NOTEBOOK\n",
    "X_new = np.array([[0.8]])\n",
    "\n",
    "# Make predictions by adding up predictions of all trees\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "y_pred"
   ]
  },
  {
   "source": [
    "> Note: See book's graphs to see how ensemble predictions take the sum of the individual predictor's errors.\n",
    "\n",
    "Another way to train GBRT ensembles is to use Scikit-Learn's `GradientBoostingRegressor` and has hyperparameters to control growth of Decision Trees (`max_depth`, `min_samples_leaf`) and of ensemble training (`n_estimators`)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "source": [
    "`learning_rate` scales the contribution of each tree. If set to low value (0.1), then you need more trees in ensemble to fit the training set, but predictions will generalize better.  \n",
    "=> This technique is called *shrinkage*.\n",
    "\n",
    "If there's not enough trees, => underfitting.\n",
    "If there's too many trees, => overfitting.\n",
    "\n",
    "Use early stopping to find optimal number of trees. Use `staged_predict()`: returns an iterator over the predictions made by the ensemble at each stage (with 1 tree, with 2 trees, etc.)\n",
    "\n",
    "Following example with train GBRT ensemble with 12 trees, measures validation error at each stage to find optimal number of trees. Then trains another GBRT with optimal number of trees."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=48)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) \n",
    "            for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "bst_n_estimators"
   ]
  },
  {
   "source": [
    "`warm_start=True` makes Scikit-Learn keep existing trees when `fit()` is called, allowing incremental training.\n",
    "\n",
    "Following example stops training when validation error does not improve for 5 iterations in a row."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break   # early stopping\n",
    "\n",
    "gbrt.n_estimators"
   ]
  },
  {
   "source": [
    "`GradientBoostingRegressor` also supports `subsample` which specifies the fraction of training instances to be used for training each tree (eg. `subsample=0.25` -> each tree is trained on 25% of the training instances, selected randomly).  \n",
    "=> This is called **Stochastic Gradient Boosting**.\n",
    "\n",
    "> Note: Gradient Boosting can be used with other cost functions, controlled by `loss` hyperparameter.\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is a popular Python library that is optimized for Gradient Boosting. XGBoost's API is similar to Scikit-Learn's.\n",
    "\n",
    "    import xgboost\n",
    "\n",
    "    xgb_reg = xgboost.XGBRegressor()\n",
    "    xgb_reg.fit(X_train, y_train)\n",
    "    y_pred = xgb_reg.predict(X_val)\n",
    "\n",
    "    # XGBoost can take care of early stopping\n",
    "\n",
    "    xgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)], eary_stopping_rounds=2)\n",
    "    y_pred = xgb_reg.predict(X_val)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.6 Stacking"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*Stacking (stacked generalization)* - Instead of using trivial function (eg. hard voting) to aggregate the predictions of all predictors in an ensemble, why don't we train a model to perform this aggregation?\n",
    "\n",
    "*Blender (meta learner)* - The final predictor used to predict the target value, using a hold-out set.\n",
    "\n",
    "For example,\n",
    "\n",
    "1. Split training set into subset 1 and subset 2\n",
    "2. Use subset 1 to train the predictors\n",
    "3. Use predictors to make predictions on subset 2\n",
    "4. Results are new \"clean\" predictions because they were never seen (held-out)\n",
    "5. Create new training set using these predicted values as input features\n",
    "6. Blender is trained on new training set and gives prediction\n",
    "\n",
    "> Note: Refer to book for detailed chart on stacking using a Blender.\n",
    "\n",
    "Scikit-Learn does not support stacking directly. You can use an open source implementation such as `DESlib` or make your own implementation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}