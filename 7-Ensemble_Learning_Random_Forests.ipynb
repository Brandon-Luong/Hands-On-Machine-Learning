{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "54b71c1b326a101c02c54fb24502b2663857f505f7b72a06051ccceba068a4ff"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 7: Ensemble Learning and Random Forests"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*Ensemble* - A group of predictors  \n",
    "*Ensemble Learning* - Technique of aggregating the predictions of a group of predictors  \n",
    "*Ensemble method* - An Ensemble Learning algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.1 Voting Classifiers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Suppose you have a few classifiers, each with 80% accuracy. A simple way to make a better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes.\n",
    "\n",
    "*Hard voting* classifier - Majority-vote classifier\n",
    "\n",
    "> Note: Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms, increasing chances of making different errors and thus improving the ensemble's accuracy.\n",
    "\n",
    "Let's take a look at moons dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "Now look at each classifier's accuracy on test set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "source": [
    "*Soft voting* - Predict the class with the highest class **probability**, averaged over all the individual classifiers.  \n",
    "It gives more weight to highly confident votes => higher performance than hard voting.\n",
    "\n",
    "> Note: Just replace `voting='hard'` with `voting='soft'` and ensure all classifiers can estimate class probabilities (eg. have `predict_proba()`)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.2 Bagging and Pasting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Another way is to train the same algorithm on different random subsets of training set.  \n",
    "\n",
    "*Bagging (bootstrap aggregating)* - When sampling is performed **with** replacement  \n",
    "*Pasting* - When sampling is performed **without** replacement\n",
    "\n",
    "The aggregation function is often the *statistical mode* - the most frequent prediction (classification) or average (regression).\n",
    "\n",
    "> Note: Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias and variance.\n",
    "\n",
    "Predictors for bagging/pasting can be trained in parallel => scales very well (ie. popular methods)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.2.1 Bagging and Pasting in Scikit-Learn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Scikit-Learn has `BaggingClassifier` and `BaggingRegressor`.\n",
    "\n",
    "> Note: `BaggingClassifier` automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "source": [
    "### 7.2.2 Out-of-Bag Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`BaggingClassifier` only samples about 63% of the training instances (when \n",
    "`bootstrap=True`, with replacement), so the remaining 37% are called *out-of-bag (oob)* instances.\n",
    "\n",
    "Set `oob_score=True` when creating a `BaggingClassifier` to request an automatic oob evaluation after training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.904"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "source": [
    "bag_clf.oob_decision_function_[:10] # Shorten output by taking 10 rows"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.4137931 , 0.5862069 ],\n",
       "       [0.37755102, 0.62244898],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00512821, 0.99487179],\n",
       "       [0.1185567 , 0.8814433 ],\n",
       "       [0.31188119, 0.68811881],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [0.98324022, 0.01675978],\n",
       "       [0.98543689, 0.01456311]])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "source": [
    "## 7.3 Random Patches and Random Subspaces"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`BaggingClassifier` can also sample the features too (`max_features` and `bootstrap_features`).\n",
    "\n",
    "*Random Patches* method - Sampling both training instances and features.  \n",
    "*Random Subspaces* method - Keeping all training instances (`bootstrap=False`, `max_samples=1.0`) and sampling the features (`bootstrap_features=True`, `max_features<1.0`)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.4 Random Forests"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A Random Forest is an ensemble of Decision Trees, generally trained with the bagging method. Use `RandomForestClassifier` (or `RandomForestRegressor`) instead of `BaggingClassifier` and `DecisionTreeClassifier` because the former is more optimized."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "source": [
    "Generally most of Decision Tree and Bagging hyperparameters are available to use - with some exceptions.\n",
    "\n",
    "Random Forests introduce extra randomness; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using these hyperparameters\n",
    "# Roughly equivalent to Random Forest Classifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "source": [
    "### 7.4.1 Extra-Trees"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*Extremely Randomized Trees (Extra-Trees)* ensemble - Using random thresholds for each feature rather than searching for the best possible thresholds.\n",
    "\n",
    "Use `ExtraTreesClassifier` (`ExtraTreesRegressor`) for Extra-Trees and they have the same hyperparameters as Random Forests.\n",
    "\n",
    "> Note: Training Extra-Trees is much faster because finding the best possible threshold is the most time-consuming task.\n",
    "\n",
    "> Note: Whether or not Extra-Trees perform better (eg. higher accuracy) than Random Forests is unclear. Best way is try both and compare their cross-validation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.4.2 Feature Importance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Scikit-Learn measures a feature's importance by looking at how much the tree nodes that use that feature reduce impurity (**recall: Gini impurity!**) on average (across all trees in the forest). It can be accessed using `feature_importances_`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sepal length (cm) 0.10149417945091706\nsepal width (cm) 0.02588642676804873\npetal length (cm) 0.4421831688641625\npetal width (cm) 0.4304362249168718\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "source": [
    "Random Forests are very handy to get a quick understanding of what features actually matter, in particular if you need to perform feature selection."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.5 Boosting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.5.1 AdaBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.5.2 Gradient Boosting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.6 Stacking"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}