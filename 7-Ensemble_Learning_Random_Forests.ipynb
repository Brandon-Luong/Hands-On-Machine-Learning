{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "54b71c1b326a101c02c54fb24502b2663857f505f7b72a06051ccceba068a4ff"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 7: Ensemble Learning and Random Forests"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*Ensemble* - A group of predictors  \n",
    "*Ensemble Learning* - Technique of aggregating the predictions of a group of predictors  \n",
    "*Ensemble method* - An Ensemble Learning algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.1 Voting Classifiers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Suppose you have a few classifiers, each with 80% accuracy. A simple way to make a better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes.\n",
    "\n",
    "*Hard voting* classifier - Majority-vote classifier\n",
    "\n",
    "> Note: Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms, increasing chances of making different errors and thus improving the ensemble's accuracy.\n",
    "\n",
    "Let's take a look at moons dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "source": [
    "Now look at each classifier's accuracy on test set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.896\n",
      "VotingClassifier 0.904\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "source": [
    "*Soft voting* - Predict the class with the highest class **probability**, averaged over all the individual classifiers.  \n",
    "It gives more weight to highly confident votes => higher performance than hard voting.\n",
    "\n",
    "> Note: Just replace `voting='hard'` with `voting='soft'` and ensure all classifiers can estimate class probabilities (eg. have `predict_proba()`)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.2 Bagging and Pasting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Another way is to train the same algorithm on different random subsets of training set.  \n",
    "\n",
    "*Bagging (bootstrap aggregating)* - When sampling is performed **with** replacement  \n",
    "*Pasting* - When sampling is performed **without** replacement\n",
    "\n",
    "The aggregation function is often the *statistical mode* - the most frequent prediction (classification) or average (regression).\n",
    "\n",
    "> Note: Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias and variance.\n",
    "\n",
    "Predictors for bagging/pasting can be trained in parallel => scales very well (ie. popular methods)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.2.1 Bagging and Pasting in Scikit-Learn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.2.2 Out-of-Bag Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.3 Random Patches and Random Subspaces"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.4 Random Forests"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.4.1 Extra-Trees"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.4.2 Feature Importance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.5 Boosting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.5.1 AdaBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 7.5.2 Gradient Boosting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 7.6 Stacking"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}