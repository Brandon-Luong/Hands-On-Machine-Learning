{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": 3
    },
    "orig_nbformat": 2,
    "colab": {
      "name": "14_Deep_Computer_Vision_Using_Convolutional_Neural_Networks.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hJVg8sgXY21"
      },
      "source": [
        "# Chapter 14: Deep Computer Vision Using Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSU-VJpGXY24"
      },
      "source": [
        "## 14.1 The Architecture of the Visual Cortex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwy7ccTDZEeK"
      },
      "source": [
        "**Convolutional neural networks** - The evolution of the neocognitron inspired by the studies of the visual cortex:\n",
        "\n",
        "- Many neurons in the visual cortex have a small **local receptive field** meaning they react only to visual stimuli located in a limited region of the visual field.\n",
        "\n",
        "- Some neurons react only to images of horizontal lines, while others react only to lines with different orientations.\n",
        "\n",
        "- Some neurons have larger receptive fields, and they react to more complex patterns that are combinations of the lower-level patterns.\n",
        "\n",
        "- Higher-level neurons are based on the outputs of neighboring lower-level neurons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HJ73a8TXY25"
      },
      "source": [
        "## 14.2 Convolutional Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HQDSR42bVPy"
      },
      "source": [
        "**Convolution** - A mathematical operation that slides one function over another and measures the integral of their pointwise multiplication.\n",
        "\n",
        "**Convolutional layer** - Neurons in the first convolutional layer are not connected to every single pixel in the input image, but only to pixels in their receptive fields.\n",
        "\n",
        "This architecture allows the network to concentrate on small low-level features in the first hidden layer, then assemble them into larger higher-level features in the next hidden layer, and so on.\n",
        "\n",
        "**Zero padding** - In order for a layer to have the same height and width as the previous layer, it is common to add zeros around the inputs.\n",
        "\n",
        "**Stride** - The shift from one receptive field to the next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWlCcpvJXY26"
      },
      "source": [
        "### 14.2.1 Filters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tgUqRgYfii_"
      },
      "source": [
        "A neuron's weights can be represented as a small image the size of the receptive field.\n",
        "\n",
        "**Filters** (also called convolutional kernels) - The set of weights.\n",
        "\n",
        "**Figure 14-5** - Applying two different filters to get two feature maps\n",
        "\n",
        "- A vertical filter is a black square with a vertical white line in the middle (7x7 matrix with central column of 1s, rest 0s).\n",
        "\n",
        "    - When applied to input image, the vertical white lines get enhanced while the rest gets blurred.\n",
        "\n",
        "- A horizontal filter is similar to vertical filter except it has a white horizontal line in the middle.\n",
        "\n",
        "    - When applied, the horizontal white lines get enhanced while rest gets blurred.\n",
        "\n",
        "**Feature map** - A layer full of neurons using the same filter highlights the areas in an image that activate the filter the most."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rLNn5sGXY26"
      },
      "source": [
        "### 14.2.2 Stacking Multiple Feature Maps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mx5NHM1h0Pv"
      },
      "source": [
        "A convolutional layer has multiple filters and outputs one feature map per filter, and is more accurately represented in 3D (as opposed to 2D). It has one neuron per pixel in each feature map, and all neurons within a given feature map share the same parameters.\n",
        "\n",
        "Sharing the same parameters dramatically reduces the number of parameters in the model. And once a CNN has learned to recognize a pattern in one location, it can recognize it in any location (regular DNN can only recognize in a particular location).\n",
        "\n",
        "> Note: All neurons located in the same row $i$ and column $j$ but in different feature maps are connected to the outputs of the exact same neurons in the previous layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qnt8uNYXY27"
      },
      "source": [
        "### 14.2.3 TensorFlow Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JKPAzJZXY28"
      },
      "source": [
        "### 14.2.4 Memory Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr463jZdXY28"
      },
      "source": [
        "## 14.3 Pooling Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtIha_weXY28"
      },
      "source": [
        "### 14.3.1 TensorFlow Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y26jntUXY29"
      },
      "source": [
        "## 14.4 CNN Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoR0-9MLXY29"
      },
      "source": [
        "### 14.4.1 LeNet-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlTmVtwIXY2-"
      },
      "source": [
        "### 14.4.2 AlexNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep9p620sXY2-"
      },
      "source": [
        "### 14.4.3 GoogLeNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXa26yIeXY2-"
      },
      "source": [
        "### 14.4.4 VGGNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NorJQJIaXY2-"
      },
      "source": [
        "### 14.4.5 ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69_p9Z9EXY2_"
      },
      "source": [
        "### 14.4.6 Xception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKU0CgpFXY3A"
      },
      "source": [
        "### 14.4.7 SENet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94EYPVgjXY3A"
      },
      "source": [
        "## 14.5 Implementing a ResNet-34 CNN Using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bv5eWDNiXY3A"
      },
      "source": [
        "## 14.6 Using Pretrained Models from Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ProqkcQkXY3B"
      },
      "source": [
        "## 14.7 Pretrained Models for Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3Eo5lJ1XY3B"
      },
      "source": [
        "## 14.8 Classification and Localization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC0jQrZRXY3B"
      },
      "source": [
        "## 14.9 Object Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHFInuB6XY3C"
      },
      "source": [
        "### 14.9.1 Fully Convolutional Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-VG6zxAXY3C"
      },
      "source": [
        "### 14.9.2 You Only Look Once (YOLO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD-0QEv1XY3C"
      },
      "source": [
        "## 14.10 Semantic Segmentation"
      ]
    }
  ]
}