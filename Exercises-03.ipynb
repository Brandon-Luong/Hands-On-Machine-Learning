{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "54b71c1b326a101c02c54fb24502b2663857f505f7b72a06051ccceba068a4ff"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 3: Classification Exercises"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "source": [
    "## 1.\n",
    "\n",
    "> Try to build a classifier for the MNIST dataset that achieves over 97% accuracy on the test set.  \n",
    "\n",
    "> Hint: The `KNeighborsClassifier` works quite well for this task; you just need to find good hyperparameter values (try a grid search on the `weights` and `n_neighbor` hyperparameters)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Import the MNIST dataset and split into training/test sets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist['data'], mnist[\"target\"]\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "source": [
    "Do a grid search on `weights: {'uniform', 'distance'}` and `n_neighbors: default=5` hyperparameters, found on Scikit-Learn docs. And then fit the training set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "param_grid = [{'weights': ['uniform', 'distance'], 'n_neighbors': [2, 4, 5]}]\n",
    "knn_clf = KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn_clf, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=KNeighborsClassifier(),\n",
       "             param_grid=[{'n_neighbors': [2, 4, 5],\n",
       "                          'weights': ['uniform', 'distance']}])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'n_neighbors': 4, 'weights': 'distance'}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9716166666666666"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "source": [
    "Calculate accuracy on test set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9714"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "y_predict = grid_search.predict(X_test)\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "source": [
    "## 2.\n",
    "\n",
    "> 1. Write a function that can shift an MNIST image in any direction (left, right, up, or down) by one pixel.  \n",
    "\n",
    ">> You can use the `shift()` function from the `scipy.ndimage.interpolation` module. For example, `shift(image, [2, 1], cval=0)` shifts the image two pixels down and one pixel to the right.  \n",
    "\n",
    "> 2. Then, for each image in the training set, create four shifted copies (one per direction) and add them to the training set.  \n",
    "\n",
    "> 3. Finally, train your best model on this expanded training set and measure its accuracy on the test set.  \n",
    "\n",
    "> You should observe that your model performs even better now! This technique of artificially growing the training set is called *data augmentation* or *training set expansion*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.interpolation import shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_shift(image, direction):\n",
    "    image = image.values.reshape((28, 28))\n",
    "    shifted_image = shift(image, [direction[0], direction[1]])\n",
    "    return shifted_image.reshape([-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = ((0, -1), (0, 1), (-1, 0), (1, 0)) # left, right, up, down\n",
    "\n",
    "X_train_shifted = []\n",
    "y_train_shifted = []\n",
    "\n",
    "for _, row in X_train.iterrows():\n",
    "    X_train_shifted.append(row)\n",
    "    for arrow in directions:\n",
    "        X_train_shifted.append(image_shift(row, arrow))\n",
    "\n",
    "for row in y_train:\n",
    "    y_train_shifted.append(row)\n",
    "    for arrow in directions:\n",
    "        y_train_shifted.append(row)\n",
    "\n",
    "X_train_shifted = np.array(X_train_shifted)\n",
    "y_train_shifted = np.array(y_train_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'n_neighbors': 4, 'weights': 'distance'}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best hyperparameters from Problem 1\n",
    "knn_shifted_clf = KNeighborsClassifier(n_neighbors=4, weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=4, weights='distance')"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "knn_shifted_clf.fit(X_train_shifted, y_train_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9763"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "y_predict = knn_shifted_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "source": [
    "## 3.\n",
    "\n",
    "> Tackle the Titanic dataset. A great place to start is on Kaggle."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.\n",
    "\n",
    "> Build a spam classifier (a more challenging exercise):  \n",
    "\n",
    "> 1. Download examples of spam and ham from Apache SpamAssassin's public datasets.\n",
    "\n",
    "> 2. Unzip the datasets and familiarize yourself with the data format.\n",
    "\n",
    "> 3. Split the datasets into a training set and a test set.\n",
    "\n",
    "> 4. Write a data preparation pipeline to convert each email into a feature vector.  \n",
    "   - Your preparation pipeline should transform an email into a (sparse) vector that indicates the presence or absence of each possible word.  \n",
    ">> For example, if all emails only ever contain four words, \"Hello,\" \"how,\" \"are,\" \"you,\" then the email \"Hello you Hello Hello you\" would be converted into a vector \\[1, 0, 0, 1] (meaning \\[\"Hello\" is present, \"how\" is absent, \"are\" is absent, \"you\" is present]), or \\[3, 0, 0, 2] if you prefer to count the number of occurrences of each word.\n",
    "   - You may want to add hyperparameters to your preparation pipeline to control whether or not to:\n",
    "   >> strip off email headers, convert each email to lowercase, remove punctuation, replace all URLs with \"URL,\" replace all numbers with \"NUMBER,\" or even perform *stemming* (ie, trim off word endings; there are Python libraries available to do this).\n",
    "5. Finally, try out several classifiers and see if you can build a great spam classifier, with both high recall and high precision."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}