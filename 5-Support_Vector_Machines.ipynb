{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "54b71c1b326a101c02c54fb24502b2663857f505f7b72a06051ccceba068a4ff"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 5: Support Vector Machines"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "## 5.1 Linear SVM Classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "You can think of an SVM classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes; this is called *larged margin classification*.\n",
    "\n",
    "The decision boundary is fully determined (\"supported\") by the instances located on the edge of the street and are called *support vectors*.\n",
    "\n",
    "> Note: SVMs are sensitive to the feature scales. Boundaries can be grouped close together if scales are not proportional."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.1.1 Soft Margin Classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*Hard margin classification* is imposing strict rules such as having all instances be off the street and on the right side. This causes some problems:\n",
    "- Only works if data is linearly separable\n",
    "- Sensitive to outliers\n",
    "\n",
    "*Soft margin classification* is the objective to find a good balance between keeping the street as large as possible and limiting the margin violations (instances that end up in the middle of the street or wrong side).\n",
    "\n",
    "The following code loads the iris dataset, scales the features, and then trains a linear SVM model (`LinearSVC` class with `C=1` and *hinge loss function*) to detect Iris virginica flowers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\"))\n",
    "])\n",
    "svm_clf.fit(X, y)\n",
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "source": [
    "> Note: SVM classifiers do not output probabilities for each class, unlike Logisitic Regresssion classifiers.\n",
    ">\n",
    "> Note: You can use SVC with a linear kernel, `SVC(kernel=\"linear\", C=1)`. Or `SGDClassifier(loss=\"hinge\", alpha=1/(m*C))`.\n",
    ">\n",
    "> Tips when using `LinearSVC`:\n",
    ">> - It regularizes the bias term, so center the training set by subtracting its mean.\n",
    ">> - `StandardScaler` automatically scales the data ($\\mu=0, \\sigma^2=1$)\n",
    ">> - Set `loss=\"hinge\"` as it's not the default value\n",
    ">> - Set `dual=False` for better performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.2 Nonlinear SVM Classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "One approach to handling nonlinear datasets is to add more features, such as polynomial features.\n",
    "\n",
    "Let's test this on the moons dataset: a toy dataset for binary classification in which the data points are shaped as two interleaving half circles. Create a `Pipeline` containing a `PolynomialFeatures` transformer, `StandardScaler`, and `LinearSVC`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('pol_features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"pol_features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "])\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "source": [
    "### 5.2.1 Polynomial Kernel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The *kernel trick* makes it possible to get the same result as if you had added many polynomial features, even with very high-degree polynomials, without actually having to add them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "poly_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "])\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "source": [
    "If your model is overfitting, reduce the polynomial degree, and increase if underfitting.  \n",
    "The hyperparameter `coef0` controls how much the model is influenced by high degree polynomials versus low-degree polynomials."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.2.2 Similarity Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Another technique to tackle nonlinear problems is to add features computed using a *similarity function*, which measures how much each instance resembles a particular *landmark*.\n",
    "\n",
    "*Equation 5-1. Gaussian RBF*\n",
    "\n",
    "$$ \\phi_\\gamma(\\vec{x}, l) = exp(-\\gamma \\lVert \\vec{x} - l \\rVert )^2$$ \n",
    "\n",
    "where $\\gamma = 0.3$ and $\\lVert \\vec{x} - l \\rVert$ is the distance between the new instance and landmark\n",
    "\n",
    "How to select landmarks?  \n",
    "Simplest approach is to create a landmark at the location of each and every instance of dataset. This creates many dimensions and increases chances the transformed training set will be linearly separable. Downside is that it'll become $(m \\times m)$ size which can be very large."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.2.3 Gaussian RBF Kernel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Similarity features method (such as Gaussian RBF) can be useful with any Machine Learning algorithm, but may be computationally expensive to compute all the additional features.  \n",
    "\n",
    "The kernel trick with the Gaussian RBF (`kernel=\"rbf\"`) achieves a similar result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=0.001, gamma=5))])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "])\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "source": [
    "$\\uparrow$ $\\gamma$ makes the bell-shaped curve narrower -> each instance's range is narrower and decision boundary becomes more irregular (more strict).\n",
    "\n",
    "$\\downarrow$ $\\gamma$ makes the bell-shaped curve wider -> each instance's range is larger and decision boundary becomes smoother.\n",
    "\n",
    "$\\gamma$ acts like a regularization hyperparameter.\n",
    "- If overfitting -> decrease $\\gamma$\n",
    "- If underfitting -> increase $\\gamma$\n",
    "\n",
    "> Note: Which kernel to choose?\n",
    ">> - Always try linear kernel first\n",
    ">> - `LinearSVC` >> `SVC(kernel=\"linear\")` (LinearSVC much faster than SVC)\n",
    ">> - If training set not too large, Gaussian RBF kernel `SVC(kernel=\"rbf\")`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.2.4 Computational Complexity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`LinearSVC` does not support kernel trick, but scales linearly with number of training examples and features. Training time complexity is $\\approx O(m \\times n) $.\n",
    "\n",
    "Algorithm takes longer if you require high precision, which is controlled by tolerance hyperparameter $\\epsilon$ (`tol` in Scikit-Learn). Most times, default tolerance is fine.\n",
    "\n",
    "`SVC` supports the kernel trick, but gets dreadfully slow when the number of training examples gets large (100,000+). Perfect for complex small or medium-sized training sets and scales well with number of features. Training time complexity is $\\approx O(m^2 \\times n) - O(m^3 \\times n) $."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.3 SVM Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To use SVMs for regression instead of classification, the trick is to reverse the objective:  \n",
    "Instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible **on** the street while limiting margin violations (instances **off** the street). The width of the street is controlled by hyperparameter $\\epsilon$ (bigger width = bigger $\\epsilon$ , smaller width = smaller $\\epsilon$).\n",
    "\n",
    "> Note: Adding more training instances within the margin does not affect the model's predictions -> model is *$\\epsilon$-insensitive*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearSVR(epsilon=1.5)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "source": [
    "To tackle nonlinear regression tasks, use a kernelized SVM model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVR(C=100, degree=2, kernel='poly')"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "source": [
    "`SVR` is the regression equivalent of `SVC` and similarly with `LinearSVR` to `LinearSVC`.\n",
    "- `LinearSVR` scales linearly with size of training set\n",
    "- `SVR` gets much too slow when training set grows large"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.4 Under the Hood"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.4.1 Decision Function and Predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The linear SVM classifier model predicts the class of a new instance by computing the decision function $ \\vec{w}^T \\vec{x} + b = w_1x_1 = ... + w_nx_n +b $, where $\\hat{y} = 1$ if result is positive, and $\\hat{y}=0$ if result is negative.\n",
    "\n",
    "**The decision boundary is the set of points where the decision function is equal to 0.**  \n",
    "Ex. In the iris dataset, it is the intersection of two planes (2 features, petal width and length) -> which is a straight line.\n",
    "\n",
    "**The dashed lines represent the points where the decision function is equal to 1 or -1.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.4.2 Training Objective"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "The smaller the weight vector $\\vec{w}$, the larger the margin. So the goal is to minimize $ \\lVert \\vec{w} \\rVert$ (minimize the norm of the weight vector, which is the slope).  \n",
    "\n",
    "For hard margin SVM, the constraint is that all positive instances be \"$>1$\" and all negative instances be \"$< -1$\" so that no instances are \"in between the street\".\n",
    "\n",
    "> Note: Minimize $ \\frac{1}{2} \\vec{w}^T \\vec{w} $ instead of $ \\lVert \\vec{w} \\rVert$.\n",
    ">> - Derivative of $ \\frac{1}{2} \\vec{w}^T \\vec{w} =  \\frac{1}{2}\\lVert \\vec{w} \\rVert ^2 = \\vec{w} $\n",
    ">> - $ \\lVert \\vec{w} \\rVert$ is not differentiable at $\\vec{w}=0$\n",
    "\n",
    "For soft margin SVM, the constaint is to minimize the slack variable, $\\zeta^{(i)} \\geq 0 $ (measures how much the i-th instance is allowed to violate the margin), and minimize $ \\frac{1}{2} \\vec{w}^T \\vec{w} $ to increase margin. `C` hyperparameter can help define the trade-off between the two objectives."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.4.3 Quadratic Programming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*Quadratic Programming* (QP) problems are convex quadratic optimization problems with linear constraints, such as the hard margin and soft margin problems.\n",
    "\n",
    "Can use an off-the-shelf QP solver to solve. But to use the kernel trick, look at a different constrained optimization problem."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.4.4 The Dual Problem"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The *primal problem* is the given constrained optimization problem.  \n",
    "The *dual problem* typically gives a lower bound to the solution of the primal problem.\n",
    "\n",
    "For SVM problems, the objective function is convex and inequality constraints are continuously differentiable and also convex, then solving the primal or dual problem will have the same solution.\n",
    "\n",
    "The dual problem is faster to solve than the primal one when training examples is smaller than features, and allows the use of kernel trick."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.4.5 Kernelized SVMs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "If you apply the transformation $\\phi$ to all training instances, then the dual problem will contain the dot product $ \\phi(\\vec{x}^{(i)})^T \\phi(\\vec{x}^{(i)}) $.\n",
    "\n",
    "But if $\\phi$ is the second-degree polynomial transformation then you can replace this dot product with $ (\\vec{x}^{(i)T} \\vec{x}^{(j)}) ^2 $.\n",
    "\n",
    "**So you don't need to transform the training instances at all, just replace the dot product with its square.**\n",
    "\n",
    "> Note: There is a lot of math behind the kernel trick. Read book for further info."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.4.6 Online SVMs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> Recall: Online learning means learning incrementally, typically as new instances arrive.\n",
    "\n",
    "Gradient Descent can be used to minimize cost function but is derived from the primal problem and converges much more slowly than the methods based on QP.\n",
    "\n",
    "You can still use Gradient Descent with the *hinge loss* function, $ max(0, 1-t)$, where it is 0 when $t \\geq 1$ and its derivates are {-1 if t<1, and 0 if t>1}."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}