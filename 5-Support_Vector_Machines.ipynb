{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 5: Support Vector Machines"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "## 5.1 Linear SVM Classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "You can think of an SVM classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes; this is called *larged margin classification*.\n",
    "\n",
    "The decision boundary is fully determined (\"supported\") by the instances located on the edge of the street and are called *support vectors*.\n",
    "\n",
    "> Note: SVMs are sensitive to the feature scales. Boundaries can be grouped close together if scales are not proportional."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.1.1 Soft Margin Classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*Hard margin classification* is imposing strict rules such as having all instances be off the street and on the right side. This causes some problems:\n",
    "- Only works if data is linearly separable\n",
    "- Sensitive to outliers\n",
    "\n",
    "*Soft margin classification* is the objective to find a good balance between keeping the street as large as possible and limiting the margin violations (instances that end up in the middle of the street or wrong side).\n",
    "\n",
    "The following code loads the iris dataset, scales the features, and then trains a linear SVM model (`LinearSVC` class with `C=1` and *hinge loss function*) to detect Iris virginica flowers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\"))\n",
    "])\n",
    "svm_clf.fit(X, y)\n",
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "source": [
    "> Note: SVM classifiers do not output probabilities for each class, unlike Logisitic Regresssion classifiers.\n",
    ">\n",
    "> Note: You can use SVC with a linear kernel, `SVC(kernel=\"linear\", C=1)`. Or `SGDClassifier(loss=\"hinge\", alpha=1/(m*C))`.\n",
    ">\n",
    "> Tips when using `LinearSVC`:\n",
    ">> - It regularizes the bias term, so center the training set by subtracting its mean.\n",
    ">> - `StandardScaler` automatically scales the data ($\\mu=0, \\sigma^2=1$)\n",
    ">> - Set `loss=\"hinge\"` as it's not the default value\n",
    ">> - Set `dual=False` for better performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.2 Nonlinear SVM Classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "One approach to handling nonlinear datasets is to add more features, such as polynomial features.\n",
    "\n",
    "Let's test this on the moons dataset: a toy dataset for binary classification in which the data points are shaped as two interleaving half circles. Create a `Pipeline` containing a `PolynomialFeatures` transformer, `StandardScaler`, and `LinearSVC`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('pol_features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"pol_features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "])\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "source": [
    "### 5.2.1 Polynomial Kernel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The *kernel trick* makes it possible to get the same result as if you had added many polynomial features, even with very high-degree polynomials, without actually having to add them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "poly_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "])\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "source": [
    "If your model is overfitting, reduce the polynomial degree, and increase if underfitting.  \n",
    "The hyperparameter `coef0` controls how much the model is influenced by high degree polynomials versus low-degree polynomials."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.2.2 Similarity Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Another technique to tackle nonlinear problems is to add features computed using a *similarity function*, which measures how much each instance resembles a particular *landmark*.\n",
    "\n",
    "*Equation 5-1. Gaussian RBF*\n",
    "\n",
    "$$ \\phi_\\gamma(\\vec{x}, l) = exp(-\\gamma \\lVert \\vec{x} - l \\rVert )^2$$ \n",
    "\n",
    "where $\\gamma = 0.3$ and $\\lVert \\vec{x} - l \\rVert$ is the distance between the new instance and landmark\n",
    "\n",
    "How to select landmarks?  \n",
    "Simplest approach is to create a landmark at the location of each and every instance of dataset. This creates many dimensions and increases chances the transformed training set will be linearly separable. Downside is that it'll become $(m \\times m)$ size which can be very large."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.2.3 Gaussian RBF Kernel"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Similarity features method (such as Gaussian RBF) can be useful with any Machine Learning algorithm, but may be computationally expensive to compute all the additional features.  \n",
    "\n",
    "The kernel trick with the Gaussian RBF (`kernel=\"rbf\"`) achieves a similar result."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=0.001, gamma=5))])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "])\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "source": [
    "$\\uparrow$ $\\gamma$ makes the bell-shaped curve narrower -> each instance's range is narrower and decision boundary becomes more irregular (more strict).\n",
    "\n",
    "$\\downarrow$ $\\gamma$ makes the bell-shaped curve wider -> each instance's range is larger and decision boundary becomes smoother.\n",
    "\n",
    "$\\gamma$ acts like a regularization hyperparameter.\n",
    "- If overfitting -> decrease $\\gamma$\n",
    "- If underfitting -> increase $\\gamma$\n",
    "\n",
    "> Note: Which kernel to choose?\n",
    ">> - Always try linear kernel first\n",
    ">> - `LinearSVC` >> `SVC(kernel=\"linear\")` (LinearSVC much faster than SVC)\n",
    ">> - If training set not too large, Gaussian RBF kernel `SVC(kernel=\"rbf\")`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.2.4 Computational Complexity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`LinearSVC` does not support kernel trick, but scales linearly with number of training examples and features. Training time complexity is $\\approx O(m \\times n) $.\n",
    "\n",
    "Algorithm takes longer if you require high precision, which is controlled by tolerance hyperparameter $\\epsilon$ (`tol` in Scikit-Learn). Most times, default tolerance is fine.\n",
    "\n",
    "`SVC` supports the kernel trick, but gets dreadfully slow when the number of training examples gets large (100,000+). Perfect for complex small or medium-sized training sets and scales well with number of features. Training time complexity is $\\approx O(m^2 \\times n) - O(m^3 \\times n) $."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.3 SVM Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To use SVMs for regression instead of classification, the trick is to reverse the objective:  \n",
    "Instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible **on** the street while limiting margin violations (instances **off** the street). The width of the street is controlled by hyperparameter $\\epsilon$ (bigger width = bigger $\\epsilon$ , smaller width = smaller $\\epsilon$).\n",
    "\n",
    "> Note: Adding more training instances within the margin does not affect the model's predictions -> model is *$\\epsilon$-insensitive*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LinearSVR(epsilon=1.5)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "source": [
    "To tackle nonlinear regression tasks, use a kernelized SVM model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVR(C=100, degree=2, kernel='poly')"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "source": [
    "`SVR` is the regression equivalent of `SVC` and similarly with `LinearSVR` to `LinearSVC`.\n",
    "- `LinearSVR` scales linearly with size of training set\n",
    "- `SVR` gets much too slow when training set grows large"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.4 Under the Hood"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.4.1 Decision Function and Predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.4.2 Training Objective"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.4.3 Quadratic Programming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.4.4 The Dual Problem"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.4.5 Kernelized SVMs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 5.4.6 Online SVMs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}