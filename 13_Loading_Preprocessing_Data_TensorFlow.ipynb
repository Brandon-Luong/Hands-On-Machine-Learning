{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.0 64-bit ('venv')",
      "metadata": {
        "interpreter": {
          "hash": "7b58ff03a81b7b220740b362f01ec4719380627cca970f4b363191d14b8c12ac"
        }
      }
    },
    "colab": {
      "name": "13_Loading_Preprocessing_Data_TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh-BDFROrrlC"
      },
      "source": [
        "# Chapter 13: Loading and Preprocessing Data with TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Xp4aRNsVSf"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrbI82TtzLnQ"
      },
      "source": [
        "Since Deep Learning systems are often trained on very large datasets that will not fit in RAM, TensorFlow's **Data API** solves this issue by taking care of all the implementation details and only needs:\n",
        "- A dataset object\n",
        "- Where to get the data\n",
        "- How to transform it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITP8mk7arrlF"
      },
      "source": [
        "## 13.1 The Data API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i4gXmC5stZG"
      },
      "source": [
        "The Data API revolves around the concept of a **dataset**: a sequence of data items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvSahjM8s3SU",
        "outputId": "572b99fd-58db-4ffe-ab1f-bfd2e3073534"
      },
      "source": [
        "# Create a dataset entirely in RAM\n",
        "X = tf.range(10) # any data tensor\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (), types: tf.int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj-6u4tatQuY"
      },
      "source": [
        "The `from_tensor_slices()` function takes a tensor and creates a `tf.data.Dataset` whose elements are all the slices of X. This is the same as `tf.data.Dataset.range(10)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8RIx3hutkst",
        "outputId": "d3341a91-d9e0-447d-d24f-8b3b49070447"
      },
      "source": [
        "# Iterate over dataset's items\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(3, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(7, shape=(), dtype=int32)\n",
            "tf.Tensor(8, shape=(), dtype=int32)\n",
            "tf.Tensor(9, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1pXrKLlrrlG"
      },
      "source": [
        "### 13.1.1 Chaining Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QmJqp6_t32e"
      },
      "source": [
        "Once you have a dataset, you can apply transformations by calling its transformation methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV0TpO65t-gX",
        "outputId": "0555b6eb-1eb6-494f-8e1b-751d947559c2"
      },
      "source": [
        "# See Figure 13-1. Chaining dataset transformations\n",
        "dataset = dataset.repeat(3).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
            "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
            "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_MvYsp4uqOA"
      },
      "source": [
        "With original dataset,\n",
        "1. Call `repeat(3)` to return a new dataset with 3 copies of the dataset.\n",
        "    - Calling with no arguments will result in a new dataset that repeats forever, so the code that iterates over the dataset must decide when to stop.\n",
        "\n",
        "2. Call `batch(7)` to return a new dataset that groups the items into batches of 7 items and any remaining items in the last batch (batch of 2).\n",
        "    - Add `drop_remainder=True` argument to drop this final batch.\n",
        "\n",
        "> Note: Dataset methods **do not** modify datasets; they create new ones (ie. assign with `dataset = ...`) or else nothing will happen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTBo5tcCwW9I",
        "outputId": "4e4c5611-957c-4679-841b-67f81cb4d8c7"
      },
      "source": [
        "# Creates new dataset with all items doubled\n",
        "dataset = dataset.map(lambda x: x * 2) # Items:[0,2,4,6,8,10,12]\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
            "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
            "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9sQk8RExFvR"
      },
      "source": [
        "While the `map()` method applies a transformation to each item, the `apply()` method applies a transformation to the dataset as a whole.\n",
        "\n",
        "> Note: `apply()` method is not used since `tf.data.Dataset.unbatch(dataset)` needs 1 argument for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogyiShKtxSrt",
        "outputId": "791a5678-db61-427c-d4ea-1281dd6c0c2e"
      },
      "source": [
        "# tf.data.experimental.unbatch() is now deprecated\n",
        "# Use tf.data.Dataset.unbatch()\n",
        "# Each item in the new dataset will be single-integer tensor\n",
        "dataset = tf.data.Dataset.unbatch(dataset)\n",
        "\n",
        "# Filter the dataset\n",
        "dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...\n",
        "\n",
        "# Look at just a few items from dataset\n",
        "for item in dataset.take(3):\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eqQ58KrrrlG"
      },
      "source": [
        "### 13.1.2 Shuffling the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZxXT6CbIJZU"
      },
      "source": [
        "Gradient Descent works best when the instances in the training set are independent and identically distributed (iid). A simple way to ensure this is to shuffle the instances, using the `shuffle()` method.\n",
        "\n",
        "> Note: You must specify the buffer size, and it's important to make it large enough, or else shuffling will not be very effective.\n",
        "\n",
        "> Note: By default, calling `repeat()` on a shuffled dataset will generate a new order at every iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLq55PaXIzB_",
        "outputId": "a1dea5ca-fd36-498d-fc5e-89a04d5c0ccf"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times\n",
        "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
            "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
            "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yAEokUOJvy-"
      },
      "source": [
        "For large datasets, simple shuffling-buffer may not be enough; buffer size is still small compared to the large dataset.\n",
        "\n",
        "Some solutions are:\n",
        "- Shuffle the source data itself.\n",
        "- Split the source data into multiple files, then read them in a random order during training.\n",
        "- Pick multiple files randomly and read them simultaneously, interleaving their records.\n",
        "- Add a shuffling buffer on top of all that using `shuffle()` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrTpfj6bKoNv"
      },
      "source": [
        "#### Interleaving lines from multiple files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GPeLYi2Lkq6",
        "outputId": "163813a6-5fda-4092-efb5-f1539c9302f7"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\n",
        "# Load CA housing dataset\n",
        "# Split into training, validation, test set\n",
        "# Scale sets\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_mean = scaler.mean_\n",
        "X_std = scaler.scale_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbE6z-DQMD9W"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\n",
        "# Split housing set\n",
        "# Save into 20 CSV files\n",
        "\n",
        "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
        "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
        "    os.makedirs(housing_dir, exist_ok=True)\n",
        "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
        "\n",
        "    filepaths = []\n",
        "    m = len(data)\n",
        "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
        "        part_csv = path_format.format(name_prefix, file_idx)\n",
        "        filepaths.append(part_csv)\n",
        "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
        "            if header is not None:\n",
        "                f.write(header)\n",
        "                f.write(\"\\n\")\n",
        "            for row_idx in row_indices:\n",
        "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
        "                f.write(\"\\n\")\n",
        "    return filepaths\n",
        "\n",
        "train_data = np.c_[X_train, y_train]\n",
        "valid_data = np.c_[X_valid, y_valid]\n",
        "test_data = np.c_[X_test, y_test]\n",
        "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
        "header = \",\".join(header_cols)\n",
        "\n",
        "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
        "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
        "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cAIIpUFMkkw",
        "outputId": "492a6533-d4ad-49db-c535-bb11f6cd08d7"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\n",
        "# Read 1st few lines of CSV file in text mode\n",
        "\n",
        "with open(train_filepaths[0]) as f:\n",
        "    for i in range(5):\n",
        "        print(f.readline(), end=\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
            "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
            "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
            "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
            "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHPJQfv8M3V-",
        "outputId": "1bc55c04-13c6-49c9-ac6e-c0c3c2934bac"
      },
      "source": [
        "train_filepaths"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['datasets/housing/my_train_00.csv',\n",
              " 'datasets/housing/my_train_01.csv',\n",
              " 'datasets/housing/my_train_02.csv',\n",
              " 'datasets/housing/my_train_03.csv',\n",
              " 'datasets/housing/my_train_04.csv',\n",
              " 'datasets/housing/my_train_05.csv',\n",
              " 'datasets/housing/my_train_06.csv',\n",
              " 'datasets/housing/my_train_07.csv',\n",
              " 'datasets/housing/my_train_08.csv',\n",
              " 'datasets/housing/my_train_09.csv',\n",
              " 'datasets/housing/my_train_10.csv',\n",
              " 'datasets/housing/my_train_11.csv',\n",
              " 'datasets/housing/my_train_12.csv',\n",
              " 'datasets/housing/my_train_13.csv',\n",
              " 'datasets/housing/my_train_14.csv',\n",
              " 'datasets/housing/my_train_15.csv',\n",
              " 'datasets/housing/my_train_16.csv',\n",
              " 'datasets/housing/my_train_17.csv',\n",
              " 'datasets/housing/my_train_18.csv',\n",
              " 'datasets/housing/my_train_19.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-a-mWbKM9zl"
      },
      "source": [
        "# Create dataset with only these file paths\n",
        "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2nDVadGNLS9"
      },
      "source": [
        "> Note: By default, the `list_files()` function returns a dataset that shuffles the file paths.\n",
        "\n",
        "Next, call `interleave()` method to read from five files at a time and interleave their lines (skipping 1st line which is the header row)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgkeqN35Njg1"
      },
      "source": [
        "n_readers = 5\n",
        "dataset = filepath_dataset.interleave(\n",
        "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "    cycle_length=n_readers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOiOYMnwOAl_"
      },
      "source": [
        "At this stage, there will be 7 datasets in all:\n",
        "- The filepath dataset\n",
        "- The interleave dataset\n",
        "- 5 `TextLineDatasets` created internally by the interleave dataset.\n",
        "\n",
        "When iterating over the interleave set, it will cycle through these 5 `TextLineDatasets` reading one line at a time until all datasets are out of items. Then it will get the next 5 file paths from `filepath_dataset` and interleave them the same way until it runs out of file paths.\n",
        "\n",
        "By default, `interleave()` does not use parallelism; it just reads one line at a time from each file. To read files in parallel, set `num_parallel_calls` argument to the number of threads you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O8ob8VLO6VO",
        "outputId": "70e5cdac-a7cb-487a-c3f5-bab20774a292"
      },
      "source": [
        "# 1st rows of 5 CSV files, chosen randomly\n",
        "\n",
        "for line in dataset.take(5):\n",
        "    print(line.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782'\n",
            "b'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215'\n",
            "b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\n",
            "b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\n",
            "b'3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXCb2tRurrlH"
      },
      "source": [
        "### 13.1.3 Preprocessing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpaLJbKKPxVF"
      },
      "source": [
        "# X_mean, X_std = # mean and scale of each feature in the training set\n",
        "# X_mean, X_std already assigned when loading CA dataset\n",
        "\n",
        "n_inputs = 8\n",
        "\n",
        "def preprocess(line):\n",
        "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "    x = tf.stack(fields[:-1])\n",
        "    y = tf.stack(fields[-1:])\n",
        "    return (x - X_mean) / X_std, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L86Id5qlQoiv"
      },
      "source": [
        "Code explanation:\n",
        "\n",
        "1. (Line 1): Precomputed the mean and standard deviation of each feature in the training set.\n",
        "\n",
        "2. (Lines 6-8): The `preprocess()` function takes one CSV line and starts by parsing it.\n",
        "    - (Line 7): All feature columns are floats and missing values should default to 0. And provide an empty array of type `tf.float32` as the default value for the last column (the target).\n",
        "    - (Line 8): `tf.io.decode_csv()` function takes 2 arguments: the line to parse and an array containing the default value for each column in the CSV file.\n",
        "\n",
        "3. (Lines 9-10): `decode_csv()` function returns a list of scalar tensors (1 per column) but we need to return 1D tensor arrays.\n",
        "    - (Line 9): Call `tf.stack()` on all tensors except last one (the target) to stack tensors into a 1D array.\n",
        "    - (Line 10): Call `tf.stack()` on target value to stack into a 1D tensor array with a single value, rather than a scalar tensor.\n",
        "\n",
        "4. (Line 11): Scale the input features by subtracting the feature means `(x - X_mean)`, dividing by the feature standard deviation `/ X_std`, and return a tuple containing the scaled features and the target.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wANud2vYQbvk",
        "outputId": "6d04f511-fb2a-4819-c8d0-0475c597b114"
      },
      "source": [
        "# Test preprocessing function\n",
        "preprocess(b'4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
              " array([ 0.16579159,  1.216324  , -0.05204396, -0.39210168, -0.5277444 ,\n",
              "        -0.26334172,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0VBpPyrrrlH"
      },
      "source": [
        "### 13.1.4 Putting Everything Together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7C9ebZO1PV2"
      },
      "source": [
        "Put everything together into a small helper function that will:\n",
        "- Create and return a dataset that will load CA housing data from multiple CSV files\n",
        "- Preprocess it\n",
        "- Shuffle it\n",
        "- Optionally repeat it\n",
        "- Batch it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov7Y6aMg1pSe"
      },
      "source": [
        "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
        "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
        "                       n_parse_threads=5, batch_size=32):\n",
        "    dataset = tf.data.Dataset.list_files(filepaths) # Create dataset from multiple CSV files\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
        "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads) # Calls preprocess function\n",
        "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat) # Shuffles dataset \"repeat\" times\n",
        "    return dataset.batch(batch_size).prefetch(1) # Important for performance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de7jQo1VrrlI"
      },
      "source": [
        "### 13.1.5 Prefetching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJZulJyU3oV8"
      },
      "source": [
        "By calling `prefetch(1)` at the end, we are creating a dataset that will do its best to always be 1 (or multiple) batch(es) ahead. \n",
        "\n",
        "In other words, while our training algorithm is working on one batch, the dataset will already be working in parallel on getting the next batch ready (eg. reading the data from disk and preprocessing it).\n",
        "\n",
        "> Note: With prefetching, the CPU and GPU work in parallel: as the GPU works on one batch, the CPU works on the next.\n",
        "\n",
        "If the dataset is small enough to fit in memory, use dataset's `cache()` method to cache its content to RAM and speed up training. Do this **after** loading and preprocessing the data, but **before** shuffling, repeating, batching, and prefetching."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S214FBxerrlJ"
      },
      "source": [
        "### 13.1.6 Using the Dataset with tf.keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW1gCFKy5swL"
      },
      "source": [
        "# Use csv_reader_dataset() to create sets\n",
        "train_set = csv_reader_dataset(train_filepaths)\n",
        "valid_set = csv_reader_dataset(valid_filepaths)\n",
        "test_set = csv_reader_dataset(test_filepaths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CBIUvJe6pVh",
        "outputId": "4da892dd-c99b-46b6-c808-f5e54e81ddb9"
      },
      "source": [
        "# From textbook notebook\n",
        "# Build and train Keras model\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "    keras.layers.Dense(1),\n",
        "])\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "\n",
        "model.fit(train_set, epochs=10, validation_data=valid_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "363/363 [==============================] - 2s 3ms/step - loss: 3.0959 - val_loss: 3.8690\n",
            "Epoch 2/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 1.0523 - val_loss: 1.1137\n",
            "Epoch 3/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.8122 - val_loss: 0.7239\n",
            "Epoch 4/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.7343 - val_loss: 0.6612\n",
            "Epoch 5/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6527 - val_loss: 0.6517\n",
            "Epoch 6/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6341 - val_loss: 0.6310\n",
            "Epoch 7/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6299 - val_loss: 0.6054\n",
            "Epoch 8/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5916 - val_loss: 0.5891\n",
            "Epoch 9/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5682 - val_loss: 0.5564\n",
            "Epoch 10/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5330 - val_loss: 0.5215\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5bcabb39d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDB6KiKu7AxE",
        "outputId": "b5e6411c-1892-427d-f3e5-bc71c055cc9a"
      },
      "source": [
        "# Evaluate on test set and make prediction\n",
        "model.evaluate(test_set)\n",
        "new_set = test_set.take(3).map(lambda X, y: X) # pretend we have 3 new instances\n",
        "model.predict(new_set) # a dataset containing new instances"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "162/162 [==============================] - 0s 1ms/step - loss: 0.5228\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.3385925 ],\n",
              "       [1.3566871 ],\n",
              "       [1.975553  ],\n",
              "       [1.1848876 ],\n",
              "       [1.5380412 ],\n",
              "       [2.5135336 ],\n",
              "       [2.2623162 ],\n",
              "       [1.2559526 ],\n",
              "       [2.6903954 ],\n",
              "       [2.714336  ],\n",
              "       [1.8362496 ],\n",
              "       [2.0574398 ],\n",
              "       [1.942119  ],\n",
              "       [2.411251  ],\n",
              "       [1.0365776 ],\n",
              "       [0.78242517],\n",
              "       [1.3213642 ],\n",
              "       [2.1210408 ],\n",
              "       [1.818429  ],\n",
              "       [2.685921  ],\n",
              "       [1.5517488 ],\n",
              "       [2.303208  ],\n",
              "       [2.1002278 ],\n",
              "       [1.8787147 ],\n",
              "       [1.8928831 ],\n",
              "       [2.6535788 ],\n",
              "       [2.1432931 ],\n",
              "       [1.6555791 ],\n",
              "       [2.926011  ],\n",
              "       [3.245479  ],\n",
              "       [1.0477158 ],\n",
              "       [0.85796463],\n",
              "       [1.278811  ],\n",
              "       [1.5953363 ],\n",
              "       [1.6979065 ],\n",
              "       [3.4787908 ],\n",
              "       [1.5887406 ],\n",
              "       [2.496935  ],\n",
              "       [3.0445418 ],\n",
              "       [2.0707269 ],\n",
              "       [2.5267224 ],\n",
              "       [2.5330534 ],\n",
              "       [3.4804492 ],\n",
              "       [0.77085966],\n",
              "       [3.8585725 ],\n",
              "       [1.5676365 ],\n",
              "       [3.6967864 ],\n",
              "       [1.776646  ],\n",
              "       [1.9306016 ],\n",
              "       [1.2505447 ],\n",
              "       [2.44846   ],\n",
              "       [2.4583907 ],\n",
              "       [1.1052907 ],\n",
              "       [2.2794266 ],\n",
              "       [2.4463267 ],\n",
              "       [2.7751799 ],\n",
              "       [0.9593088 ],\n",
              "       [1.3960657 ],\n",
              "       [1.0992378 ],\n",
              "       [1.1609217 ],\n",
              "       [1.5225141 ],\n",
              "       [1.6614828 ],\n",
              "       [1.6341687 ],\n",
              "       [2.9423337 ],\n",
              "       [2.1151907 ],\n",
              "       [1.0627215 ],\n",
              "       [2.5984626 ],\n",
              "       [2.814077  ],\n",
              "       [0.59269845],\n",
              "       [2.2759132 ],\n",
              "       [2.1180294 ],\n",
              "       [5.558236  ],\n",
              "       [1.8572055 ],\n",
              "       [1.9872177 ],\n",
              "       [0.96959066],\n",
              "       [1.6656854 ],\n",
              "       [2.6435957 ],\n",
              "       [1.1506864 ],\n",
              "       [1.5844939 ],\n",
              "       [2.3743312 ],\n",
              "       [6.325423  ],\n",
              "       [1.6709833 ],\n",
              "       [1.3414682 ],\n",
              "       [2.002791  ],\n",
              "       [1.6535814 ],\n",
              "       [0.6405405 ],\n",
              "       [1.6653508 ],\n",
              "       [2.3194337 ],\n",
              "       [3.7059941 ],\n",
              "       [1.9020087 ],\n",
              "       [2.8478203 ],\n",
              "       [2.7843728 ],\n",
              "       [1.4878148 ],\n",
              "       [1.5322847 ],\n",
              "       [1.9400336 ],\n",
              "       [2.141818  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GE4uhVD8B-_"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\n",
        "\n",
        "# To build custom training loop,\n",
        "# iterate over the training set\n",
        "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "global_step = 0\n",
        "for X_batch, y_batch in train_set:\n",
        "    global_step += 1\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(X_batch)\n",
        "        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "        loss = tf.add_n([main_loss] + model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuSnDhqg9lFq"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\n",
        "\n",
        "# TF Function that performs the whole training loop\n",
        "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "@tf.function\n",
        "def train(model, n_epochs, batch_size=32,\n",
        "          n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n",
        "    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n",
        "                       n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n",
        "                       n_parse_threads=n_parse_threads, batch_size=batch_size)\n",
        "    for X_batch, y_batch in train_set:\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_batch)\n",
        "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "            loss = tf.add_n([main_loss] + model.losses)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "train(model, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9cVdvOYrrlJ"
      },
      "source": [
        "## 13.2 The TFRecord Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo5rhyMXQGVj"
      },
      "source": [
        "The TFRecord format is TensorFlow's preferred format for storing large amounts of data and reading it efficiently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0c4eGc0Qyp_"
      },
      "source": [
        "# Create a TFRecord file\n",
        "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
        "    f.write(b\"This is the first record\")\n",
        "    f.write(b\"And this is the second record\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nflaI38iRD6P",
        "outputId": "088d8de6-2ed4-4ec8-b369-33685bd40dad"
      },
      "source": [
        "# Read TFRecord file\n",
        "filepaths = [\"my_data.tfrecord\"]\n",
        "dataset = tf.data.TFRecordDataset(filepaths)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
            "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsCswQNMrrlL"
      },
      "source": [
        "### 13.2.1 Compressed TFRecord Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxiTWVQZRdqf"
      },
      "source": [
        "To create a compressed TFRecord file, set the options argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auyxcAUBRhV2"
      },
      "source": [
        "# Create a compressed TFRecord file\n",
        "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
        "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
        "    f.write(b\"This is the first record\")\n",
        "    f.write(b\"And this is the second record\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16QFii2IR4xa",
        "outputId": "c33fa997-ac9e-47ba-b119-1b65afccf29d"
      },
      "source": [
        "# Read a compressed TFRecord file\n",
        "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n",
        "                                  compression_type=\"GZIP\")\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
            "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux4lBCsSrrlM"
      },
      "source": [
        "### 13.2.2 A Brief Introduction to Protocol Buffers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pITcwJ5kSX0Z"
      },
      "source": [
        "TFRecord files usually contain serialized **protocol buffers** (also called **protobufs**). This is a portable, extensible, and efficient binary format developed at Google."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8Fnsn6USnSz",
        "outputId": "9ffdd6ea-cefc-452c-fbb8-7b808a435e2e"
      },
      "source": [
        "# protobuf defined that looks like\n",
        "# from textbook notebook \n",
        "%%writefile person.proto\n",
        "syntax = \"proto3\";\n",
        "message Person {\n",
        "    string name = 1;\n",
        "    int32 id = 2;\n",
        "    repeated string email = 3;\n",
        "}"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing person.proto\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8pFaJ6PTSQ5"
      },
      "source": [
        "Definition explanation:\n",
        "\n",
        "1. (Line 3): We are using version 3 of the protobuf format\n",
        "\n",
        "2. (Line 4): Protobuf objects are meant to be serialized and transmitted, therefore are called *messages*.\n",
        "\n",
        "3. (Lines 5-7): The \"contents\" of the protobuf object\n",
        "    - The numbers `1, 2, 3` are the field identifiers: they will be used in each record's binary representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7ji6f55WfaZ"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\n",
        "!protoc person.proto --python_out=. # importing protobuf compiler"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDW0r6SAURxl"
      },
      "source": [
        "# Using access classes generated for Person protobuf\n",
        "from person_pb2 import Person # import the generated access class"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcazSxghXI2J",
        "outputId": "e967a891-664b-46e3-b791-b8aefaf9469b"
      },
      "source": [
        "person = Person(name=\"Al\", id=123, email=[\"a@b.com\"]) # create a Person\n",
        "print(person) # display the Person"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name: \"Al\"\n",
            "id: 123\n",
            "email: \"a@b.com\"\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Mu5mipCXX1UK",
        "outputId": "bd2c7389-5374-4cc5-a78b-7a5a8a94174a"
      },
      "source": [
        "person.name # read a field"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Al'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1j5ddz1MX4Ht",
        "outputId": "37ad9cdb-51e1-494a-90e0-3b9eac49e2e5"
      },
      "source": [
        "person.name = \"Alice\" # modify a field\n",
        "person.email[0] # repeated fields can be accessed like arrays"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'a@b.com'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHVqznzBX_z9",
        "outputId": "9afa81c2-7785-4ae1-fe98-cd13f0e934f9"
      },
      "source": [
        "person.email.append(\"c@d.com\") # add an email address\n",
        "s = person.SerializeToString() # serialize the object to a byte string\n",
        "s"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbkmzaaTYQO1",
        "outputId": "d23b7ab0-8a5a-4120-fd16-19780abb6b9e"
      },
      "source": [
        "person2 = Person() # create a new Person\n",
        "person2.ParseFromString(s) # parse the byte string (27 bytes long)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYU3ls1iYfkW",
        "outputId": "27ddccfb-57ec-4ca9-ed05-0840f4a18b08"
      },
      "source": [
        "person == person2 # now they are equal"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvO5FlmhYy6F"
      },
      "source": [
        "To summarize,\n",
        "1. Import `Person` class generated by protoc.\n",
        "2. Create an instance and visualize it, read/write some fields.\n",
        "3. Serialize it using `SerializeToString()` method.\n",
        "4. To read and parse the binary data, use `ParseFromString()` method.\n",
        "5. Returns a copy of the object that was serialized.\n",
        "\n",
        "> Note: Since these operations are not TensorFlow operations, they cannot be included in a TensorFlow Function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9orkoC-hrrlO"
      },
      "source": [
        "### 13.2.3 TensorFlow Protobufs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp6-s231Z6UO"
      },
      "source": [
        "The main protobuf typically used in a TFRecord file is the `Example` protobuf, which represents one instance in a dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uitAeOC4aDAZ",
        "outputId": "e9991dd1-8c47-46e5-de0a-955173af09dd"
      },
      "source": [
        "# Example protobuf definition\n",
        "# %%writefile so no error\n",
        "%%writefile Example.proto\n",
        "syntax = \"proto3\";\n",
        "message BytesList { repeated bytes value = 1;}\n",
        "message FloatList { repeated float value = 1 [packed = true]; }\n",
        "message Int64List { repeated int64 value = 1 [packed = true]; }\n",
        "message Feature {\n",
        "    oneof kind {\n",
        "        BytesList bytes_list = 1;\n",
        "        FloatList float_list = 2;\n",
        "        Int64List int64_list = 3;\n",
        "    }\n",
        "};\n",
        "message Features { map<string, Feature> feature = 1; };\n",
        "message Example { Features features = 1; };"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing Example.proto\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5FuAWYweYaj"
      },
      "source": [
        "Definition explanation:\n",
        "\n",
        "- (Lines 6 & 7): `[packed = true]` is used for repeated numerical fields, for a more efficient encoding.\n",
        "\n",
        "- (Lines 8-14): A `Feature` contains either a `BytesList`, a `FloatList`, or an `Int64List`.\n",
        "\n",
        "- (Line 15): A `Features` contains a **dictionary** that maps a feature name to the corresponding feature value.\n",
        "\n",
        "- (Line 16): An `Example` only contains a `Features` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHJ3Jh6Gfe7o"
      },
      "source": [
        "\"\"\"Create a tf.train.Example\n",
        "representing the same person as earlier\n",
        "and write it to a TFRecord file.\"\"\"\n",
        "from tensorflow.train import BytesList, FloatList, Int64List\n",
        "from tensorflow.train import Feature, Features, Example"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcMxFPxgf6nv"
      },
      "source": [
        "person_example = Example(\n",
        "    features=Features(\n",
        "        feature={\n",
        "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
        "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
        "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\",\n",
        "                                                          b\"c@d.com\"]))\n",
        "        }\n",
        "    )\n",
        ")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyj73ZAagvxN"
      },
      "source": [
        "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
        "    f.write(person_example.SerializeToString())"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-UY2DdorrlP"
      },
      "source": [
        "### 13.2.4 Loading and Parsing Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep2KPm63i2pq"
      },
      "source": [
        "To load the serialized `Example` protobufs, we will use a `tf.data.TFRecordDataset` once again, and parse each `Example` using `tf.io.parse_single_example()`. Since this is a TensorFlow operation, it will be included in a TF Function.\n",
        "\n",
        "It requires at least 2 arguments:\n",
        "\n",
        "1. A string scalar tensor containing the serialized data\n",
        "2. A description of each feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdkHxItPjiXl",
        "outputId": "5d7619f9-5dd3-4c33-f9da-e982af4e61c6"
      },
      "source": [
        "\"\"\"Define a description dictionary.\n",
        "Then iterate over the TFRecord Dataset,\n",
        "and parse the serialized Example protobuf.\"\"\"\n",
        "\n",
        "feature_description = {\n",
        "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
        "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "    \"emails\": tf.io.VarLenFeature(tf.string)\n",
        "}\n",
        "\n",
        "for serialized_example in tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]):\n",
        "    parsed_example = tf.io.parse_single_example(serialized_example,\n",
        "                                                feature_description)\n",
        "parsed_example"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'emails': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7f474500f310>,\n",
              " 'id': <tf.Tensor: shape=(), dtype=int64, numpy=123>,\n",
              " 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-_E0BEXktVE",
        "outputId": "a13c560f-9f23-4d91-dac6-f50b7be78582"
      },
      "source": [
        "# Variable-length features are parsed as sparse tensors\n",
        "tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13klRY26lLxK",
        "outputId": "c2886503-f248-460c-9624-8f957a52219f"
      },
      "source": [
        "# Simpler to access its values\n",
        "parsed_example[\"emails\"].values"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ib5WjvTlmjg"
      },
      "source": [
        "A `BytesList` can contain any binary data you want.\n",
        "\n",
        "- Encode an image using JPEG format and `tf.io.encode_jpeg()`  and put this binary data in a `BytesList`.\n",
        "\n",
        "- Then when reading, it was parse the `Example` and then call `tf.io.decode_jpeg()` or `tf.io.decode_image()` to decode the image.\n",
        "\n",
        "- Store any tensor by serializing it using `tf.io.serialize_tensor()` then put the resulting byte string in a `BytesList` feature.\n",
        "\n",
        "- Then parse using `tf.io.parse_tensor()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBQwDATHmure",
        "outputId": "673a94c0-2985-48c0-e685-697fbf269a24"
      },
      "source": [
        "# Parse batch by batch\n",
        "dataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(10)\n",
        "for serialized_examples in dataset:\n",
        "    parsed_examples = tf.io.parse_example(serialized_examples,\n",
        "                                          feature_description)\n",
        "parsed_examples"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'emails': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7f4745016550>,\n",
              " 'id': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([123])>,\n",
              " 'name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Alice'], dtype=object)>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GreUVz2inLS6"
      },
      "source": [
        "> Note: The `Example` protobuf is sufficient for most use cases, except when dealing with lists of lists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_N5mt_hrrlP"
      },
      "source": [
        "### 13.2.5 Handling Lists of Lists Using the SequenceExample Protobuf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2Pi16wxnaYQ",
        "outputId": "df790e2d-7c1e-40de-87d3-17357227b448"
      },
      "source": [
        "# %%writefile so no error\n",
        "%%writefile SequenceExample.proto\n",
        "message FeatureList { repeated Feature feature = 1; };\n",
        "message FeatureLists { map<string, FeatureList> feature_list = 1; };\n",
        "message SequenceExample {\n",
        "    Features context = 1;\n",
        "    FeatureLists feature_lists = 2;\n",
        "};"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing SequenceExample.proto\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTReiGnEoO0_"
      },
      "source": [
        "Definition explanation:\n",
        "\n",
        "- (Line 5-7): A `SequenceExample` contains:\n",
        "    - A `Features` object for the contextual data\n",
        "    - A `FeatureLists` object that contains one or more named `FeatureList` objects (eg. a `FeatureList` named `\"content\"` and another named `\"comments\"`).\n",
        "\n",
        "- (Line 3): Each `FeatureList` contains a list of `Feature` objects.\n",
        "\n",
        "Building, serializing, and parsing a `SequenceExample` is similar to that of `Example` but you must use `tf.io.parse_single_sequence_example()` or `tf.io.parse_sequence_example()` to parse a single or batch.\n",
        "\n",
        "> Note: If the feature lists contain sequences of varying sizes, you may want to convert them to ragged tensors using `tf.RaggedTensor.from_sparse()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CRaQW4Fqtn1"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\n",
        "\n",
        "#from tensorflow.train import FeatureList, FeatureLists, SequenceExample\n",
        "FeatureList = tf.train.FeatureList\n",
        "FeatureLists = tf.train.FeatureLists\n",
        "SequenceExample = tf.train.SequenceExample\n",
        "\n",
        "context = Features(feature={\n",
        "    \"author_id\": Feature(int64_list=Int64List(value=[123])),\n",
        "    \"title\": Feature(bytes_list=BytesList(value=[b\"A\", b\"desert\", b\"place\", b\".\"])),\n",
        "    \"pub_date\": Feature(int64_list=Int64List(value=[1623, 12, 25]))\n",
        "})\n",
        "\n",
        "content = [[\"When\", \"shall\", \"we\", \"three\", \"meet\", \"again\", \"?\"],\n",
        "           [\"In\", \"thunder\", \",\", \"lightning\", \",\", \"or\", \"in\", \"rain\", \"?\"]]\n",
        "comments = [[\"When\", \"the\", \"hurlyburly\", \"'s\", \"done\", \".\"],\n",
        "            [\"When\", \"the\", \"battle\", \"'s\", \"lost\", \"and\", \"won\", \".\"]]\n",
        "\n",
        "def words_to_feature(words):\n",
        "    return Feature(bytes_list=BytesList(value=[word.encode(\"utf-8\")\n",
        "                                               for word in words]))\n",
        "\n",
        "content_features = [words_to_feature(sentence) for sentence in content]\n",
        "comments_features = [words_to_feature(comment) for comment in comments]\n",
        "            \n",
        "sequence_example = SequenceExample(\n",
        "    context=context,\n",
        "    feature_lists=FeatureLists(feature_list={\n",
        "        \"content\": FeatureList(feature=content_features),\n",
        "        \"comments\": FeatureList(feature=comments_features)\n",
        "    }))\n",
        "\n",
        "serialized_sequence_example = sequence_example.SerializeToString()\n",
        "\n",
        "context_feature_descriptions = {\n",
        "    \"author_id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
        "    \"title\": tf.io.VarLenFeature(tf.string),\n",
        "    \"pub_date\": tf.io.FixedLenFeature([3], tf.int64, default_value=[0, 0, 0]),\n",
        "}\n",
        "sequence_feature_descriptions = {\n",
        "    \"content\": tf.io.VarLenFeature(tf.string),\n",
        "    \"comments\": tf.io.VarLenFeature(tf.string),\n",
        "}"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdA8q281p-I-",
        "outputId": "5372672f-d6da-406a-9222-6624e119cc83"
      },
      "source": [
        "parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n",
        "    serialized_sequence_example, context_feature_descriptions,\n",
        "    sequence_feature_descriptions\n",
        ")\n",
        "parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"])\n",
        "parsed_content"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'When', b'shall', b'we', b'three', b'meet', b'again', b'?'], [b'In', b'thunder', b',', b'lightning', b',', b'or', b'in', b'rain', b'?']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iOTJwEQrrlP"
      },
      "source": [
        "## 13.3 Preprocessing the Input Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi459OyWrrlQ"
      },
      "source": [
        "### 13.3.1 Encoding Categorical Features Using One-Hot Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqiTx2-grrlQ"
      },
      "source": [
        "### 13.3.2 Encoding Categorical Features Using Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQRP9A67rrlQ"
      },
      "source": [
        "### 13.3.3 Keras Preprocessing Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amrKe1DirrlR"
      },
      "source": [
        "## 13.4 TF Transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxyCzpDLrrlR"
      },
      "source": [
        "## 13.5 The TensorFlow Datasets (TFDS) Project"
      ]
    }
  ]
}