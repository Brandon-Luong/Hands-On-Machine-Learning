{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.0 64-bit ('venv')",
      "metadata": {
        "interpreter": {
          "hash": "7b58ff03a81b7b220740b362f01ec4719380627cca970f4b363191d14b8c12ac"
        }
      }
    },
    "colab": {
      "name": "13_Loading_Preprocessing_Data_TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh-BDFROrrlC"
      },
      "source": [
        "# Chapter 13: Loading and Preprocessing Data with TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Xp4aRNsVSf"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrbI82TtzLnQ"
      },
      "source": [
        "Since Deep Learning systems are often trained on very large datasets that will not fit in RAM, TensorFlow's **Data API** solves this issue by taking care of all the implementation details and only needs:\n",
        "- A dataset object\n",
        "- Where to get the data\n",
        "- How to transform it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITP8mk7arrlF"
      },
      "source": [
        "## 13.1 The Data API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i4gXmC5stZG"
      },
      "source": [
        "The Data API revolves around the concept of a **dataset**: a sequence of data items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvSahjM8s3SU",
        "outputId": "572b99fd-58db-4ffe-ab1f-bfd2e3073534"
      },
      "source": [
        "# Create a dataset entirely in RAM\n",
        "X = tf.range(10) # any data tensor\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "dataset"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (), types: tf.int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj-6u4tatQuY"
      },
      "source": [
        "The `from_tensor_slices()` function takes a tensor and creates a `tf.data.Dataset` whose elements are all the slices of X. This is the same as `tf.data.Dataset.range(10)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8RIx3hutkst",
        "outputId": "d3341a91-d9e0-447d-d24f-8b3b49070447"
      },
      "source": [
        "# Iterate over dataset's items\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(3, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(7, shape=(), dtype=int32)\n",
            "tf.Tensor(8, shape=(), dtype=int32)\n",
            "tf.Tensor(9, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1pXrKLlrrlG"
      },
      "source": [
        "### 13.1.1 Chaining Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QmJqp6_t32e"
      },
      "source": [
        "Once you have a dataset, you can apply transformations by calling its transformation methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV0TpO65t-gX",
        "outputId": "0555b6eb-1eb6-494f-8e1b-751d947559c2"
      },
      "source": [
        "# See Figure 13-1. Chaining dataset transformations\n",
        "dataset = dataset.repeat(3).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
            "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
            "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_MvYsp4uqOA"
      },
      "source": [
        "With original dataset,\n",
        "1. Call `repeat(3)` to return a new dataset with 3 copies of the dataset.\n",
        "    - Calling with no arguments will result in a new dataset that repeats forever, so the code that iterates over the dataset must decide when to stop.\n",
        "\n",
        "2. Call `batch(7)` to return a new dataset that groups the items into batches of 7 items and any remaining items in the last batch (batch of 2).\n",
        "    - Add `drop_remainder=True` argument to drop this final batch.\n",
        "\n",
        "> Note: Dataset methods **do not** modify datasets; they create new ones (ie. assign with `dataset = ...`) or else nothing will happen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTBo5tcCwW9I",
        "outputId": "4e4c5611-957c-4679-841b-67f81cb4d8c7"
      },
      "source": [
        "# Creates new dataset with all items doubled\n",
        "dataset = dataset.map(lambda x: x * 2) # Items:[0,2,4,6,8,10,12]\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
            "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
            "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9sQk8RExFvR"
      },
      "source": [
        "While the `map()` method applies a transformation to each item, the `apply()` method applies a transformation to the dataset as a whole.\n",
        "\n",
        "> Note: `apply()` method is not used since `tf.data.Dataset.unbatch(dataset)` needs 1 argument for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogyiShKtxSrt",
        "outputId": "791a5678-db61-427c-d4ea-1281dd6c0c2e"
      },
      "source": [
        "# tf.data.experimental.unbatch() is now deprecated\n",
        "# Use tf.data.Dataset.unbatch()\n",
        "# Each item in the new dataset will be single-integer tensor\n",
        "dataset = tf.data.Dataset.unbatch(dataset)\n",
        "\n",
        "# Filter the dataset\n",
        "dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...\n",
        "\n",
        "# Look at just a few items from dataset\n",
        "for item in dataset.take(3):\n",
        "    print(item)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eqQ58KrrrlG"
      },
      "source": [
        "### 13.1.2 Shuffling the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXCb2tRurrlH"
      },
      "source": [
        "### 13.1.3 Preprocessing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0VBpPyrrrlH"
      },
      "source": [
        "### 13.1.4 Putting Everything Together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de7jQo1VrrlI"
      },
      "source": [
        "### 13.1.5 Prefetching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S214FBxerrlJ"
      },
      "source": [
        "### 13.1.6 Using the Dataset with tf.keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9cVdvOYrrlJ"
      },
      "source": [
        "## 13.2 The TFRecord Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsCswQNMrrlL"
      },
      "source": [
        "### 13.2.1 Compressed TFRecord Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux4lBCsSrrlM"
      },
      "source": [
        "### 13.2.2 A Brief Introduction to Protocol Buffers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9orkoC-hrrlO"
      },
      "source": [
        "### 13.2.3 TensorFlow Protobufs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-UY2DdorrlP"
      },
      "source": [
        "### 13.2.4 Loading and Parsing Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_N5mt_hrrlP"
      },
      "source": [
        "### 13.2.5 Handling Lists of Lists Using the SequenceExample Protobuf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iOTJwEQrrlP"
      },
      "source": [
        "## 13.3 Preprocessing the Input Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi459OyWrrlQ"
      },
      "source": [
        "### 13.3.1 Encoding Categorical Features Using One-Hot Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqiTx2-grrlQ"
      },
      "source": [
        "### 13.3.2 Encoding Categorical Features Using Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQRP9A67rrlQ"
      },
      "source": [
        "### 13.3.3 Keras Preprocessing Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amrKe1DirrlR"
      },
      "source": [
        "## 13.4 TF Transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxyCzpDLrrlR"
      },
      "source": [
        "## 13.5 The TensorFlow Datasets (TFDS) Project"
      ]
    }
  ]
}