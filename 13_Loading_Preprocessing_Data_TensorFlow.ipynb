{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.0 64-bit ('venv')",
      "metadata": {
        "interpreter": {
          "hash": "7b58ff03a81b7b220740b362f01ec4719380627cca970f4b363191d14b8c12ac"
        }
      }
    },
    "colab": {
      "name": "13_Loading_Preprocessing_Data_TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh-BDFROrrlC"
      },
      "source": [
        "# Chapter 13: Loading and Preprocessing Data with TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Xp4aRNsVSf"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrbI82TtzLnQ"
      },
      "source": [
        "Since Deep Learning systems are often trained on very large datasets that will not fit in RAM, TensorFlow's **Data API** solves this issue by taking care of all the implementation details and only needs:\n",
        "- A dataset object\n",
        "- Where to get the data\n",
        "- How to transform it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITP8mk7arrlF"
      },
      "source": [
        "## 13.1 The Data API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i4gXmC5stZG"
      },
      "source": [
        "The Data API revolves around the concept of a **dataset**: a sequence of data items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvSahjM8s3SU",
        "outputId": "572b99fd-58db-4ffe-ab1f-bfd2e3073534"
      },
      "source": [
        "# Create a dataset entirely in RAM\n",
        "X = tf.range(10) # any data tensor\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (), types: tf.int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj-6u4tatQuY"
      },
      "source": [
        "The `from_tensor_slices()` function takes a tensor and creates a `tf.data.Dataset` whose elements are all the slices of X. This is the same as `tf.data.Dataset.range(10)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8RIx3hutkst",
        "outputId": "d3341a91-d9e0-447d-d24f-8b3b49070447"
      },
      "source": [
        "# Iterate over dataset's items\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(3, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(7, shape=(), dtype=int32)\n",
            "tf.Tensor(8, shape=(), dtype=int32)\n",
            "tf.Tensor(9, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1pXrKLlrrlG"
      },
      "source": [
        "### 13.1.1 Chaining Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QmJqp6_t32e"
      },
      "source": [
        "Once you have a dataset, you can apply transformations by calling its transformation methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV0TpO65t-gX",
        "outputId": "0555b6eb-1eb6-494f-8e1b-751d947559c2"
      },
      "source": [
        "# See Figure 13-1. Chaining dataset transformations\n",
        "dataset = dataset.repeat(3).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
            "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
            "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_MvYsp4uqOA"
      },
      "source": [
        "With original dataset,\n",
        "1. Call `repeat(3)` to return a new dataset with 3 copies of the dataset.\n",
        "    - Calling with no arguments will result in a new dataset that repeats forever, so the code that iterates over the dataset must decide when to stop.\n",
        "\n",
        "2. Call `batch(7)` to return a new dataset that groups the items into batches of 7 items and any remaining items in the last batch (batch of 2).\n",
        "    - Add `drop_remainder=True` argument to drop this final batch.\n",
        "\n",
        "> Note: Dataset methods **do not** modify datasets; they create new ones (ie. assign with `dataset = ...`) or else nothing will happen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTBo5tcCwW9I",
        "outputId": "4e4c5611-957c-4679-841b-67f81cb4d8c7"
      },
      "source": [
        "# Creates new dataset with all items doubled\n",
        "dataset = dataset.map(lambda x: x * 2) # Items:[0,2,4,6,8,10,12]\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
            "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
            "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9sQk8RExFvR"
      },
      "source": [
        "While the `map()` method applies a transformation to each item, the `apply()` method applies a transformation to the dataset as a whole.\n",
        "\n",
        "> Note: `apply()` method is not used since `tf.data.Dataset.unbatch(dataset)` needs 1 argument for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogyiShKtxSrt",
        "outputId": "791a5678-db61-427c-d4ea-1281dd6c0c2e"
      },
      "source": [
        "# tf.data.experimental.unbatch() is now deprecated\n",
        "# Use tf.data.Dataset.unbatch()\n",
        "# Each item in the new dataset will be single-integer tensor\n",
        "dataset = tf.data.Dataset.unbatch(dataset)\n",
        "\n",
        "# Filter the dataset\n",
        "dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...\n",
        "\n",
        "# Look at just a few items from dataset\n",
        "for item in dataset.take(3):\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eqQ58KrrrlG"
      },
      "source": [
        "### 13.1.2 Shuffling the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZxXT6CbIJZU"
      },
      "source": [
        "Gradient Descent works best when the instances in the training set are independent and identically distributed (iid). A simple way to ensure this is to shuffle the instances, using the `shuffle()` method.\n",
        "\n",
        "> Note: You must specify the buffer size, and it's important to make it large enough, or else shuffling will not be very effective.\n",
        "\n",
        "> Note: By default, calling `repeat()` on a shuffled dataset will generate a new order at every iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLq55PaXIzB_",
        "outputId": "a1dea5ca-fd36-498d-fc5e-89a04d5c0ccf"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times\n",
        "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
            "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
            "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yAEokUOJvy-"
      },
      "source": [
        "For large datasets, simple shuffling-buffer may not be enough; buffer size is still small compared to the large dataset.\n",
        "\n",
        "Some solutions are:\n",
        "- Shuffle the source data itself.\n",
        "- Split the source data into multiple files, then read them in a random order during training.\n",
        "- Pick multiple files randomly and read them simultaneously, interleaving their records.\n",
        "- Add a shuffling buffer on top of all that using `shuffle()` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrTpfj6bKoNv"
      },
      "source": [
        "#### Interleaving lines from multiple files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GPeLYi2Lkq6",
        "outputId": "c3c8667f-dd0f-4ba8-b77a-a0f2356c19cf"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\n",
        "# Load CA housing dataset\n",
        "# Split into training, validation, test set\n",
        "# Scale sets\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_mean = scaler.mean_\n",
        "X_std = scaler.scale_"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbE6z-DQMD9W"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\n",
        "# Split housing set\n",
        "# Save into 20 CSV files\n",
        "\n",
        "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
        "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
        "    os.makedirs(housing_dir, exist_ok=True)\n",
        "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
        "\n",
        "    filepaths = []\n",
        "    m = len(data)\n",
        "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
        "        part_csv = path_format.format(name_prefix, file_idx)\n",
        "        filepaths.append(part_csv)\n",
        "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
        "            if header is not None:\n",
        "                f.write(header)\n",
        "                f.write(\"\\n\")\n",
        "            for row_idx in row_indices:\n",
        "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
        "                f.write(\"\\n\")\n",
        "    return filepaths\n",
        "\n",
        "train_data = np.c_[X_train, y_train]\n",
        "valid_data = np.c_[X_valid, y_valid]\n",
        "test_data = np.c_[X_test, y_test]\n",
        "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
        "header = \",\".join(header_cols)\n",
        "\n",
        "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
        "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
        "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cAIIpUFMkkw",
        "outputId": "013a43b7-8b6b-434a-b34a-2e71a88cf850"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\n",
        "# Read 1st few lines of CSV file in text mode\n",
        "\n",
        "with open(train_filepaths[0]) as f:\n",
        "    for i in range(5):\n",
        "        print(f.readline(), end=\"\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
            "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
            "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
            "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
            "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHPJQfv8M3V-",
        "outputId": "4957d99b-20de-4004-be53-7fa1df80b7d8"
      },
      "source": [
        "train_filepaths"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['datasets/housing/my_train_00.csv',\n",
              " 'datasets/housing/my_train_01.csv',\n",
              " 'datasets/housing/my_train_02.csv',\n",
              " 'datasets/housing/my_train_03.csv',\n",
              " 'datasets/housing/my_train_04.csv',\n",
              " 'datasets/housing/my_train_05.csv',\n",
              " 'datasets/housing/my_train_06.csv',\n",
              " 'datasets/housing/my_train_07.csv',\n",
              " 'datasets/housing/my_train_08.csv',\n",
              " 'datasets/housing/my_train_09.csv',\n",
              " 'datasets/housing/my_train_10.csv',\n",
              " 'datasets/housing/my_train_11.csv',\n",
              " 'datasets/housing/my_train_12.csv',\n",
              " 'datasets/housing/my_train_13.csv',\n",
              " 'datasets/housing/my_train_14.csv',\n",
              " 'datasets/housing/my_train_15.csv',\n",
              " 'datasets/housing/my_train_16.csv',\n",
              " 'datasets/housing/my_train_17.csv',\n",
              " 'datasets/housing/my_train_18.csv',\n",
              " 'datasets/housing/my_train_19.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-a-mWbKM9zl"
      },
      "source": [
        "# Create dataset with only these file paths\n",
        "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2nDVadGNLS9"
      },
      "source": [
        "> Note: By default, the `list_files()` function returns a dataset that shuffles the file paths.\n",
        "\n",
        "Next, call `interleave()` method to read from five files at a time and interleave their lines (skipping 1st line which is the header row)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgkeqN35Njg1"
      },
      "source": [
        "n_readers = 5\n",
        "dataset = filepath_dataset.interleave(\n",
        "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "    cycle_length=n_readers)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOiOYMnwOAl_"
      },
      "source": [
        "At this stage, there will be 7 datasets in all:\n",
        "- The filepath dataset\n",
        "- The interleave dataset\n",
        "- 5 `TextLineDatasets` created internally by the interleave dataset.\n",
        "\n",
        "When iterating over the interleave set, it will cycle through these 5 `TextLineDatasets` reading one line at a time until all datasets are out of items. Then it will get the next 5 file paths from `filepath_dataset` and interleave them the same way until it runs out of file paths.\n",
        "\n",
        "By default, `interleave()` does not use parallelism; it just reads one line at a time from each file. To read files in parallel, set `num_parallel_calls` argument to the number of threads you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O8ob8VLO6VO",
        "outputId": "1d9a6a32-c175-4f00-b1a0-5e884097eb55"
      },
      "source": [
        "# 1st rows of 5 CSV files, chosen randomly\n",
        "\n",
        "for line in dataset.take(5):\n",
        "    print(line.numpy())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782'\n",
            "b'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215'\n",
            "b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\n",
            "b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\n",
            "b'3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXCb2tRurrlH"
      },
      "source": [
        "### 13.1.3 Preprocessing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpaLJbKKPxVF"
      },
      "source": [
        "# X_mean, X_std = # mean and scale of each feature in the training set\n",
        "# X_mean, X_std already assigned when loading CA dataset\n",
        "\n",
        "n_inputs = 8\n",
        "\n",
        "def preprocess(line):\n",
        "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "    x = tf.stack(fields[:-1])\n",
        "    y = tf.stack(fields[-1:])\n",
        "    return (x - X_mean) / X_std, y"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L86Id5qlQoiv"
      },
      "source": [
        "Code explanation:\n",
        "\n",
        "1. (Line 1): Precomputed the mean and standard deviation of each feature in the training set.\n",
        "\n",
        "2. (Lines 6-8): The `preprocess()` function takes one CSV line and starts by parsing it.\n",
        "    - (Line 7): All feature columns are floats and missing values should default to 0. And provide an empty array of type `tf.float32` as the default value for the last column (the target).\n",
        "    - (Line 8): `tf.io.decode_csv()` function takes 2 arguments: the line to parse and an array containing the default value for each column in the CSV file.\n",
        "\n",
        "3. (Lines 9-10): `decode_csv()` function returns a list of scalar tensors (1 per column) but we need to return 1D tensor arrays.\n",
        "    - (Line 9): Call `tf.stack()` on all tensors except last one (the target) to stack tensors into a 1D array.\n",
        "    - (Line 10): Call `tf.stack()` on target value to stack into a 1D tensor array with a single value, rather than a scalar tensor.\n",
        "\n",
        "4. (Line 11): Scale the input features by subtracting the feature means `(x - X_mean)`, dividing by the feature standard deviation `/ X_std`, and return a tuple containing the scaled features and the target.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wANud2vYQbvk",
        "outputId": "5a785a75-3eae-4fe5-8667-8684ecba031c"
      },
      "source": [
        "# Test preprocessing function\n",
        "preprocess(b'4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
              " array([ 0.16579159,  1.216324  , -0.05204396, -0.39210168, -0.5277444 ,\n",
              "        -0.26334172,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0VBpPyrrrlH"
      },
      "source": [
        "### 13.1.4 Putting Everything Together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de7jQo1VrrlI"
      },
      "source": [
        "### 13.1.5 Prefetching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S214FBxerrlJ"
      },
      "source": [
        "### 13.1.6 Using the Dataset with tf.keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9cVdvOYrrlJ"
      },
      "source": [
        "## 13.2 The TFRecord Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsCswQNMrrlL"
      },
      "source": [
        "### 13.2.1 Compressed TFRecord Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux4lBCsSrrlM"
      },
      "source": [
        "### 13.2.2 A Brief Introduction to Protocol Buffers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9orkoC-hrrlO"
      },
      "source": [
        "### 13.2.3 TensorFlow Protobufs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-UY2DdorrlP"
      },
      "source": [
        "### 13.2.4 Loading and Parsing Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_N5mt_hrrlP"
      },
      "source": [
        "### 13.2.5 Handling Lists of Lists Using the SequenceExample Protobuf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iOTJwEQrrlP"
      },
      "source": [
        "## 13.3 Preprocessing the Input Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi459OyWrrlQ"
      },
      "source": [
        "### 13.3.1 Encoding Categorical Features Using One-Hot Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqiTx2-grrlQ"
      },
      "source": [
        "### 13.3.2 Encoding Categorical Features Using Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQRP9A67rrlQ"
      },
      "source": [
        "### 13.3.3 Keras Preprocessing Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amrKe1DirrlR"
      },
      "source": [
        "## 13.4 TF Transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxyCzpDLrrlR"
      },
      "source": [
        "## 13.5 The TensorFlow Datasets (TFDS) Project"
      ]
    }
  ]
}