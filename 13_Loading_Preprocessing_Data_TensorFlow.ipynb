{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.0 64-bit ('venv')",
      "metadata": {
        "interpreter": {
          "hash": "7b58ff03a81b7b220740b362f01ec4719380627cca970f4b363191d14b8c12ac"
        }
      }
    },
    "colab": {
      "name": "13_Loading_Preprocessing_Data_TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh-BDFROrrlC"
      },
      "source": [
        "# Chapter 13: Loading and Preprocessing Data with TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Xp4aRNsVSf"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrbI82TtzLnQ"
      },
      "source": [
        "Since Deep Learning systems are often trained on very large datasets that will not fit in RAM, TensorFlow's **Data API** solves this issue by taking care of all the implementation details and only needs:\n",
        "- A dataset object\n",
        "- Where to get the data\n",
        "- How to transform it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITP8mk7arrlF"
      },
      "source": [
        "## 13.1 The Data API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i4gXmC5stZG"
      },
      "source": [
        "The Data API revolves around the concept of a **dataset**: a sequence of data items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvSahjM8s3SU",
        "outputId": "572b99fd-58db-4ffe-ab1f-bfd2e3073534"
      },
      "source": [
        "# Create a dataset entirely in RAM\n",
        "X = tf.range(10) # any data tensor\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: (), types: tf.int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj-6u4tatQuY"
      },
      "source": [
        "The `from_tensor_slices()` function takes a tensor and creates a `tf.data.Dataset` whose elements are all the slices of X. This is the same as `tf.data.Dataset.range(10)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8RIx3hutkst",
        "outputId": "d3341a91-d9e0-447d-d24f-8b3b49070447"
      },
      "source": [
        "# Iterate over dataset's items\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(3, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(7, shape=(), dtype=int32)\n",
            "tf.Tensor(8, shape=(), dtype=int32)\n",
            "tf.Tensor(9, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1pXrKLlrrlG"
      },
      "source": [
        "### 13.1.1 Chaining Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QmJqp6_t32e"
      },
      "source": [
        "Once you have a dataset, you can apply transformations by calling its transformation methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UV0TpO65t-gX",
        "outputId": "0555b6eb-1eb6-494f-8e1b-751d947559c2"
      },
      "source": [
        "# See Figure 13-1. Chaining dataset transformations\n",
        "dataset = dataset.repeat(3).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
            "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
            "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_MvYsp4uqOA"
      },
      "source": [
        "With original dataset,\n",
        "1. Call `repeat(3)` to return a new dataset with 3 copies of the dataset.\n",
        "    - Calling with no arguments will result in a new dataset that repeats forever, so the code that iterates over the dataset must decide when to stop.\n",
        "\n",
        "2. Call `batch(7)` to return a new dataset that groups the items into batches of 7 items and any remaining items in the last batch (batch of 2).\n",
        "    - Add `drop_remainder=True` argument to drop this final batch.\n",
        "\n",
        "> Note: Dataset methods **do not** modify datasets; they create new ones (ie. assign with `dataset = ...`) or else nothing will happen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTBo5tcCwW9I",
        "outputId": "4e4c5611-957c-4679-841b-67f81cb4d8c7"
      },
      "source": [
        "# Creates new dataset with all items doubled\n",
        "dataset = dataset.map(lambda x: x * 2) # Items:[0,2,4,6,8,10,12]\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
            "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
            "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
            "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9sQk8RExFvR"
      },
      "source": [
        "While the `map()` method applies a transformation to each item, the `apply()` method applies a transformation to the dataset as a whole.\n",
        "\n",
        "> Note: `apply()` method is not used since `tf.data.Dataset.unbatch(dataset)` needs 1 argument for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogyiShKtxSrt",
        "outputId": "791a5678-db61-427c-d4ea-1281dd6c0c2e"
      },
      "source": [
        "# tf.data.experimental.unbatch() is now deprecated\n",
        "# Use tf.data.Dataset.unbatch()\n",
        "# Each item in the new dataset will be single-integer tensor\n",
        "dataset = tf.data.Dataset.unbatch(dataset)\n",
        "\n",
        "# Filter the dataset\n",
        "dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...\n",
        "\n",
        "# Look at just a few items from dataset\n",
        "for item in dataset.take(3):\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "tf.Tensor(4, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eqQ58KrrrlG"
      },
      "source": [
        "### 13.1.2 Shuffling the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZxXT6CbIJZU"
      },
      "source": [
        "Gradient Descent works best when the instances in the training set are independent and identically distributed (iid). A simple way to ensure this is to shuffle the instances, using the `shuffle()` method.\n",
        "\n",
        "> Note: You must specify the buffer size, and it's important to make it large enough, or else shuffling will not be very effective.\n",
        "\n",
        "> Note: By default, calling `repeat()` on a shuffled dataset will generate a new order at every iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLq55PaXIzB_",
        "outputId": "a1dea5ca-fd36-498d-fc5e-89a04d5c0ccf"
      },
      "source": [
        "dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times\n",
        "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
        "for item in dataset:\n",
        "    print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
            "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
            "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
            "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yAEokUOJvy-"
      },
      "source": [
        "For large datasets, simple shuffling-buffer may not be enough; buffer size is still small compared to the large dataset.\n",
        "\n",
        "Some solutions are:\n",
        "- Shuffle the source data itself.\n",
        "- Split the source data into multiple files, then read them in a random order during training.\n",
        "- Pick multiple files randomly and read them simultaneously, interleaving their records.\n",
        "- Add a shuffling buffer on top of all that using `shuffle()` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrTpfj6bKoNv"
      },
      "source": [
        "#### Interleaving lines from multiple files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GPeLYi2Lkq6",
        "outputId": "163813a6-5fda-4092-efb5-f1539c9302f7"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\n",
        "# Load CA housing dataset\n",
        "# Split into training, validation, test set\n",
        "# Scale sets\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_mean = scaler.mean_\n",
        "X_std = scaler.scale_"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbE6z-DQMD9W"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\n",
        "# Split housing set\n",
        "# Save into 20 CSV files\n",
        "\n",
        "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
        "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
        "    os.makedirs(housing_dir, exist_ok=True)\n",
        "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
        "\n",
        "    filepaths = []\n",
        "    m = len(data)\n",
        "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
        "        part_csv = path_format.format(name_prefix, file_idx)\n",
        "        filepaths.append(part_csv)\n",
        "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
        "            if header is not None:\n",
        "                f.write(header)\n",
        "                f.write(\"\\n\")\n",
        "            for row_idx in row_indices:\n",
        "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
        "                f.write(\"\\n\")\n",
        "    return filepaths\n",
        "\n",
        "train_data = np.c_[X_train, y_train]\n",
        "valid_data = np.c_[X_valid, y_valid]\n",
        "test_data = np.c_[X_test, y_test]\n",
        "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
        "header = \",\".join(header_cols)\n",
        "\n",
        "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
        "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
        "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cAIIpUFMkkw",
        "outputId": "492a6533-d4ad-49db-c535-bb11f6cd08d7"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\n",
        "# Read 1st few lines of CSV file in text mode\n",
        "\n",
        "with open(train_filepaths[0]) as f:\n",
        "    for i in range(5):\n",
        "        print(f.readline(), end=\"\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
            "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
            "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
            "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
            "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHPJQfv8M3V-",
        "outputId": "1bc55c04-13c6-49c9-ac6e-c0c3c2934bac"
      },
      "source": [
        "train_filepaths"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['datasets/housing/my_train_00.csv',\n",
              " 'datasets/housing/my_train_01.csv',\n",
              " 'datasets/housing/my_train_02.csv',\n",
              " 'datasets/housing/my_train_03.csv',\n",
              " 'datasets/housing/my_train_04.csv',\n",
              " 'datasets/housing/my_train_05.csv',\n",
              " 'datasets/housing/my_train_06.csv',\n",
              " 'datasets/housing/my_train_07.csv',\n",
              " 'datasets/housing/my_train_08.csv',\n",
              " 'datasets/housing/my_train_09.csv',\n",
              " 'datasets/housing/my_train_10.csv',\n",
              " 'datasets/housing/my_train_11.csv',\n",
              " 'datasets/housing/my_train_12.csv',\n",
              " 'datasets/housing/my_train_13.csv',\n",
              " 'datasets/housing/my_train_14.csv',\n",
              " 'datasets/housing/my_train_15.csv',\n",
              " 'datasets/housing/my_train_16.csv',\n",
              " 'datasets/housing/my_train_17.csv',\n",
              " 'datasets/housing/my_train_18.csv',\n",
              " 'datasets/housing/my_train_19.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-a-mWbKM9zl"
      },
      "source": [
        "# Create dataset with only these file paths\n",
        "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2nDVadGNLS9"
      },
      "source": [
        "> Note: By default, the `list_files()` function returns a dataset that shuffles the file paths.\n",
        "\n",
        "Next, call `interleave()` method to read from five files at a time and interleave their lines (skipping 1st line which is the header row)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgkeqN35Njg1"
      },
      "source": [
        "n_readers = 5\n",
        "dataset = filepath_dataset.interleave(\n",
        "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "    cycle_length=n_readers)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOiOYMnwOAl_"
      },
      "source": [
        "At this stage, there will be 7 datasets in all:\n",
        "- The filepath dataset\n",
        "- The interleave dataset\n",
        "- 5 `TextLineDatasets` created internally by the interleave dataset.\n",
        "\n",
        "When iterating over the interleave set, it will cycle through these 5 `TextLineDatasets` reading one line at a time until all datasets are out of items. Then it will get the next 5 file paths from `filepath_dataset` and interleave them the same way until it runs out of file paths.\n",
        "\n",
        "By default, `interleave()` does not use parallelism; it just reads one line at a time from each file. To read files in parallel, set `num_parallel_calls` argument to the number of threads you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O8ob8VLO6VO",
        "outputId": "70e5cdac-a7cb-487a-c3f5-bab20774a292"
      },
      "source": [
        "# 1st rows of 5 CSV files, chosen randomly\n",
        "\n",
        "for line in dataset.take(5):\n",
        "    print(line.numpy())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782'\n",
            "b'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215'\n",
            "b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\n",
            "b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\n",
            "b'3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXCb2tRurrlH"
      },
      "source": [
        "### 13.1.3 Preprocessing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpaLJbKKPxVF"
      },
      "source": [
        "# X_mean, X_std = # mean and scale of each feature in the training set\n",
        "# X_mean, X_std already assigned when loading CA dataset\n",
        "\n",
        "n_inputs = 8\n",
        "\n",
        "def preprocess(line):\n",
        "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
        "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "    x = tf.stack(fields[:-1])\n",
        "    y = tf.stack(fields[-1:])\n",
        "    return (x - X_mean) / X_std, y"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L86Id5qlQoiv"
      },
      "source": [
        "Code explanation:\n",
        "\n",
        "1. (Line 1): Precomputed the mean and standard deviation of each feature in the training set.\n",
        "\n",
        "2. (Lines 6-8): The `preprocess()` function takes one CSV line and starts by parsing it.\n",
        "    - (Line 7): All feature columns are floats and missing values should default to 0. And provide an empty array of type `tf.float32` as the default value for the last column (the target).\n",
        "    - (Line 8): `tf.io.decode_csv()` function takes 2 arguments: the line to parse and an array containing the default value for each column in the CSV file.\n",
        "\n",
        "3. (Lines 9-10): `decode_csv()` function returns a list of scalar tensors (1 per column) but we need to return 1D tensor arrays.\n",
        "    - (Line 9): Call `tf.stack()` on all tensors except last one (the target) to stack tensors into a 1D array.\n",
        "    - (Line 10): Call `tf.stack()` on target value to stack into a 1D tensor array with a single value, rather than a scalar tensor.\n",
        "\n",
        "4. (Line 11): Scale the input features by subtracting the feature means `(x - X_mean)`, dividing by the feature standard deviation `/ X_std`, and return a tuple containing the scaled features and the target.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wANud2vYQbvk",
        "outputId": "6d04f511-fb2a-4819-c8d0-0475c597b114"
      },
      "source": [
        "# Test preprocessing function\n",
        "preprocess(b'4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
              " array([ 0.16579159,  1.216324  , -0.05204396, -0.39210168, -0.5277444 ,\n",
              "        -0.26334172,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0VBpPyrrrlH"
      },
      "source": [
        "### 13.1.4 Putting Everything Together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7C9ebZO1PV2"
      },
      "source": [
        "Put everything together into a small helper function that will:\n",
        "- Create and return a dataset that will load CA housing data from multiple CSV files\n",
        "- Preprocess it\n",
        "- Shuffle it\n",
        "- Optionally repeat it\n",
        "- Batch it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov7Y6aMg1pSe"
      },
      "source": [
        "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
        "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
        "                       n_parse_threads=5, batch_size=32):\n",
        "    dataset = tf.data.Dataset.list_files(filepaths) # Create dataset from multiple CSV files\n",
        "    dataset = dataset.interleave(\n",
        "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
        "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads) # Calls preprocess function\n",
        "    dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat) # Shuffles dataset \"repeat\" times\n",
        "    return dataset.batch(batch_size).prefetch(1) # Important for performance"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de7jQo1VrrlI"
      },
      "source": [
        "### 13.1.5 Prefetching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJZulJyU3oV8"
      },
      "source": [
        "By calling `prefetch(1)` at the end, we are creating a dataset that will do its best to always be 1 (or multiple) batch(es) ahead. \n",
        "\n",
        "In other words, while our training algorithm is working on one batch, the dataset will already be working in parallel on getting the next batch ready (eg. reading the data from disk and preprocessing it).\n",
        "\n",
        "> Note: With prefetching, the CPU and GPU work in parallel: as the GPU works on one batch, the CPU works on the next.\n",
        "\n",
        "If the dataset is small enough to fit in memory, use dataset's `cache()` method to cache its content to RAM and speed up training. Do this **after** loading and preprocessing the data, but **before** shuffling, repeating, batching, and prefetching."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S214FBxerrlJ"
      },
      "source": [
        "### 13.1.6 Using the Dataset with tf.keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW1gCFKy5swL"
      },
      "source": [
        "# Use csv_reader_dataset() to create sets\n",
        "train_set = csv_reader_dataset(train_filepaths)\n",
        "valid_set = csv_reader_dataset(valid_filepaths)\n",
        "test_set = csv_reader_dataset(test_filepaths)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CBIUvJe6pVh",
        "outputId": "4da892dd-c99b-46b6-c808-f5e54e81ddb9"
      },
      "source": [
        "# From textbook notebook\n",
        "# Build and train Keras model\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "    keras.layers.Dense(1),\n",
        "])\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
        "\n",
        "model.fit(train_set, epochs=10, validation_data=valid_set)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "363/363 [==============================] - 2s 3ms/step - loss: 3.0959 - val_loss: 3.8690\n",
            "Epoch 2/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 1.0523 - val_loss: 1.1137\n",
            "Epoch 3/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.8122 - val_loss: 0.7239\n",
            "Epoch 4/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.7343 - val_loss: 0.6612\n",
            "Epoch 5/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6527 - val_loss: 0.6517\n",
            "Epoch 6/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6341 - val_loss: 0.6310\n",
            "Epoch 7/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.6299 - val_loss: 0.6054\n",
            "Epoch 8/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5916 - val_loss: 0.5891\n",
            "Epoch 9/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5682 - val_loss: 0.5564\n",
            "Epoch 10/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5330 - val_loss: 0.5215\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5bcabb39d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDB6KiKu7AxE",
        "outputId": "b5e6411c-1892-427d-f3e5-bc71c055cc9a"
      },
      "source": [
        "# Evaluate on test set and make prediction\n",
        "model.evaluate(test_set)\n",
        "new_set = test_set.take(3).map(lambda X, y: X) # pretend we have 3 new instances\n",
        "model.predict(new_set) # a dataset containing new instances"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "162/162 [==============================] - 0s 1ms/step - loss: 0.5228\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.3385925 ],\n",
              "       [1.3566871 ],\n",
              "       [1.975553  ],\n",
              "       [1.1848876 ],\n",
              "       [1.5380412 ],\n",
              "       [2.5135336 ],\n",
              "       [2.2623162 ],\n",
              "       [1.2559526 ],\n",
              "       [2.6903954 ],\n",
              "       [2.714336  ],\n",
              "       [1.8362496 ],\n",
              "       [2.0574398 ],\n",
              "       [1.942119  ],\n",
              "       [2.411251  ],\n",
              "       [1.0365776 ],\n",
              "       [0.78242517],\n",
              "       [1.3213642 ],\n",
              "       [2.1210408 ],\n",
              "       [1.818429  ],\n",
              "       [2.685921  ],\n",
              "       [1.5517488 ],\n",
              "       [2.303208  ],\n",
              "       [2.1002278 ],\n",
              "       [1.8787147 ],\n",
              "       [1.8928831 ],\n",
              "       [2.6535788 ],\n",
              "       [2.1432931 ],\n",
              "       [1.6555791 ],\n",
              "       [2.926011  ],\n",
              "       [3.245479  ],\n",
              "       [1.0477158 ],\n",
              "       [0.85796463],\n",
              "       [1.278811  ],\n",
              "       [1.5953363 ],\n",
              "       [1.6979065 ],\n",
              "       [3.4787908 ],\n",
              "       [1.5887406 ],\n",
              "       [2.496935  ],\n",
              "       [3.0445418 ],\n",
              "       [2.0707269 ],\n",
              "       [2.5267224 ],\n",
              "       [2.5330534 ],\n",
              "       [3.4804492 ],\n",
              "       [0.77085966],\n",
              "       [3.8585725 ],\n",
              "       [1.5676365 ],\n",
              "       [3.6967864 ],\n",
              "       [1.776646  ],\n",
              "       [1.9306016 ],\n",
              "       [1.2505447 ],\n",
              "       [2.44846   ],\n",
              "       [2.4583907 ],\n",
              "       [1.1052907 ],\n",
              "       [2.2794266 ],\n",
              "       [2.4463267 ],\n",
              "       [2.7751799 ],\n",
              "       [0.9593088 ],\n",
              "       [1.3960657 ],\n",
              "       [1.0992378 ],\n",
              "       [1.1609217 ],\n",
              "       [1.5225141 ],\n",
              "       [1.6614828 ],\n",
              "       [1.6341687 ],\n",
              "       [2.9423337 ],\n",
              "       [2.1151907 ],\n",
              "       [1.0627215 ],\n",
              "       [2.5984626 ],\n",
              "       [2.814077  ],\n",
              "       [0.59269845],\n",
              "       [2.2759132 ],\n",
              "       [2.1180294 ],\n",
              "       [5.558236  ],\n",
              "       [1.8572055 ],\n",
              "       [1.9872177 ],\n",
              "       [0.96959066],\n",
              "       [1.6656854 ],\n",
              "       [2.6435957 ],\n",
              "       [1.1506864 ],\n",
              "       [1.5844939 ],\n",
              "       [2.3743312 ],\n",
              "       [6.325423  ],\n",
              "       [1.6709833 ],\n",
              "       [1.3414682 ],\n",
              "       [2.002791  ],\n",
              "       [1.6535814 ],\n",
              "       [0.6405405 ],\n",
              "       [1.6653508 ],\n",
              "       [2.3194337 ],\n",
              "       [3.7059941 ],\n",
              "       [1.9020087 ],\n",
              "       [2.8478203 ],\n",
              "       [2.7843728 ],\n",
              "       [1.4878148 ],\n",
              "       [1.5322847 ],\n",
              "       [1.9400336 ],\n",
              "       [2.141818  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GE4uhVD8B-_"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\n",
        "\n",
        "# To build custom training loop,\n",
        "# iterate over the training set\n",
        "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "global_step = 0\n",
        "for X_batch, y_batch in train_set:\n",
        "    global_step += 1\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(X_batch)\n",
        "        main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "        loss = tf.add_n([main_loss] + model.losses)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuSnDhqg9lFq"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\n",
        "\n",
        "# TF Function that performs the whole training loop\n",
        "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
        "loss_fn = keras.losses.mean_squared_error\n",
        "\n",
        "@tf.function\n",
        "def train(model, n_epochs, batch_size=32,\n",
        "          n_readers=5, n_read_threads=5, shuffle_buffer_size=10000, n_parse_threads=5):\n",
        "    train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, n_readers=n_readers,\n",
        "                       n_read_threads=n_read_threads, shuffle_buffer_size=shuffle_buffer_size,\n",
        "                       n_parse_threads=n_parse_threads, batch_size=batch_size)\n",
        "    for X_batch, y_batch in train_set:\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_batch)\n",
        "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "            loss = tf.add_n([main_loss] + model.losses)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "train(model, 5)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9cVdvOYrrlJ"
      },
      "source": [
        "## 13.2 The TFRecord Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsCswQNMrrlL"
      },
      "source": [
        "### 13.2.1 Compressed TFRecord Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux4lBCsSrrlM"
      },
      "source": [
        "### 13.2.2 A Brief Introduction to Protocol Buffers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9orkoC-hrrlO"
      },
      "source": [
        "### 13.2.3 TensorFlow Protobufs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-UY2DdorrlP"
      },
      "source": [
        "### 13.2.4 Loading and Parsing Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_N5mt_hrrlP"
      },
      "source": [
        "### 13.2.5 Handling Lists of Lists Using the SequenceExample Protobuf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iOTJwEQrrlP"
      },
      "source": [
        "## 13.3 Preprocessing the Input Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi459OyWrrlQ"
      },
      "source": [
        "### 13.3.1 Encoding Categorical Features Using One-Hot Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqiTx2-grrlQ"
      },
      "source": [
        "### 13.3.2 Encoding Categorical Features Using Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQRP9A67rrlQ"
      },
      "source": [
        "### 13.3.3 Keras Preprocessing Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amrKe1DirrlR"
      },
      "source": [
        "## 13.4 TF Transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxyCzpDLrrlR"
      },
      "source": [
        "## 13.5 The TensorFlow Datasets (TFDS) Project"
      ]
    }
  ]
}