{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.0 64-bit ('venv')",
      "metadata": {
        "interpreter": {
          "hash": "7b58ff03a81b7b220740b362f01ec4719380627cca970f4b363191d14b8c12ac"
        }
      }
    },
    "colab": {
      "name": "11_Training_Deep_Neural_Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElBOxYD0oH7j"
      },
      "source": [
        "# Chapter 11: Training Deep Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXpkzwGAotfe"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SAWjzOroH7t"
      },
      "source": [
        "Some problems training a deep DNN (deep neural networks):\n",
        "- Vanishing or exploding gradients problem. Gradients grow smaller and smaller / bigger and bigger and makes training lower layers very hard.\n",
        "- Not enough training data, or too costly to label.\n",
        "- Training may be extremely slow.\n",
        "- Model with millions of parameters risk overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Pcawp4oH7v"
      },
      "source": [
        "## 11.1 The Vanishing/Exploding Gradients Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ss4SNvJoH7v"
      },
      "source": [
        "Backpropagation computes and propagates the error gradient of each layer from output to input. \n",
        "\n",
        "**Vanishing gradients problem** - Gradients get smaller and smaller as it progresses to lower layers. So Gradient Descent leaves the lower layers' connection weights virtually unchanged and never converges to a good solution.\n",
        "\n",
        "**Exploding gradients problem** - Similar effect but gradients get bigger and bigger and algorithm diverges.\n",
        "\n",
        "> In general, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.\n",
        "\n",
        "It was discovered that using logistic sigmoid activation function and initializing weights using a normal distribution ($\\mu=0, \\sigma = 1$) caused this issue.\n",
        "\n",
        "- Initializing weights using a normal distribution:\n",
        "    - The variance (the spread) of the outputs of each layer is much greater than its inputs.\n",
        "    \n",
        "    - Going forward in the network, the variance keeps increasing after each layer until the activation function saturates (ends up far right/left) at the top layers.\n",
        "\n",
        "- Logistic sigmoid activation function:\n",
        "    - Because the variance keeps increasing, inputs become large (negative or positive, \"far left/right\"), with outputs of 0 or 1 and derivative extremely close to 0.\n",
        "    - So backpropagation has no error gradient to propagate to the lower layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbGgiiBToH7w"
      },
      "source": [
        "### 11.1.1 Glorot and He Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LJ0ru-NoH7w"
      },
      "source": [
        "For the signal to flow properly, we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction.\n",
        "\n",
        "> Microphone Amplifier Analogy: Setting knob too close to 0, voice is inaudible but too close to max, voice is too saturated. For a chain of amplifiers, they all need to be set properly so that voice is loud and clear at the end of the chain.  \n",
        "\n",
        "> Your voice has to come out of each amplifier at the same amplitude as it came in.\n",
        "\n",
        "It's impossible to guarantee both (output & gradient variances) unless the layer has an equal number of inputs and neurons (*fan-in*, *fan-out*). Glorot and Bengio proposed a good compromise that the connection weights to be initialized randomly according to *Equation 11-1*, where $\\text{fan}_\\text{avg} = (\\text{fan}_\\text{in} + \\text{fan}_\\text{out})/2$ and is called **Xavier initialization** or **Glorot initialization**. This strategy is used for the logistic activation function.\n",
        "\n",
        "**LeCun initialization** - Equivalent to Glorot initialization when $\\text{fan}_\\text{in} = \\text{fan}_\\text{out} $.\n",
        "\n",
        "**He initialization** - The initialization strategy for the ReLU activation function (and its variants)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GJ0qLbZog6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25890a83-ed88-4756-c7d7-05a29528ece6"
      },
      "source": [
        "# Default is Glorot with uniform distribution \r\n",
        "# Change to He initialization\r\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\r\n",
        "\r\n",
        "# He initialization with uniform distribution based on fan_avg rather than fan_in\r\n",
        "# Use VarianceScaling\r\n",
        "he_avg_init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg',\r\n",
        "                                                 distribution='uniform')\r\n",
        "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.core.Dense at 0x7f6a37de7d10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgUq6SZjoH7x"
      },
      "source": [
        "### 11.1.2 Nonsaturating Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6mG0mVtG2ic"
      },
      "source": [
        "**Dying ReLUs** - During training, some neurons effectively \"die,\" meaning they stop outputting anything other than 0.\r\n",
        "\r\n",
        "A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting 0s, and Gradient Descent does not affect it anymore because the gradient of the ReLU function is 0 when its input is negative.\r\n",
        "\r\n",
        "To solve this problem, use **leaky ReLU** where $\\text{LeakyReLU}_\\alpha(z) = \\text{max}(\\alpha z, z) $. The hyperparameter $\\alpha$ defines how much the function \"leaks\": it is the slope of the function for $z<0$ and is typically set to $0.01$.\r\n",
        "\r\n",
        "> Leaky ReLU is just like ReLU, but with a small slope for negative values. A small slope ensures that leaky ReLUs never die.\r\n",
        "\r\n",
        "**Exponential linear unit (ELU)** - Outperforms all the ReLU variants. Looks a lot like the ReLU function with some major differences:\r\n",
        "- Takes on nagative values when $z<0$, allowing an average output closer to 0 and alleviating the vanishing gradients problem.\r\n",
        "- Hyperparameter $\\alpha$ defines the value when z is a large negative number (usually set to 1).\r\n",
        "- Has nonzero gradient for $z<0$, avoiding the dead neurons problem.\r\n",
        "- If $\\alpha = 1$, then the function is smooth everywhere including around $z=0$, speeding up Gradient Descent.\r\n",
        "\r\n",
        "The main drawback of the ELU function is that it is slower to compute than the ReLU function and its variants (due to the exponential function).\r\n",
        "\r\n",
        "**Scaled ELU (SELU)** - A scaled variant of the ELU activation function. The network will *self-normalize* ($\\mu =0, \\sigma =1$) but under specific conditions:\r\n",
        "- Input features must be standarized ($\\mu =0, \\sigma =1$).\r\n",
        "- Every hidden layer's weights must be initialized with LeCun normal initialization.\r\n",
        "- Network's architecture must be sequential.\r\n",
        "\r\n",
        "> In general, SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic.\r\n",
        "\r\n",
        "> Because ReLU is the most used activation function, many libraries and hardware accelerators provide ReLU-specific optimizations; if speed is your priority, ReLU might be the best choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kknDGFptMF_1"
      },
      "source": [
        "# On fashion MNIST as example\r\n",
        "\r\n",
        "# For leaky ReLU\r\n",
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),                  # FROM TEXTBOOK NOTEBOOK \r\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),     # FROM TEXTBOOK NOTEBOOK \r\n",
        "    keras.layers.LeakyReLU(alpha=0.2),                           # Activation function after each layer\r\n",
        "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\r\n",
        "    keras.layers.LeakyReLU(alpha=0.2)\r\n",
        "])\r\n",
        "\r\n",
        "# For PReLU\r\n",
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),                  # FROM TEXTBOOK NOTEBOOK \r\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),     # FROM TEXTBOOK NOTEBOOK \r\n",
        "    keras.layers.PReLU(),                                        # Activation function after each layer\r\n",
        "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\r\n",
        "    keras.layers.PReLU()\r\n",
        "])\r\n",
        "\r\n",
        "# For SELU\r\n",
        "layer = keras.layers.Dense(10, activation=\"selu\",\r\n",
        "                           kernel_initializer=\"lecun_normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2URhcKtoH7y"
      },
      "source": [
        "### 11.1.3 Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sAV0hhGoH7y"
      },
      "source": [
        "### 11.1.4 Gradient Clipping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUqMDM1joH7y"
      },
      "source": [
        "## 11.2 Reusing Pretrained Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbSwOD0toH7z"
      },
      "source": [
        "### 11.2.1 Transfer Learning with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHu3akFJoH7z"
      },
      "source": [
        "### 11.2.2 Unsupervised Pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs3N-SonoH70"
      },
      "source": [
        "### 11.2.3 Pretraining on an Auxiliary Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yIBSoZuoH70"
      },
      "source": [
        "## 11.3 Faster Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96ynKD5_oH71"
      },
      "source": [
        "### 11.3.1 Momentum Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH7ZwtwRoH71"
      },
      "source": [
        "### 11.3.2 Nesterov Accelerated Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VkENr_ooH71"
      },
      "source": [
        "### 11.3.3 AdaGrad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRjiUCp3oH72"
      },
      "source": [
        "### 11.3.4 RMSProp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL-hc4zToH72"
      },
      "source": [
        "### 11.3.5 Adam and Nadam Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHJGaPqqoH72"
      },
      "source": [
        "### 11.3.6 Learning Rate Scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etBuzcsHoH73"
      },
      "source": [
        "## 11.4 Avoiding Overfitting Through Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsQQYq4NoH73"
      },
      "source": [
        "### 11.4.1 $\\ell_1$ and $\\ell_2$ Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6i4g_OJoH73"
      },
      "source": [
        "### 11.4.2 Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFjCtsUjoH74"
      },
      "source": [
        "### 11.4.3 Monte Carlo (MC) Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd_2iKlCoH74"
      },
      "source": [
        "### 11.4.4 Max-Norm Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Gvo0vZoH74"
      },
      "source": [
        "## 11.5 Summary and Practical Guidelines"
      ]
    }
  ]
}