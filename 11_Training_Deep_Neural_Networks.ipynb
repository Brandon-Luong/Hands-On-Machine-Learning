{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.0 64-bit ('venv')",
      "metadata": {
        "interpreter": {
          "hash": "7b58ff03a81b7b220740b362f01ec4719380627cca970f4b363191d14b8c12ac"
        }
      }
    },
    "colab": {
      "name": "11_Training_Deep_Neural_Networks.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElBOxYD0oH7j"
      },
      "source": [
        "# Chapter 11: Training Deep Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXpkzwGAotfe"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SAWjzOroH7t"
      },
      "source": [
        "Some problems training a deep DNN (deep neural networks):\n",
        "- Vanishing or exploding gradients problem. Gradients grow smaller and smaller / bigger and bigger and makes training lower layers very hard.\n",
        "- Not enough training data, or too costly to label.\n",
        "- Training may be extremely slow.\n",
        "- Model with millions of parameters risk overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Pcawp4oH7v"
      },
      "source": [
        "## 11.1 The Vanishing/Exploding Gradients Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ss4SNvJoH7v"
      },
      "source": [
        "Backpropagation computes and propagates the error gradient of each layer from output to input. \n",
        "\n",
        "**Vanishing gradients problem** - Gradients get smaller and smaller as it progresses to lower layers. So Gradient Descent leaves the lower layers' connection weights virtually unchanged and never converges to a good solution.\n",
        "\n",
        "**Exploding gradients problem** - Similar effect but gradients get bigger and bigger and algorithm diverges.\n",
        "\n",
        "> In general, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.\n",
        "\n",
        "It was discovered that using logistic sigmoid activation function and initializing weights using a normal distribution ($\\mu=0, \\sigma = 1$) caused this issue.\n",
        "\n",
        "- Initializing weights using a normal distribution:\n",
        "    - The variance (the spread) of the outputs of each layer is much greater than its inputs.\n",
        "    \n",
        "    - Going forward in the network, the variance keeps increasing after each layer until the activation function saturates (ends up far right/left) at the top layers.\n",
        "\n",
        "- Logistic sigmoid activation function:\n",
        "    - Because the variance keeps increasing, inputs become large (negative or positive, \"far left/right\"), with outputs of 0 or 1 and derivative extremely close to 0.\n",
        "    - So backpropagation has no error gradient to propagate to the lower layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbGgiiBToH7w"
      },
      "source": [
        "### 11.1.1 Glorot and He Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LJ0ru-NoH7w"
      },
      "source": [
        "For the signal to flow properly, we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction.\n",
        "\n",
        "> Microphone Amplifier Analogy: Setting knob too close to 0, voice is inaudible but too close to max, voice is too saturated. For a chain of amplifiers, they all need to be set properly so that voice is loud and clear at the end of the chain.  \n",
        "\n",
        "> Your voice has to come out of each amplifier at the same amplitude as it came in.\n",
        "\n",
        "It's impossible to guarantee both (output & gradient variances) unless the layer has an equal number of inputs and neurons (*fan-in*, *fan-out*). Glorot and Bengio proposed a good compromise that the connection weights to be initialized randomly according to *Equation 11-1*, where $\\text{fan}_\\text{avg} = (\\text{fan}_\\text{in} + \\text{fan}_\\text{out})/2$ and is called **Xavier initialization** or **Glorot initialization**. This strategy is used for the logistic activation function.\n",
        "\n",
        "**LeCun initialization** - Equivalent to Glorot initialization when $\\text{fan}_\\text{in} = \\text{fan}_\\text{out} $.\n",
        "\n",
        "**He initialization** - The initialization strategy for the ReLU activation function (and its variants)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GJ0qLbZog6J",
        "outputId": "25890a83-ed88-4756-c7d7-05a29528ece6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Default is Glorot with uniform distribution \r\n",
        "# Change to He initialization\r\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\r\n",
        "\r\n",
        "# He initialization with uniform distribution based on fan_avg rather than fan_in\r\n",
        "# Use VarianceScaling\r\n",
        "he_avg_init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg',\r\n",
        "                                                 distribution='uniform')\r\n",
        "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.core.Dense at 0x7f6a37de7d10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgUq6SZjoH7x"
      },
      "source": [
        "### 11.1.2 Nonsaturating Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2URhcKtoH7y"
      },
      "source": [
        "### 11.1.3 Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sAV0hhGoH7y"
      },
      "source": [
        "### 11.1.4 Gradient Clipping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUqMDM1joH7y"
      },
      "source": [
        "## 11.2 Reusing Pretrained Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbSwOD0toH7z"
      },
      "source": [
        "### 11.2.1 Transfer Learning with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHu3akFJoH7z"
      },
      "source": [
        "### 11.2.2 Unsupervised Pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs3N-SonoH70"
      },
      "source": [
        "### 11.2.3 Pretraining on an Auxiliary Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yIBSoZuoH70"
      },
      "source": [
        "## 11.3 Faster Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96ynKD5_oH71"
      },
      "source": [
        "### 11.3.1 Momentum Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH7ZwtwRoH71"
      },
      "source": [
        "### 11.3.2 Nesterov Accelerated Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VkENr_ooH71"
      },
      "source": [
        "### 11.3.3 AdaGrad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRjiUCp3oH72"
      },
      "source": [
        "### 11.3.4 RMSProp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL-hc4zToH72"
      },
      "source": [
        "### 11.3.5 Adam and Nadam Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHJGaPqqoH72"
      },
      "source": [
        "### 11.3.6 Learning Rate Scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etBuzcsHoH73"
      },
      "source": [
        "## 11.4 Avoiding Overfitting Through Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsQQYq4NoH73"
      },
      "source": [
        "### 11.4.1 $\\ell_1$ and $\\ell_2$ Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6i4g_OJoH73"
      },
      "source": [
        "### 11.4.2 Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFjCtsUjoH74"
      },
      "source": [
        "### 11.4.3 Monte Carlo (MC) Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd_2iKlCoH74"
      },
      "source": [
        "### 11.4.4 Max-Norm Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Gvo0vZoH74"
      },
      "source": [
        "## 11.5 Summary and Practical Guidelines"
      ]
    }
  ]
}