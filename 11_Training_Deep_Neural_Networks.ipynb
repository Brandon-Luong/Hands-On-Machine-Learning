{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.0 64-bit ('venv')",
      "metadata": {
        "interpreter": {
          "hash": "7b58ff03a81b7b220740b362f01ec4719380627cca970f4b363191d14b8c12ac"
        }
      }
    },
    "colab": {
      "name": "11_Training_Deep_Neural_Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElBOxYD0oH7j"
      },
      "source": [
        "# Chapter 11: Training Deep Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXpkzwGAotfe"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SAWjzOroH7t"
      },
      "source": [
        "Some problems training a deep DNN (deep neural networks):\n",
        "- Vanishing or exploding gradients problem. Gradients grow smaller and smaller / bigger and bigger and makes training lower layers very hard.\n",
        "- Not enough training data, or too costly to label.\n",
        "- Training may be extremely slow.\n",
        "- Model with millions of parameters risk overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Pcawp4oH7v"
      },
      "source": [
        "## 11.1 The Vanishing/Exploding Gradients Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ss4SNvJoH7v"
      },
      "source": [
        "Backpropagation computes and propagates the error gradient of each layer from output to input. \n",
        "\n",
        "**Vanishing gradients problem** - Gradients get smaller and smaller as it progresses to lower layers. So Gradient Descent leaves the lower layers' connection weights virtually unchanged and never converges to a good solution.\n",
        "\n",
        "**Exploding gradients problem** - Similar effect but gradients get bigger and bigger and algorithm diverges.\n",
        "\n",
        "> In general, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.\n",
        "\n",
        "It was discovered that using logistic sigmoid activation function and initializing weights using a normal distribution ($\\mu=0, \\sigma = 1$) caused this issue.\n",
        "\n",
        "- Initializing weights using a normal distribution:\n",
        "    - The variance (the spread) of the outputs of each layer is much greater than its inputs.\n",
        "    \n",
        "    - Going forward in the network, the variance keeps increasing after each layer until the activation function saturates (ends up far right/left) at the top layers.\n",
        "\n",
        "- Logistic sigmoid activation function:\n",
        "    - Because the variance keeps increasing, inputs become large (negative or positive, \"far left/right\"), with outputs of 0 or 1 and derivative extremely close to 0.\n",
        "    - So backpropagation has no error gradient to propagate to the lower layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbGgiiBToH7w"
      },
      "source": [
        "### 11.1.1 Glorot and He Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LJ0ru-NoH7w"
      },
      "source": [
        "For the signal to flow properly, we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction.\n",
        "\n",
        "> Microphone Amplifier Analogy: Setting knob too close to 0, voice is inaudible but too close to max, voice is too saturated. For a chain of amplifiers, they all need to be set properly so that voice is loud and clear at the end of the chain.  \n",
        "\n",
        "> Your voice has to come out of each amplifier at the same amplitude as it came in.\n",
        "\n",
        "It's impossible to guarantee both (output & gradient variances) unless the layer has an equal number of inputs and neurons (*fan-in*, *fan-out*). Glorot and Bengio proposed a good compromise that the connection weights to be initialized randomly according to *Equation 11-1*, where $\\text{fan}_\\text{avg} = (\\text{fan}_\\text{in} + \\text{fan}_\\text{out})/2$ and is called **Xavier initialization** or **Glorot initialization**. This strategy is used for the logistic activation function.\n",
        "\n",
        "**LeCun initialization** - Equivalent to Glorot initialization when $\\text{fan}_\\text{in} = \\text{fan}_\\text{out} $.\n",
        "\n",
        "**He initialization** - The initialization strategy for the ReLU activation function (and its variants)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GJ0qLbZog6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25890a83-ed88-4756-c7d7-05a29528ece6"
      },
      "source": [
        "# Default is Glorot with uniform distribution \r\n",
        "# Change to He initialization\r\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\r\n",
        "\r\n",
        "# He initialization with uniform distribution based on fan_avg rather than fan_in\r\n",
        "# Use VarianceScaling\r\n",
        "he_avg_init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg',\r\n",
        "                                                 distribution='uniform')\r\n",
        "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.core.Dense at 0x7f6a37de7d10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgUq6SZjoH7x"
      },
      "source": [
        "### 11.1.2 Nonsaturating Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6mG0mVtG2ic"
      },
      "source": [
        "**Dying ReLUs** - During training, some neurons effectively \"die,\" meaning they stop outputting anything other than 0.\r\n",
        "\r\n",
        "A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting 0s, and Gradient Descent does not affect it anymore because the gradient of the ReLU function is 0 when its input is negative.\r\n",
        "\r\n",
        "To solve this problem, use **leaky ReLU** where $\\text{LeakyReLU}_\\alpha(z) = \\text{max}(\\alpha z, z) $. The hyperparameter $\\alpha$ defines how much the function \"leaks\": it is the slope of the function for $z<0$ and is typically set to $0.01$.\r\n",
        "\r\n",
        "> Leaky ReLU is just like ReLU, but with a small slope for negative values. A small slope ensures that leaky ReLUs never die.\r\n",
        "\r\n",
        "**Exponential linear unit (ELU)** - Outperforms all the ReLU variants. Looks a lot like the ReLU function with some major differences:\r\n",
        "- Takes on nagative values when $z<0$, allowing an average output closer to 0 and alleviating the vanishing gradients problem.\r\n",
        "- Hyperparameter $\\alpha$ defines the value when z is a large negative number (usually set to 1).\r\n",
        "- Has nonzero gradient for $z<0$, avoiding the dead neurons problem.\r\n",
        "- If $\\alpha = 1$, then the function is smooth everywhere including around $z=0$, speeding up Gradient Descent.\r\n",
        "\r\n",
        "The main drawback of the ELU function is that it is slower to compute than the ReLU function and its variants (due to the exponential function).\r\n",
        "\r\n",
        "**Scaled ELU (SELU)** - A scaled variant of the ELU activation function. The network will *self-normalize* ($\\mu =0, \\sigma =1$) but under specific conditions:\r\n",
        "- Input features must be standarized ($\\mu =0, \\sigma =1$).\r\n",
        "- Every hidden layer's weights must be initialized with LeCun normal initialization.\r\n",
        "- Network's architecture must be sequential.\r\n",
        "\r\n",
        "> In general, SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic.\r\n",
        "\r\n",
        "> Because ReLU is the most used activation function, many libraries and hardware accelerators provide ReLU-specific optimizations; if speed is your priority, ReLU might be the best choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kknDGFptMF_1"
      },
      "source": [
        "# On fashion MNIST as example\r\n",
        "\r\n",
        "# For leaky ReLU\r\n",
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),                  # FROM TEXTBOOK NOTEBOOK \r\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),     # FROM TEXTBOOK NOTEBOOK \r\n",
        "    keras.layers.LeakyReLU(alpha=0.2),                           # Activation function after each layer\r\n",
        "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\r\n",
        "    keras.layers.LeakyReLU(alpha=0.2)\r\n",
        "])\r\n",
        "\r\n",
        "# For PReLU\r\n",
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),                  # FROM TEXTBOOK NOTEBOOK \r\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),     # FROM TEXTBOOK NOTEBOOK \r\n",
        "    keras.layers.PReLU(),                                        # Activation function after each layer\r\n",
        "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\r\n",
        "    keras.layers.PReLU()\r\n",
        "])\r\n",
        "\r\n",
        "# For SELU\r\n",
        "layer = keras.layers.Dense(10, activation=\"selu\",\r\n",
        "                           kernel_initializer=\"lecun_normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2URhcKtoH7y"
      },
      "source": [
        "### 11.1.3 Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibk-uwfaQux1"
      },
      "source": [
        "Although different initialization and a variant activation function reduces the vanishing/exploding gradients problem, it doesn't guarantee that they won't come back during training.\r\n",
        "\r\n",
        "**Batch Normalization** - Adding an operation in the model just before or after the activation function of each hidden layer. This operation simply zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: 1 for scaling, other for shifting.\r\n",
        "\r\n",
        "> If you add a BN layer as the very first layer, you do not need to standardize your training set (eg. using `StandardScaler`).\r\n",
        "\r\n",
        "Come testing a new instance, how to calculate the batch mean/standard deviation?\r\n",
        "\r\n",
        "Estimate the final statistics (overall to use on new instance, instead of the batch values) by using a moving average of the layer's input means and standard deviation.\r\n",
        "\r\n",
        "4 parameters are learned in each batch-normalized layer:\r\n",
        "- $\\mathbf{\\gamma}$, the output scale vector\r\n",
        "- $\\mathbf{\\beta}$, the output offset vector\r\n",
        "- $\\mathbf{\\mu}$, the final input mean vector\r\n",
        "- $\\mathbf{\\sigma}$, the final input standard deviation vector\r\n",
        "\r\n",
        "> $\\mathbf{\\mu}$ and $\\mathbf{\\sigma}$ are estimated during training, but only used after training (to replace batch input means and standard deviation).\r\n",
        "\r\n",
        "Batch Normalization acts like a regularizer, reducing the need for other regularization techniques.\r\n",
        "\r\n",
        "There is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer. Fortunately, it's often possible to fuse the BN layer with the previous layer, after training, thereby avoiding the runtime penalty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnUrN84fUp8v"
      },
      "source": [
        "#### Implementing Batch Normalization with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-R9PrMlU6Au"
      },
      "source": [
        "Just add a `BatchNormalization` layer before or after each hidden layer's activation function.\r\n",
        "\r\n",
        "For example, this model applies BN after every hidden layer and as the first layer in the model (after flattening the input images)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuJ2LUSIVNSN",
        "outputId": "5f4e5f13-6d26-4432-e655-34c6def1e3fe"
      },
      "source": [
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\r\n",
        "    keras.layers.BatchNormalization(),\r\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\r\n",
        "    keras.layers.BatchNormalization(),\r\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\r\n",
        "    keras.layers.BatchNormalization(),\r\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\r\n",
        "])\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 271,346\n",
            "Trainable params: 268,978\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUTd3QL7WTMr"
      },
      "source": [
        "Bn layer adds 4 parameters per input: $\\gamma, \\beta, \\mu, \\sigma$. So for the first BN layer, $4 \\times 784 = 3,136$ parameters.\r\n",
        "\r\n",
        "$\\mu, \\sigma$ are moving averages and \"non-trainable\" so $(3,136 + 1,200 + 400) / 2 = 2,368$ the total number of non-trainable parameters in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yih8G_oW5y-",
        "outputId": "861bc631-030b-4452-ab7b-ce17b0c5027a"
      },
      "source": [
        "[(var.name, var.trainable) for var in model.layers[1].variables]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('batch_normalization/gamma:0', True),\n",
              " ('batch_normalization/beta:0', True),\n",
              " ('batch_normalization/moving_mean:0', False),\n",
              " ('batch_normalization/moving_variance:0', False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6_1igPGXH6k",
        "outputId": "bdaac4a5-3b77-4888-d330-2ce44214d075"
      },
      "source": [
        "# layers.updates method is deprecated\r\n",
        "model.layers[1].updates"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1402: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`layer.updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN8WtPx6ZALH"
      },
      "source": [
        "# To add BN layers before the activation functions\r\n",
        "# Remove activation function from hidden layers\r\n",
        "# Add them as separate layers after BN layers\r\n",
        "# Remove bias term from previous layer, 'use_bias=False'\r\n",
        "\r\n",
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\r\n",
        "    keras.layers.BatchNormalization(),\r\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\r\n",
        "    keras.layers.BatchNormalization(),\r\n",
        "    keras.layers.Activation(\"elu\"),\r\n",
        "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\r\n",
        "    keras.layers.BatchNormalization(),\r\n",
        "    keras.layers.Activation(\"elu\"),\r\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOCSami5aZnH"
      },
      "source": [
        "`BatchNormalization` class hyperparameters:\r\n",
        "- `momentum`: Uses when updating the exponential moving averages. A good momentum value is close to 1.\r\n",
        "- `axis`: Determines which axis should be normalized. Defaults to -1, normalizing the last axis (using the means and standard deviations computed across the other axes).\r\n",
        "\r\n",
        "> Note: BN uses batch statistics during training and the \"final\" statistics after training (ie. the final values of the moving averages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sAV0hhGoH7y"
      },
      "source": [
        "### 11.1.4 Gradient Clipping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97IlZ7u8b1Go"
      },
      "source": [
        "**Gradient Clipping** - Clip the gradients during backpropagation so that they never exceed some threshold in order to mitigate the exploding gradients problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qzOuvkJcD1k"
      },
      "source": [
        "# Set clipvalue or clipnorm for Gradient Clipping\r\n",
        "optimizer = keras.optimizers.SGD(clipvalue=1.0)\r\n",
        "model.compile(loss=\"mse\", optimizer=optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b75BxirRcZUt"
      },
      "source": [
        "This optimizer will clip every component of the gradient vector to a value between -1.0 and 1.0.\r\n",
        "\r\n",
        "If you want to ensure that Gradient Clipping does not change the direction of the gradient vector, you should clip by norm by setting `clipnorm` instead of `clipvalue`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUqMDM1joH7y"
      },
      "source": [
        "## 11.2 Reusing Pretrained Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX0YPD1Xr8Bt"
      },
      "source": [
        "**Transfer learning** - Finding an existing neural network that accomplishes a similar task to the one you are trying to tackle and reusing the lower layers of this network.\r\n",
        "\r\n",
        "> Note: Transfer learning will work best when the inputs have similar low-level features.\r\n",
        "\r\n",
        "> Note: The more similar the tasks are, the more layers you want to reuse (starting with the lower layers). For very similar tasks, try keeping all the hidden layers and just replacing the output layer.\r\n",
        "\r\n",
        "Freeze all the reused layers first (ie. make their weights non-trainable so that Gradient Descent won't modify them). Then slowly unfreeze the top layers and see if performance improves.\r\n",
        "\r\n",
        "It is also useful to reduce the learning rate when you unfreeze reused layers to avoid wrecking their fine-tuned weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbSwOD0toH7z"
      },
      "source": [
        "### 11.2.1 Transfer Learning with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foHWN8azu3Ah"
      },
      "source": [
        "Suppose the Fashion MNIST only contained eight classes - all except sandal and shirt. There's a prebuilt Keras model (model A) with >90% accuracy.\r\n",
        "\r\n",
        "We want to train a binary classifier (positive=shirt, negative=sandal). You want to build a model (model B) that uses transfer learning from model A.\r\n",
        "\r\n",
        "First, we need to split the Fashion MNIST into two sets of train, valid, and test sets:\r\n",
        "- `X_train_A`: \"Model A\" with all images except for sandals and shirts\r\n",
        "- `X_train_B`: \"Model B\" with just the first 200 images of sandals or shirts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFUjWbvrxhsb",
        "outputId": "d36c22a9-74d2-4abf-fe14-ac0d719eceb7"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\r\n",
        "# Import Fashion MNIST dataset and split into train, valid, test sets\r\n",
        "\r\n",
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\r\n",
        "X_train_full = X_train_full / 255.0\r\n",
        "X_test = X_test / 255.0\r\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\r\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJR_NN1puw24"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\r\n",
        "# Split into \"Model A\" dataset: all images except sandals or shirts\r\n",
        "# Split into \"Model B\" dataset: 200 images of sandals or shirts\r\n",
        "\r\n",
        "def split_dataset(X, y):\r\n",
        "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\r\n",
        "    y_A = y[~y_5_or_6]\r\n",
        "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\r\n",
        "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\r\n",
        "    return ((X[~y_5_or_6], y_A),\r\n",
        "            (X[y_5_or_6], y_B))\r\n",
        "\r\n",
        "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\r\n",
        "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\r\n",
        "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\r\n",
        "X_train_B = X_train_B[:200]\r\n",
        "y_train_B = y_train_B[:200]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOErZ0Zow1jf",
        "outputId": "56e26dd8-e04e-47d2-d381-8edde09f2d11"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\r\n",
        "# Build Model A neural network\r\n",
        "\r\n",
        "model_A = keras.models.Sequential()\r\n",
        "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\r\n",
        "for n_hidden in (300, 100, 50, 50, 50):\r\n",
        "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\r\n",
        "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))\r\n",
        "\r\n",
        "model_A.compile(loss=\"sparse_categorical_crossentropy\",\r\n",
        "                optimizer=keras.optimizers.SGD(lr=1e-3),\r\n",
        "                metrics=[\"accuracy\"])\r\n",
        "\r\n",
        "history = model_A.fit(X_train_A, y_train_A, epochs=20,\r\n",
        "                    validation_data=(X_valid_A, y_valid_A))\r\n",
        "\r\n",
        "model_A.save(\"my_model_A.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1375/1375 [==============================] - 6s 4ms/step - loss: 0.8101 - accuracy: 0.7349 - val_loss: 0.3676 - val_accuracy: 0.8787\n",
            "Epoch 2/20\n",
            "1375/1375 [==============================] - 6s 4ms/step - loss: 0.3510 - accuracy: 0.8827 - val_loss: 0.3223 - val_accuracy: 0.8924\n",
            "Epoch 3/20\n",
            "1375/1375 [==============================] - 6s 4ms/step - loss: 0.3200 - accuracy: 0.8941 - val_loss: 0.2986 - val_accuracy: 0.9013\n",
            "Epoch 4/20\n",
            "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2963 - accuracy: 0.9013 - val_loss: 0.2830 - val_accuracy: 0.9051\n",
            "Epoch 5/20\n",
            "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2764 - accuracy: 0.9062 - val_loss: 0.2821 - val_accuracy: 0.9071\n",
            "Epoch 6/20\n",
            "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2717 - accuracy: 0.9081 - val_loss: 0.2677 - val_accuracy: 0.9143\n",
            "Epoch 7/20\n",
            "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2620 - accuracy: 0.9127 - val_loss: 0.2608 - val_accuracy: 0.9158\n",
            "Epoch 8/20\n",
            "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2517 - accuracy: 0.9159 - val_loss: 0.2559 - val_accuracy: 0.9190\n",
            "Epoch 9/20\n",
            "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2511 - accuracy: 0.9158 - val_loss: 0.2560 - val_accuracy: 0.9195\n",
            "Epoch 10/20\n",
            "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2470 - accuracy: 0.9166 - val_loss: 0.2499 - val_accuracy: 0.9203\n",
            "Epoch 11/20\n",
            "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2476 - accuracy: 0.9161 - val_loss: 0.2502 - val_accuracy: 0.9195\n",
            "Epoch 12/20\n",
            "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2434 - accuracy: 0.9167 - val_loss: 0.2445 - val_accuracy: 0.9213\n",
            "Epoch 13/20\n",
            "1375/1375 [==============================] - 5s 3ms/step - loss: 0.2355 - accuracy: 0.9204 - val_loss: 0.2446 - val_accuracy: 0.9215\n",
            "Epoch 14/20\n",
            "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2301 - accuracy: 0.9227 - val_loss: 0.2433 - val_accuracy: 0.9185\n",
            "Epoch 15/20\n",
            "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2298 - accuracy: 0.9213 - val_loss: 0.2413 - val_accuracy: 0.9215\n",
            "Epoch 16/20\n",
            "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2217 - accuracy: 0.9265 - val_loss: 0.2379 - val_accuracy: 0.9198\n",
            "Epoch 17/20\n",
            "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2199 - accuracy: 0.9243 - val_loss: 0.2339 - val_accuracy: 0.9220\n",
            "Epoch 18/20\n",
            "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2185 - accuracy: 0.9249 - val_loss: 0.2404 - val_accuracy: 0.9188\n",
            "Epoch 19/20\n",
            "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2191 - accuracy: 0.9263 - val_loss: 0.2340 - val_accuracy: 0.9215\n",
            "Epoch 20/20\n",
            "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2205 - accuracy: 0.9245 - val_loss: 0.2334 - val_accuracy: 0.9230\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrJiwQc1zixu"
      },
      "source": [
        "model_A = keras.models.load_model(\"my_model_A.h5\")\r\n",
        "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])     # Use all layers except output layer\r\n",
        "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))   # Model B output layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4zm1H5q1G48"
      },
      "source": [
        "> Note: `model_A` and `model_B_on_A` now share some layers. When you train `model_B_on_A`, `model_A` is also affected. **Clone** `model_A` before you reuse its layers with `clone_model()` and copy its weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5i18B_11lWn"
      },
      "source": [
        "# Clone Model A and copy its weights\r\n",
        "\r\n",
        "model_A_clone = keras.models.clone_model(model_A)\r\n",
        "model_A_clone.set_weights(model_A.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwdefO1I2K8F"
      },
      "source": [
        "Since the new output layer is initialized randomly, it will make large errors during the 1st few epochs that will produce large error gradients and wreck the reused weights. To avoid this, freeze the reused layers, giving the new layer some time to learn reasonable weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuZ5JpGe1-2d"
      },
      "source": [
        "# Freeze the reused layers\r\n",
        "for layer in model_B_on_A.layers[:-1]:\r\n",
        "    layer.trainable = False\r\n",
        "\r\n",
        "# Must compile the model after freezing or unfreezing layers\r\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\",\r\n",
        "                     metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI8nSX3Y26gs",
        "outputId": "f59639d0-ec7f-4da4-a0e7-5904faa275e3"
      },
      "source": [
        "# Train for a few epochs\r\n",
        "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\r\n",
        "                           validation_data=(X_valid_B, y_valid_B))\r\n",
        "# Unfreeze the reused layers\r\n",
        "for layer in model_B_on_A.layers[:-1]:\r\n",
        "    layer.trainable = True\r\n",
        "# Decrease learning rate\r\n",
        "optimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-2\r\n",
        "# Compile model again\r\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\r\n",
        "                     metrics=[\"accuracy\"])\r\n",
        "# Train model\r\n",
        "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\r\n",
        "                           validation_data=(X_valid_B, y_valid_B))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "7/7 [==============================] - 0s 27ms/step - loss: 0.0870 - accuracy: 0.9800 - val_loss: 0.0860 - val_accuracy: 0.9878\n",
            "Epoch 2/4\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.0750 - accuracy: 0.9900 - val_loss: 0.0766 - val_accuracy: 0.9888\n",
            "Epoch 3/4\n",
            "7/7 [==============================] - 0s 20ms/step - loss: 0.0658 - accuracy: 1.0000 - val_loss: 0.0696 - val_accuracy: 0.9899\n",
            "Epoch 4/4\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.0589 - accuracy: 1.0000 - val_loss: 0.0647 - val_accuracy: 0.9899\n",
            "Epoch 1/16\n",
            "7/7 [==============================] - 1s 45ms/step - loss: 0.0552 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 0.9899\n",
            "Epoch 2/16\n",
            "7/7 [==============================] - 0s 19ms/step - loss: 0.0543 - accuracy: 1.0000 - val_loss: 0.0637 - val_accuracy: 0.9909\n",
            "Epoch 3/16\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.0530 - accuracy: 1.0000 - val_loss: 0.0632 - val_accuracy: 0.9909\n",
            "Epoch 4/16\n",
            "7/7 [==============================] - 0s 19ms/step - loss: 0.0530 - accuracy: 1.0000 - val_loss: 0.0627 - val_accuracy: 0.9909\n",
            "Epoch 5/16\n",
            "7/7 [==============================] - 0s 19ms/step - loss: 0.0493 - accuracy: 1.0000 - val_loss: 0.0622 - val_accuracy: 0.9909\n",
            "Epoch 6/16\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.0521 - accuracy: 1.0000 - val_loss: 0.0618 - val_accuracy: 0.9919\n",
            "Epoch 7/16\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.0559 - accuracy: 1.0000 - val_loss: 0.0614 - val_accuracy: 0.9919\n",
            "Epoch 8/16\n",
            "7/7 [==============================] - 0s 20ms/step - loss: 0.0511 - accuracy: 1.0000 - val_loss: 0.0610 - val_accuracy: 0.9929\n",
            "Epoch 9/16\n",
            "7/7 [==============================] - 0s 19ms/step - loss: 0.0414 - accuracy: 1.0000 - val_loss: 0.0605 - val_accuracy: 0.9929\n",
            "Epoch 10/16\n",
            "7/7 [==============================] - 0s 20ms/step - loss: 0.0446 - accuracy: 1.0000 - val_loss: 0.0601 - val_accuracy: 0.9929\n",
            "Epoch 11/16\n",
            "7/7 [==============================] - 0s 19ms/step - loss: 0.0460 - accuracy: 1.0000 - val_loss: 0.0597 - val_accuracy: 0.9929\n",
            "Epoch 12/16\n",
            "7/7 [==============================] - 0s 18ms/step - loss: 0.0555 - accuracy: 1.0000 - val_loss: 0.0594 - val_accuracy: 0.9929\n",
            "Epoch 13/16\n",
            "7/7 [==============================] - 0s 19ms/step - loss: 0.0516 - accuracy: 1.0000 - val_loss: 0.0590 - val_accuracy: 0.9929\n",
            "Epoch 14/16\n",
            "7/7 [==============================] - 0s 19ms/step - loss: 0.0495 - accuracy: 1.0000 - val_loss: 0.0586 - val_accuracy: 0.9929\n",
            "Epoch 15/16\n",
            "7/7 [==============================] - 0s 20ms/step - loss: 0.0458 - accuracy: 1.0000 - val_loss: 0.0583 - val_accuracy: 0.9929\n",
            "Epoch 16/16\n",
            "7/7 [==============================] - 0s 20ms/step - loss: 0.0463 - accuracy: 1.0000 - val_loss: 0.0580 - val_accuracy: 0.9929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgb1KI3C3_1A",
        "outputId": "db30aaf5-5392-47dc-b5bd-88f6d0d62ca7"
      },
      "source": [
        "model_B_on_A.evaluate(X_test_B, y_test_B)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.056174881756305695, 0.987500011920929]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7liYM7QR4bgp"
      },
      "source": [
        "If you try to change the classes or the random seed, you will see that the improvement generally drops or even vanishes or reverses. What happened is \"torturing the data until it confesses\" (ie. went through many iterations and picked the best result, when in general may not be the case).\r\n",
        "\r\n",
        "It turns out that transfer learning does not work very well with small dense networks, presumably because small networks learn few patterns, and dense networks learn very specific patterns.\r\n",
        "\r\n",
        "Transfer learning works best with deep convolutional neural networks, which tend to learn feature detectors that are much more general (especially in the lower layers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHu3akFJoH7z"
      },
      "source": [
        "### 11.2.2 Unsupervised Pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akBHC_Da5uCH"
      },
      "source": [
        "Suppose you want to tackle a complex task for which you don't have much labeled training data. If you can gather plenty of unlabeled training data, you can try to use it to train an unsupervised model, such as an autoencoder or a generative adversarial network (GAN). Then you can reuse the lower layers, add the output layer for your task on top, and fine-tune the final network using supervised learning.\r\n",
        "\r\n",
        "**Greedy layer-wise pretraining** - Used in early days of Deep Learning.\r\n",
        "1. First train an unsupervised model with a single layer (typically restricted Boltzmann machines, RBMs).\r\n",
        "2. Then freeze that layer and add another one on top of it.\r\n",
        "3. Train the model again (effectively just training the new layer).\r\n",
        "4. Repeat layer by layer.\r\n",
        "\r\n",
        "Today, people generally train the full unsupervised model in one shot and use autoencoders or GANs rather than RBMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs3N-SonoH70"
      },
      "source": [
        "### 11.2.3 Pretraining on an Auxiliary Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZqFFmHC8CsJ"
      },
      "source": [
        "One last option is to train a 1st neural network on an auxiliary task for which you can easily obtain or generate labeled training data, then reuse the lower layers of that network for your actual task. The 1st neural network's lower layers will learn feature detectors that will likely be reusable by the 2nd neural network.\r\n",
        "\r\n",
        "For example, you want to build a system to recognize faces but have only a few pictures of each individual.\r\n",
        "1. You can gather a lot of pictures of random people and train a 1st neural network to detect whether or not two different pictures feature the same person.\r\n",
        "2. Reuse its lower layers, as it would learn good feature detectors for faces, to train a good face classifier that uses little training data.\r\n",
        "\r\n",
        "For **natural language processing (NLP)**, you can download millions of text documents and automatically generate labeled data from it.\r\n",
        "1. Mask out some words and train a model to predict what the missing words are.\r\n",
        "2. If it performs well, it means it already knows a lot about language.\r\n",
        "3. You can reuse it for the actual task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yIBSoZuoH70"
      },
      "source": [
        "## 11.3 Faster Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96ynKD5_oH71"
      },
      "source": [
        "### 11.3.1 Momentum Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MytXtwGqixyt"
      },
      "source": [
        "**Momentum optimization** - Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity.\r\n",
        "\r\n",
        "In contrast, regular Gradient Descent simply takes small, regular steps down the slope.\r\n",
        "\r\n",
        "Momentum optimization cares a great deal about what previous gradients were: at each iteration, it subtracts the local gradient from the **momentum vector** $\\mathbf{m}$ and updates the weights by adding this momentum vector.\r\n",
        "\r\n",
        "Hyperparameter $\\beta$ called the **momentum** simulates as a sort of friction mechanism and prevent the momentum from growing too large (set between 0 \"high friction\" and 1 \"no friction\" - typical value is 0.9).\r\n",
        "\r\n",
        "In deep neural networks that don't use Batch Normalization, the upper layers will often end up having inputs with very different scales, so using momentum optimization helps a lot and helps roll past local optima.\r\n",
        "\r\n",
        "> Note: Due to the momentum, the optimizer may overshoot a bit, then come back, overshoot again, and oscillate many times before stabilizing at a minimum. Having a bit of friction gets rid of these oscillations and speeds up convergence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBcGgc8Qlvpw"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH7ZwtwRoH71"
      },
      "source": [
        "### 11.3.2 Nesterov Accelerated Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz2UNTnfmGx_"
      },
      "source": [
        "**Nesterov Accelerated Gradient (NAG)** method (also called *Nesterov momentum optimization*) - Measures the gradient of the cost function not at the local position $\\mathbf{\\theta}$ but slightly ahead in the direction of the momentum, $\\theta + \\beta \\mathbf{m}$.\r\n",
        "\r\n",
        "This small tweak works because in general the momentum vector will be pointing in the right direction (ie. toward the optimum), so it will be slightly more accurate to use the gradient measured a bit further in that direction rather than the gradient at the original position.\r\n",
        "\r\n",
        "> Note: NAG gradient pushes toward the bottom of the valley (instead of across). So NAG is generally faster than regular momentum optimization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsvqG8N-nuTv"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VkENr_ooH71"
      },
      "source": [
        "### 11.3.3 AdaGrad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vx6zvBEoDW2"
      },
      "source": [
        "> Recall: For an elongated bowl problem, Gradient Descent would go down the steepest slope, which does not point straight toward the global optimum, then it very slowly goes down to the bottom of the valley.\r\n",
        "\r\n",
        "**AdaGrad** algorithm - Corrects its direction to point more toward the global optimum by scaling down the gradient vector along the steepest dimensions. The steps are:\r\n",
        "\r\n",
        "1. Accumulates the square of the gradients into the vector $\\mathbf{s}$. Each $s_i$ accumulates the squares of the partial derivative of the cost function with regard to parameter $\\theta_i$. If the cost function is steep along the $i^{th}$ dimension, then $s_i$ will get larger and larger at each iteration.\r\n",
        "\r\n",
        "2. Almost identical to Gradient Descent, but the gradient vector is scaled down (element-wise division) by a factor of $\\sqrt{\\mathbf{s} + \\epsilon}$.\r\n",
        "\r\n",
        "**Adaptive learning rate** - Decays the learning rate such that steeper dimensions decay faster than dimensions with gentler slopes.\r\n",
        "\r\n",
        "AdaGrad often stops too early when training neural networks. The learning rate gets scaled down too much that the algorithm ends up stopping before reaching the global optimum.\r\n",
        "\r\n",
        "> Note: You **should not** use AdaGrad to train deep neural networks (may be efficient for simpler tasks such as Linear Regression)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRjiUCp3oH72"
      },
      "source": [
        "### 11.3.4 RMSProp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nww7Yb46rVXv"
      },
      "source": [
        "**RMSProp** algorithm - Fixes AdaGrad's problem of slowing down too fast and never converging to global optimum by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It uses exponential decay in the 1st step.\r\n",
        "\r\n",
        "> Note: The decay rate $\\beta$ is typically set to 0.9, and often works well so no need to tune it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ1o9ygar5eI"
      },
      "source": [
        "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9) # rho -> beta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pxHGQiYsFAV"
      },
      "source": [
        "> Note: Except on very simple problems, RMSProp almost always performs much better than AdaGrad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL-hc4zToH72"
      },
      "source": [
        "### 11.3.5 Adam and Nadam Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er4uxUKfs0cF"
      },
      "source": [
        "**Adam (adaptive moment estimation)** - Combines the ideas of momentum optimization and RMSProp: it keeps track of an exponentially decaying average of past gradients (momentum optimization) and squared gradients (RMSProp).\r\n",
        "\r\n",
        "> Note: The mean is often called the *first moment* and the variance is called *second moment*, hence the name of the algorithm.\r\n",
        "\r\n",
        "> Note: Refer to *Equation 11-8. Adam algorithm* in book.\r\n",
        "\r\n",
        "Steps 1, 2, and 5 are very similar to both momentum optimization and RMSProp, except that step 1 computes an eponentially decaying average rather than a decaying sum.\r\n",
        "\r\n",
        "Steps 3 and 4: since $\\mathbf{m}$ and $\\mathbf{s}$ are initialized at 0, they will be biased toward 0 at the beginning of training, so these 2 steps help boost $\\mathbf{m}$ and $\\mathbf{s}$ at the beginning of training.\r\n",
        "\r\n",
        "The momentum decay hyperparameter $\\beta_1$ is typically initalized to 0.9 and scaling decay hyperparameter $\\beta_2$ to 0.999."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab68kwuwvWIs"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCItp6aKvkZw"
      },
      "source": [
        "> Note: You can often use the default learning rate value, $\\eta=0.001$ since adaptive learning rate algorithms {Adam, AdaGrad, RMSProp} require less tuning of the learning rate, making Adam even easier to use than Gradient Descent.\r\n",
        "\r\n",
        "2 variants of Adam:\r\n",
        "- **AdaMax**: Replaces the $\\ell_2$ norm, \"$\\sqrt{\\text{sum of squares}}$\" with $\\ell_\\infty$.\r\n",
        "    - Replaces step 2 with $ \\mathbf{s} \\leftarrow \\text{max}(\\beta_2 \\nabla_\\theta J(\\theta))$\r\n",
        "    - Drops step 4\r\n",
        "    - For step 5, scales down the gradient updates by a factor of $\\mathbf{s}$, the max of the time-decayed gradients\r\n",
        "\r\n",
        "- **Nadam**: Adam optimization + the Nesterov trick, so it will converge slightly faster than Adam.\r\n",
        "\r\n",
        "> Note: Adaptive optimization methods {RMSProp, Adam, and Nadam optimization} are often great, converging fast to a good solution, but generalize poorly on some datasets.\r\n",
        "\r\n",
        "> #### Training Sparse Models\r\n",
        "\r\n",
        ">> All the optimization algorithms presented produce dense models, meaning that most parameters will be nonzero. If you need a fast model at runtime or need to take up less memory, you may prefer to end up with a sparse model instead.\r\n",
        "\r\n",
        ">>Apply strong $\\ell_1$ regularization during training as it pushes the optimizer to zero out as many weights as it can - much better than just setting tiny weights to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHJGaPqqoH72"
      },
      "source": [
        "### 11.3.6 Learning Rate Scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McxKg1p54fze"
      },
      "source": [
        "If you start with a large learning rate and then reduce it once training stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate.\r\n",
        "\r\n",
        "The most commonly used learning schedules are:\r\n",
        "\r\n",
        "**Power scheduling**: \r\n",
        "- Set the learning rate to a function of the iteration number t: $ \\eta(t) = \\eta_0 / (1 + t/s)^c $.\r\n",
        "- Requires tuning the hyperparameters, $\\eta_0, s, c$, (c typically set to 1).\r\n",
        "- After s steps it is down to $ \\eta_0 /2$ then s more steps will be $\\eta_0/3$ and so on.\r\n",
        "- Schedule first drops quickly, then more and more slowly.\r\n",
        "\r\n",
        "**Exponential scheduling**:\r\n",
        "- Set learning rate to $ \\eta(t) = \\eta_0 0.1^{t/s}$.\r\n",
        "- The learning rate will gradually drop by a factor of 10 every s steps.\r\n",
        "\r\n",
        "**Piecewise constant scheduling**:\r\n",
        "- Use a constant learning rate for a number of epochs (eg. $\\eta_0 = 0.1$ for 5 epochs).\r\n",
        "- Then a smaller learning rate for another number of epochs (eg. $\\eta_1 = 0.001$ for 50 epochs) and so on.\r\n",
        "- Requires fiddling around to figure out right sequence of learning rates and how long to use each of them.\r\n",
        "\r\n",
        "**Performance scheduling**:\r\n",
        "- Measure the validation error every $N$ steps (just like for early stopping).\r\n",
        "- Reduce the learning rate by a factor of $\\lambda$ when the error stops dropping.\r\n",
        "\r\n",
        "**1cycle scheduling**:\r\n",
        "- Increase the initial learning rate $\\eta_0$ linearly up to $\\eta_1$ halfway through training.\r\n",
        "- Then decrease the learning rate linearly down to $\\eta_0$ during the second half of training.\r\n",
        "- Finish the last few epochs by dropping the rate down by several orders of magnitude (still linearly)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDMDW5AX7Qzl"
      },
      "source": [
        "# Power scheduling\r\n",
        "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4) # decay = 1/s\r\n",
        "\r\n",
        "# Exponential scheduling\r\n",
        "def exponential_decay_fn(epoch):\r\n",
        "    # Takes the current epoch and returns learning rate\r\n",
        "    return 0.01 * 0.1**(epoch/ 20) # where n_0=0.01, s=20\r\n",
        "\r\n",
        "def exponential_decay(lr0, s):\r\n",
        "    def exponential_decay_fn(epoch):\r\n",
        "        return lr0 * 0.1**(epoch / s)\r\n",
        "    return exponential_decay_fn # Returns a configured function\r\n",
        "\r\n",
        "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe0E4oNw-8K_"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\r\n",
        "# Scaling Fashion MNIST\r\n",
        "pixel_means = X_train.mean(axis=0, keepdims=True)\r\n",
        "pixel_stds = X_train.std(axis=0, keepdims=True)\r\n",
        "X_train_scaled = (X_train - pixel_means) / pixel_stds\r\n",
        "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\r\n",
        "X_test_scaled = (X_test - pixel_means) / pixel_stds"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIvYgcdu_LjO"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\r\n",
        "# Building the model\r\n",
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\r\n",
        "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\r\n",
        "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\r\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\r\n",
        "])\r\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\r\n",
        "n_epochs = 25"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxWBOJGX_UQc",
        "outputId": "cc6a37fb-20d3-44cf-86bc-cea1c0b04067"
      },
      "source": [
        "# Create LearningRateScheduler callback, giving it the schedule function\r\n",
        "# Pass callback to fit() method\r\n",
        "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\r\n",
        "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\r\n",
        "                    validation_data=(X_valid_scaled, y_valid),\r\n",
        "                    callbacks=[lr_scheduler])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "1719/1719 [==============================] - 14s 7ms/step - loss: 1.1163 - accuracy: 0.7336 - val_loss: 0.8910 - val_accuracy: 0.7512\n",
            "Epoch 2/25\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.6789 - accuracy: 0.7954 - val_loss: 0.6525 - val_accuracy: 0.8354\n",
            "Epoch 3/25\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.6006 - accuracy: 0.8186 - val_loss: 0.5345 - val_accuracy: 0.8430\n",
            "Epoch 4/25\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.5201 - accuracy: 0.8375 - val_loss: 0.5534 - val_accuracy: 0.8344\n",
            "Epoch 5/25\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.4918 - accuracy: 0.8436 - val_loss: 0.4745 - val_accuracy: 0.8444\n",
            "Epoch 6/25\n",
            "1719/1719 [==============================] - 13s 8ms/step - loss: 0.4417 - accuracy: 0.8599 - val_loss: 0.4539 - val_accuracy: 0.8680\n",
            "Epoch 7/25\n",
            "1719/1719 [==============================] - 13s 7ms/step - loss: 0.4015 - accuracy: 0.8675 - val_loss: 0.5299 - val_accuracy: 0.8570\n",
            "Epoch 8/25\n",
            "1719/1719 [==============================] - 13s 7ms/step - loss: 0.3754 - accuracy: 0.8779 - val_loss: 0.4798 - val_accuracy: 0.8624\n",
            "Epoch 9/25\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.3584 - accuracy: 0.8833 - val_loss: 0.4357 - val_accuracy: 0.8680\n",
            "Epoch 10/25\n",
            "1719/1719 [==============================] - 13s 7ms/step - loss: 0.3168 - accuracy: 0.8938 - val_loss: 0.4313 - val_accuracy: 0.8798\n",
            "Epoch 11/25\n",
            "1719/1719 [==============================] - 13s 7ms/step - loss: 0.2998 - accuracy: 0.8998 - val_loss: 0.4432 - val_accuracy: 0.8702\n",
            "Epoch 12/25\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2786 - accuracy: 0.9059 - val_loss: 0.5346 - val_accuracy: 0.8802\n",
            "Epoch 13/25\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2546 - accuracy: 0.9162 - val_loss: 0.4486 - val_accuracy: 0.8842\n",
            "Epoch 14/25\n",
            "1719/1719 [==============================] - 11s 7ms/step - loss: 0.2368 - accuracy: 0.9205 - val_loss: 0.4659 - val_accuracy: 0.8828\n",
            "Epoch 15/25\n",
            "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2202 - accuracy: 0.9251 - val_loss: 0.4633 - val_accuracy: 0.8854\n",
            "Epoch 16/25\n",
            "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1994 - accuracy: 0.9331 - val_loss: 0.4497 - val_accuracy: 0.8856\n",
            "Epoch 17/25\n",
            "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1895 - accuracy: 0.9347 - val_loss: 0.4811 - val_accuracy: 0.8878\n",
            "Epoch 18/25\n",
            "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1759 - accuracy: 0.9399 - val_loss: 0.4588 - val_accuracy: 0.8934\n",
            "Epoch 19/25\n",
            "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1645 - accuracy: 0.9449 - val_loss: 0.4947 - val_accuracy: 0.8882\n",
            "Epoch 20/25\n",
            "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1519 - accuracy: 0.9502 - val_loss: 0.4912 - val_accuracy: 0.8890\n",
            "Epoch 21/25\n",
            "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1386 - accuracy: 0.9527 - val_loss: 0.5038 - val_accuracy: 0.8878\n",
            "Epoch 22/25\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.1301 - accuracy: 0.9572 - val_loss: 0.5689 - val_accuracy: 0.8880\n",
            "Epoch 23/25\n",
            "1719/1719 [==============================] - 13s 7ms/step - loss: 0.1257 - accuracy: 0.9574 - val_loss: 0.5551 - val_accuracy: 0.8906\n",
            "Epoch 24/25\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.1154 - accuracy: 0.9620 - val_loss: 0.5656 - val_accuracy: 0.8914\n",
            "Epoch 25/25\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.1111 - accuracy: 0.9634 - val_loss: 0.6031 - val_accuracy: 0.8898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ_-lFAuAQ4C"
      },
      "source": [
        "The `LearningRateScheduler` will update the optimizer's `learning_rate` attribute at the beginning of each epoch. Updating the learning rate at every step makes sense if there are many steps per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mIfASNLAsAe"
      },
      "source": [
        "def exponential_decay_fn(epoch, lr):\r\n",
        "    return lr * 0.1**(1 / 20)   # Decay now starts at the beginning of epoch 0 instead of 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6YwH5MFBUhM"
      },
      "source": [
        "When you save a model, the optimizer and its learning rate get saved along with it. However, the epoch does not get saved, and it gets reset to 0 every time you call the `fit()` method.\r\n",
        "\r\n",
        "One solution is to manually set the `fit()` method's `initial_epoch` argument so each `epoch` starts at the right value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1GvKpJWBxWj"
      },
      "source": [
        "# Piecewise constant scheduling\r\n",
        "def piecewise_constant_fn(epoch):\r\n",
        "    if epoch < 5:\r\n",
        "        return 0.01\r\n",
        "    elif epoch < 15:\r\n",
        "        return 0.005\r\n",
        "    else:\r\n",
        "        return 0.001"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcn3UprmCQbz"
      },
      "source": [
        "For performance scheduling, use the `ReduceLROnPlateau` callback, that multiplies the learning rate by 0.5 whenever the best validation loss does not improve for 5 consecutive epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKnjaemVCgcs"
      },
      "source": [
        "# Performance scheduling\r\n",
        "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9Bb_VStC3I6"
      },
      "source": [
        "An alternate way to implement learning rate scheduling is: define the learning rate using one of the schedules available in `keras.optimizers.schedules`, then pass this learning rate to any optimizer. This updates the learning rate at each step rather than at each epoch.\r\n",
        "\r\n",
        "So an alternate way to define `exponential_decay_fn()` would be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vymKueHEDQFP"
      },
      "source": [
        "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\r\n",
        "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\r\n",
        "optimizer = keras.optimizers.SGD(learning_rate)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgHEt1n4DhKf"
      },
      "source": [
        "This is nice and simple, and when saving the model, the learning rate and its schedule (including its state) is saved as well.\r\n",
        "\r\n",
        "> Note: This is specific to `tf.keras`. Not part of the Keras API.\r\n",
        "\r\n",
        "For 1cycle scheduling, just create a custom callback (similar to the rest) that modifies the learning rate at each iteration (`self.model.optimizer.lr`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etBuzcsHoH73"
      },
      "source": [
        "## 11.4 Avoiding Overfitting Through Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsQQYq4NoH73"
      },
      "source": [
        "### 11.4.1 $\\ell_1$ and $\\ell_2$ Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6i4g_OJoH73"
      },
      "source": [
        "### 11.4.2 Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFjCtsUjoH74"
      },
      "source": [
        "### 11.4.3 Monte Carlo (MC) Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd_2iKlCoH74"
      },
      "source": [
        "### 11.4.4 Max-Norm Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Gvo0vZoH74"
      },
      "source": [
        "## 11.5 Summary and Practical Guidelines"
      ]
    }
  ]
}