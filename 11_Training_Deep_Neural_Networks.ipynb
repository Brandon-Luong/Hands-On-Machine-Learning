{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.0 64-bit ('venv')",
      "metadata": {
        "interpreter": {
          "hash": "7b58ff03a81b7b220740b362f01ec4719380627cca970f4b363191d14b8c12ac"
        }
      }
    },
    "colab": {
      "name": "11_Training_Deep_Neural_Networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElBOxYD0oH7j"
      },
      "source": [
        "# Chapter 11: Training Deep Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXpkzwGAotfe"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SAWjzOroH7t"
      },
      "source": [
        "Some problems training a deep DNN (deep neural networks):\n",
        "- Vanishing or exploding gradients problem. Gradients grow smaller and smaller / bigger and bigger and makes training lower layers very hard.\n",
        "- Not enough training data, or too costly to label.\n",
        "- Training may be extremely slow.\n",
        "- Model with millions of parameters risk overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Pcawp4oH7v"
      },
      "source": [
        "## 11.1 The Vanishing/Exploding Gradients Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ss4SNvJoH7v"
      },
      "source": [
        "Backpropagation computes and propagates the error gradient of each layer from output to input. \n",
        "\n",
        "**Vanishing gradients problem** - Gradients get smaller and smaller as it progresses to lower layers. So Gradient Descent leaves the lower layers' connection weights virtually unchanged and never converges to a good solution.\n",
        "\n",
        "**Exploding gradients problem** - Similar effect but gradients get bigger and bigger and algorithm diverges.\n",
        "\n",
        "> In general, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.\n",
        "\n",
        "It was discovered that using logistic sigmoid activation function and initializing weights using a normal distribution ($\\mu=0, \\sigma = 1$) caused this issue.\n",
        "\n",
        "- Initializing weights using a normal distribution:\n",
        "    - The variance (the spread) of the outputs of each layer is much greater than its inputs.\n",
        "    \n",
        "    - Going forward in the network, the variance keeps increasing after each layer until the activation function saturates (ends up far right/left) at the top layers.\n",
        "\n",
        "- Logistic sigmoid activation function:\n",
        "    - Because the variance keeps increasing, inputs become large (negative or positive, \"far left/right\"), with outputs of 0 or 1 and derivative extremely close to 0.\n",
        "    - So backpropagation has no error gradient to propagate to the lower layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbGgiiBToH7w"
      },
      "source": [
        "### 11.1.1 Glorot and He Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LJ0ru-NoH7w"
      },
      "source": [
        "For the signal to flow properly, we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction.\n",
        "\n",
        "> Microphone Amplifier Analogy: Setting knob too close to 0, voice is inaudible but too close to max, voice is too saturated. For a chain of amplifiers, they all need to be set properly so that voice is loud and clear at the end of the chain.  \n",
        "\n",
        "> Your voice has to come out of each amplifier at the same amplitude as it came in.\n",
        "\n",
        "It's impossible to guarantee both (output & gradient variances) unless the layer has an equal number of inputs and neurons (*fan-in*, *fan-out*). Glorot and Bengio proposed a good compromise that the connection weights to be initialized randomly according to *Equation 11-1*, where $\\text{fan}_\\text{avg} = (\\text{fan}_\\text{in} + \\text{fan}_\\text{out})/2$ and is called **Xavier initialization** or **Glorot initialization**. This strategy is used for the logistic activation function.\n",
        "\n",
        "**LeCun initialization** - Equivalent to Glorot initialization when $\\text{fan}_\\text{in} = \\text{fan}_\\text{out} $.\n",
        "\n",
        "**He initialization** - The initialization strategy for the ReLU activation function (and its variants)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GJ0qLbZog6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25890a83-ed88-4756-c7d7-05a29528ece6"
      },
      "source": [
        "# Default is Glorot with uniform distribution \r\n",
        "# Change to He initialization\r\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\r\n",
        "\r\n",
        "# He initialization with uniform distribution based on fan_avg rather than fan_in\r\n",
        "# Use VarianceScaling\r\n",
        "he_avg_init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg',\r\n",
        "                                                 distribution='uniform')\r\n",
        "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.core.Dense at 0x7f6a37de7d10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgUq6SZjoH7x"
      },
      "source": [
        "### 11.1.2 Nonsaturating Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6mG0mVtG2ic"
      },
      "source": [
        "**Dying ReLUs** - During training, some neurons effectively \"die,\" meaning they stop outputting anything other than 0.\r\n",
        "\r\n",
        "A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting 0s, and Gradient Descent does not affect it anymore because the gradient of the ReLU function is 0 when its input is negative.\r\n",
        "\r\n",
        "To solve this problem, use **leaky ReLU** where $\\text{LeakyReLU}_\\alpha(z) = \\text{max}(\\alpha z, z) $. The hyperparameter $\\alpha$ defines how much the function \"leaks\": it is the slope of the function for $z<0$ and is typically set to $0.01$.\r\n",
        "\r\n",
        "> Leaky ReLU is just like ReLU, but with a small slope for negative values. A small slope ensures that leaky ReLUs never die.\r\n",
        "\r\n",
        "**Exponential linear unit (ELU)** - Outperforms all the ReLU variants. Looks a lot like the ReLU function with some major differences:\r\n",
        "- Takes on nagative values when $z<0$, allowing an average output closer to 0 and alleviating the vanishing gradients problem.\r\n",
        "- Hyperparameter $\\alpha$ defines the value when z is a large negative number (usually set to 1).\r\n",
        "- Has nonzero gradient for $z<0$, avoiding the dead neurons problem.\r\n",
        "- If $\\alpha = 1$, then the function is smooth everywhere including around $z=0$, speeding up Gradient Descent.\r\n",
        "\r\n",
        "The main drawback of the ELU function is that it is slower to compute than the ReLU function and its variants (due to the exponential function).\r\n",
        "\r\n",
        "**Scaled ELU (SELU)** - A scaled variant of the ELU activation function. The network will *self-normalize* ($\\mu =0, \\sigma =1$) but under specific conditions:\r\n",
        "- Input features must be standarized ($\\mu =0, \\sigma =1$).\r\n",
        "- Every hidden layer's weights must be initialized with LeCun normal initialization.\r\n",
        "- Network's architecture must be sequential.\r\n",
        "\r\n",
        "> In general, SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic.\r\n",
        "\r\n",
        "> Because ReLU is the most used activation function, many libraries and hardware accelerators provide ReLU-specific optimizations; if speed is your priority, ReLU might be the best choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kknDGFptMF_1"
      },
      "source": [
        "# On fashion MNIST as example\r\n",
        "\r\n",
        "# For leaky ReLU\r\n",
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),                  # FROM TEXTBOOK NOTEBOOK \r\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),     # FROM TEXTBOOK NOTEBOOK \r\n",
        "    keras.layers.LeakyReLU(alpha=0.2),                           # Activation function after each layer\r\n",
        "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\r\n",
        "    keras.layers.LeakyReLU(alpha=0.2)\r\n",
        "])\r\n",
        "\r\n",
        "# For PReLU\r\n",
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),                  # FROM TEXTBOOK NOTEBOOK \r\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),     # FROM TEXTBOOK NOTEBOOK \r\n",
        "    keras.layers.PReLU(),                                        # Activation function after each layer\r\n",
        "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"),\r\n",
        "    keras.layers.PReLU()\r\n",
        "])\r\n",
        "\r\n",
        "# For SELU\r\n",
        "layer = keras.layers.Dense(10, activation=\"selu\",\r\n",
        "                           kernel_initializer=\"lecun_normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2URhcKtoH7y"
      },
      "source": [
        "### 11.1.3 Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibk-uwfaQux1"
      },
      "source": [
        "Although different initialization and a variant activation function reduces the vanishing/exploding gradients problem, it doesn't guarantee that they won't come back during training.\r\n",
        "\r\n",
        "**Batch Normalization** - Adding an operation in the model just before or after the activation function of each hidden layer. This operation simply zero-centers and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: 1 for scaling, other for shifting.\r\n",
        "\r\n",
        "> If you add a BN layer as the very first layer, you do not need to standardize your training set (eg. using `StandardScaler`).\r\n",
        "\r\n",
        "Come testing a new instance, how to calculate the batch mean/standard deviation?\r\n",
        "\r\n",
        "Estimate the final statistics (overall to use on new instance, instead of the batch values) by using a moving average of the layer's input means and standard deviation.\r\n",
        "\r\n",
        "4 parameters are learned in each batch-normalized layer:\r\n",
        "- $\\mathbf{\\gamma}$, the output scale vector\r\n",
        "- $\\mathbf{\\beta}$, the output offset vector\r\n",
        "- $\\mathbf{\\mu}$, the final input mean vector\r\n",
        "- $\\mathbf{\\sigma}$, the final input standard deviation vector\r\n",
        "\r\n",
        "> $\\mathbf{\\mu}$ and $\\mathbf{\\sigma}$ are estimated during training, but only used after training (to replace batch input means and standard deviation).\r\n",
        "\r\n",
        "Batch Normalization acts like a regularizer, reducing the need for other regularization techniques.\r\n",
        "\r\n",
        "There is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer. Fortunately, it's often possible to fuse the BN layer with the previous layer, after training, thereby avoiding the runtime penalty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnUrN84fUp8v"
      },
      "source": [
        "#### Implementing Batch Normalization with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-R9PrMlU6Au"
      },
      "source": [
        "Just add a `BatchNormalization` layer before or after each hidden layer's activation function.\r\n",
        "\r\n",
        "For example, this model applies BN after every hidden layer and as the first layer in the model (after flattening the input images)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuJ2LUSIVNSN",
        "outputId": "5f4e5f13-6d26-4432-e655-34c6def1e3fe"
      },
      "source": [
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\r\n",
        "    keras.layers.BatchNormalization(),\r\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\r\n",
        "    keras.layers.BatchNormalization(),\r\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\r\n",
        "    keras.layers.BatchNormalization(),\r\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\r\n",
        "])\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 271,346\n",
            "Trainable params: 268,978\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUTd3QL7WTMr"
      },
      "source": [
        "Bn layer adds 4 parameters per input: $\\gamma, \\beta, \\mu, \\sigma$. So for the first BN layer, $4 \\times 784 = 3,136$ parameters.\r\n",
        "\r\n",
        "$\\mu, \\sigma$ are moving averages and \"non-trainable\" so $(3,136 + 1,200 + 400) / 2 = 2,368$ the total number of non-trainable parameters in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yih8G_oW5y-",
        "outputId": "861bc631-030b-4452-ab7b-ce17b0c5027a"
      },
      "source": [
        "[(var.name, var.trainable) for var in model.layers[1].variables]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('batch_normalization/gamma:0', True),\n",
              " ('batch_normalization/beta:0', True),\n",
              " ('batch_normalization/moving_mean:0', False),\n",
              " ('batch_normalization/moving_variance:0', False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6_1igPGXH6k",
        "outputId": "bdaac4a5-3b77-4888-d330-2ce44214d075"
      },
      "source": [
        "# layers.updates method is deprecated\r\n",
        "model.layers[1].updates"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1402: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`layer.updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN8WtPx6ZALH"
      },
      "source": [
        "# To add BN layers before the activation functions\r\n",
        "# Remove activation function from hidden layers\r\n",
        "# Add them as separate layers after BN layers\r\n",
        "# Remove bias term from previous layer, 'use_bias=False'\r\n",
        "\r\n",
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\r\n",
        "    keras.layers.BatchNormalization(),\r\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\r\n",
        "    keras.layers.BatchNormalization(),\r\n",
        "    keras.layers.Activation(\"elu\"),\r\n",
        "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\r\n",
        "    keras.layers.BatchNormalization(),\r\n",
        "    keras.layers.Activation(\"elu\"),\r\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOCSami5aZnH"
      },
      "source": [
        "`BatchNormalization` class hyperparameters:\r\n",
        "- `momentum`: Uses when updating the exponential moving averages. A good momentum value is close to 1.\r\n",
        "- `axis`: Determines which axis should be normalized. Defaults to -1, normalizing the last axis (using the means and standard deviations computed across the other axes).\r\n",
        "\r\n",
        "> Note: BN uses batch statistics during training and the \"final\" statistics after training (ie. the final values of the moving averages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sAV0hhGoH7y"
      },
      "source": [
        "### 11.1.4 Gradient Clipping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97IlZ7u8b1Go"
      },
      "source": [
        "**Gradient Clipping** - Clip the gradients during backpropagation so that they never exceed some threshold in order to mitigate the exploding gradients problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qzOuvkJcD1k"
      },
      "source": [
        "# Set clipvalue or clipnorm for Gradient Clipping\r\n",
        "optimizer = keras.optimizers.SGD(clipvalue=1.0)\r\n",
        "model.compile(loss=\"mse\", optimizer=optimizer)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b75BxirRcZUt"
      },
      "source": [
        "This optimizer will clip every component of the gradient vector to a value between -1.0 and 1.0.\r\n",
        "\r\n",
        "If you want to ensure that Gradient Clipping does not change the direction of the gradient vector, you should clip by norm by setting `clipnorm` instead of `clipvalue`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUqMDM1joH7y"
      },
      "source": [
        "## 11.2 Reusing Pretrained Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbSwOD0toH7z"
      },
      "source": [
        "### 11.2.1 Transfer Learning with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHu3akFJoH7z"
      },
      "source": [
        "### 11.2.2 Unsupervised Pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs3N-SonoH70"
      },
      "source": [
        "### 11.2.3 Pretraining on an Auxiliary Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yIBSoZuoH70"
      },
      "source": [
        "## 11.3 Faster Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96ynKD5_oH71"
      },
      "source": [
        "### 11.3.1 Momentum Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH7ZwtwRoH71"
      },
      "source": [
        "### 11.3.2 Nesterov Accelerated Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VkENr_ooH71"
      },
      "source": [
        "### 11.3.3 AdaGrad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRjiUCp3oH72"
      },
      "source": [
        "### 11.3.4 RMSProp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL-hc4zToH72"
      },
      "source": [
        "### 11.3.5 Adam and Nadam Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHJGaPqqoH72"
      },
      "source": [
        "### 11.3.6 Learning Rate Scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etBuzcsHoH73"
      },
      "source": [
        "## 11.4 Avoiding Overfitting Through Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsQQYq4NoH73"
      },
      "source": [
        "### 11.4.1 $\\ell_1$ and $\\ell_2$ Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6i4g_OJoH73"
      },
      "source": [
        "### 11.4.2 Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFjCtsUjoH74"
      },
      "source": [
        "### 11.4.3 Monte Carlo (MC) Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd_2iKlCoH74"
      },
      "source": [
        "### 11.4.4 Max-Norm Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Gvo0vZoH74"
      },
      "source": [
        "## 11.5 Summary and Practical Guidelines"
      ]
    }
  ]
}