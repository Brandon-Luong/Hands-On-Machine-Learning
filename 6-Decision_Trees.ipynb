{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "54b71c1b326a101c02c54fb24502b2663857f505f7b72a06051ccceba068a4ff"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 6: Decision Trees "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "## 6.1 Training and Visualizing a Decision Tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The following code trains a `DecisionTreeClassifier` on the iris dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "source": [
    "You can visualize the trained Decision Tree by first using `export_graphviz()` to output a graph definition file called *'iris_tree.dot'*.\n",
    "\n",
    "> Note: Visualizing the graph needs Graphviz package's \"dot\" command. **See book for graph picture.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'digraph Tree {\\nnode [shape=box, style=\"filled, rounded\", color=\"black\", fontname=helvetica] ;\\nedge [fontname=helvetica] ;\\n0 [label=\"petal width (cm) <= 0.8\\\\ngini = 0.667\\\\nsamples = 150\\\\nvalue = [50, 50, 50]\\\\nclass = setosa\", fillcolor=\"#ffffff\"] ;\\n1 [label=\"gini = 0.0\\\\nsamples = 50\\\\nvalue = [50, 0, 0]\\\\nclass = setosa\", fillcolor=\"#e58139\"] ;\\n0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"True\"] ;\\n2 [label=\"petal width (cm) <= 1.75\\\\ngini = 0.5\\\\nsamples = 100\\\\nvalue = [0, 50, 50]\\\\nclass = versicolor\", fillcolor=\"#ffffff\"] ;\\n0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel=\"False\"] ;\\n3 [label=\"gini = 0.168\\\\nsamples = 54\\\\nvalue = [0, 49, 5]\\\\nclass = versicolor\", fillcolor=\"#4de88e\"] ;\\n2 -> 3 ;\\n4 [label=\"gini = 0.043\\\\nsamples = 46\\\\nvalue = [0, 1, 45]\\\\nclass = virginica\", fillcolor=\"#843de6\"] ;\\n2 -> 4 ;\\n}'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=None,\n",
    "    # out_file=\"iris_tree.dot\",\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "source": [
    "## 6.2 Making Predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To classify a new flower,\n",
    "\n",
    "1. Start at the **root node** (depth 0).\n",
    "2. Is petal length smaller than 2.45cm?\n",
    "3. **Yes**, move down to root's left child node (depth 1, left).\n",
    "4. **No**, move down to the root's right child node (depth 1, right).\n",
    "5. Is child node a **leaf node** (does not have any more child nodes)?\n",
    "6. **Yes**, do not ask any more questions and look at the predicted class for that node.\n",
    "7. **No**, ask more questions and repeat.\n",
    "\n",
    "Node attributes:\n",
    "- `samples`: Counts how many training instances it applies to.\n",
    "- `value`: Tells you how many training instances of each class this node applies to.\n",
    "- `gini`: Measures its impurity.\n",
    "\n",
    "> Note: Decision Trees do not require feature scaling or centering at all.\n",
    "\n",
    "> Note: Scikit-Learn uses CART algorithm - only **binary trees** -> nonleaf nodes always have two children \"yes/no answers\".\n",
    "\n",
    "> #### Model Interpretation: White Box Versus Black Box\n",
    ">> - **White box models** are intuitive and easy to interpret (eg. Decision Trees).\n",
    ">>\n",
    ">> - **Black box models** are usually hard to explain in simple terms why the predictions were made (eg. Random Forests, neural networks)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.3 Estimating Class Probabilities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A Decision Tree estimates the probability that an instance belongs to a particular class $k$ by:\n",
    "\n",
    "1. Traverses the tree to find the leaf node for this instance\n",
    "2. Returns the ratio of training instances of class $k$ in this node\n",
    "\n",
    "For examples, a flower with (length=5cm, width=1.5cm) corresponds to depth-2 left leaf node. It should output the following probabilities:\n",
    "\n",
    "- $0\\%$ $(0/54)$ for *Iris setosa*\n",
    "- $90.7\\%$ $(49/54)$ for *Iris versicolor*\n",
    "- $9.3\\%$ $(5/54)$ for *Iris virginica*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "source": [
    "## 6.4 The CART Training Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Scikit-Learn uses the **Classification and Regression Tree (CART)** to train Decision Trees (also called \"growing trees\").\n",
    "\n",
    "It first splits the training set into two subsets using a single feature $k$ and a threshold $t_k$ (eg. \"petal length <= 2.45 cm\").\n",
    "\n",
    "It chooses a pair $(k, t_k)$ such that it produces the purest subsets (minimizes the CART cost function).\n",
    "\n",
    "And once it splits, it splits the subsets using the same logic, stopping once it reaches the maximum depth (`max_depth` hyperparameter) or cannot find a split that will reduce impurity.\n",
    "\n",
    "These hyperparameters are additional stopping conditions:\n",
    "\n",
    "- `min_samples_split`\n",
    "- `min_samples_leaf`\n",
    "- `min_weight_fraction_leaf`\n",
    "- `max_leaf_nodes`\n",
    "\n",
    "> Note: CART algorithm is a **greedy algorithm**. It greedily searches for an optimum split at the top level, then repeats at each subsequent level. It does not check whether or not the split will lead to the lowest possible impurity several levels down.\n",
    "\n",
    "> Note: Greedy algorithms produce a solution that's reasonably good but not guaranteed to be optimal."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.5 Computational Complexity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Since each node only requires checking the value of one feature, the overall prediction complexity is $O(log_2(m))$, independent of the number of features.  \n",
    "=> Predictions are fast, even when dealing with large training sets.\n",
    "\n",
    "Comparing all features on all samples at each node results in a training complexity of $ O(n \\times m log_2 (m)) $."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.6 Gini Impurity or Entropy?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Gini impurity measure is used by default, but **entropy impurity measure** can be used by setting `criterion=\"entropy\"`.\n",
    "\n",
    "In Machine Learning, entropy is frequently used as an impurity measure: a set's entropy is zero when it contains instances of only one class.\n",
    "\n",
    "Most of the time it does not make a difference: they lead to similar trees. Gini impurity is slightly faster (good as default).\n",
    "\n",
    "But Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.7 Regularization Hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**Nonparametric model** (eg. Decision Trees) - The number of parameters is not determined prior to training, so the model structure is free to stick closely to the data (most likely overfitting).  \n",
    "\n",
    "**Parametric model** (eg. linear model) - Has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing risk of underfitting). \n",
    "\n",
    "To avoid overfitting the training data, you need to restrict the Decision Tree's freedom, ie. regularization.\n",
    "\n",
    "Reduce `max_depth` (`default=None`) to regularize the model and reduce risk of overfitting.\n",
    "\n",
    "`DecisionTreeClassifier` has other parameters that similarly restricts:\n",
    "\n",
    "- `min_samples_split`: The minimum number of samples a node must have before it can split\n",
    "- `min_samples_leaf`: The minimum number of samples a leaf node must have\n",
    "- `min_weight_fraction_leaf`: Same as `min_samples_leaf` but expressed as a fraction of the total number of weighted instances\n",
    "- `max_leaf_nodes`: The maximum number of leaf nodes\n",
    "- `max_features`: The maximum number of features that are evaluated for splitting at each node.\n",
    "\n",
    "**Summary to regularize model**:  \n",
    "- Increase `min_*` hyperparameters\n",
    "- Reduce `max_*` hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.8 Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM ACCOMPANYING JUPYTER NOTEBOOK\n",
    "\n",
    "# Quadratic training set + noise\n",
    "np.random.seed(42)\n",
    "m = 200\n",
    "X = np.random.rand(m, 1)\n",
    "y = 4 * (X - 0.5) ** 2\n",
    "y = y + np.random.randn(m, 1) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "source": [
    "> Note: **See book for Decision Tree graph picture**.\n",
    "\n",
    "The tree is similar to classification tree, except that it predicts a value instead of class.\n",
    "\n",
    "Suppose you wanted to make a prediction for new instance $x_1=0.6$.  \n",
    "Following the tree, you reach leaf node that predicts `value=0.111`.  \n",
    "The prediction is the average target value of the 100 training instances of that associated leaf.  \n",
    "And results in a mean squared error `mse=0.015` over these 100 instances.\n",
    "\n",
    "> Note: **See book for Decision Tree predictions plot**\n",
    "\n",
    "Notice how the predicted value for each region is always the average target value of the instances in that region.\n",
    "\n",
    "The CART algorithm now tries to split the training set in a way that minimizes the MSE (instead of impurity).\n",
    "\n",
    "Same with classification, Decision Trees are prone to overfitting when dealing with regression tasks and must use regularization."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.9 Instability"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation.\n",
    "\n",
    "> Note: **See book for plot**\n",
    "\n",
    "On left graph, the decision boundary is easily split down the middle.  \n",
    "On right graph (after $45^\\circ$ rotation), the decision boundary is unnecessarily convoluted - a \"staircase\". It would not generalize well.  \n",
    "Use Principal Component Analysis (PCA) to get better orientation of training data.\n",
    "\n",
    "More generally, the main issue with Decision Trees is that they are very sensitive to small variations in the training data.\n",
    "\n",
    "For example, remove the widest Iris versicolor (petals length=4.8cm, width=1.8cm) and the decision boundaries are completely different.\n",
    "\n",
    "> Note: Scikit-Learn's Decision Trees are stochastic, so you may get very different models even on the same training data (unless you set the `random_state` hyperparameter).\n",
    "\n",
    "> Note: Random Forests limit this instability by averaging predictions over many trees."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}