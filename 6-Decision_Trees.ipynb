{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "54b71c1b326a101c02c54fb24502b2663857f505f7b72a06051ccceba068a4ff"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 6: Decision Trees "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "## 6.1 Training and Visualizing a Decision Tree"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The following code trains a `DecisionTreeClassifier` on the iris dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "source": [
    "You can visualize the trained Decision Tree by first using `export_graphviz()` to output a graph definition file called *'iris_tree.dot'*.\n",
    "\n",
    "> Note: Visualizing the graph needs Graphviz package's \"dot\" command. **See book for graph picture.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'digraph Tree {\\nnode [shape=box, style=\"filled, rounded\", color=\"black\", fontname=helvetica] ;\\nedge [fontname=helvetica] ;\\n0 [label=\"petal width (cm) <= 0.8\\\\ngini = 0.667\\\\nsamples = 150\\\\nvalue = [50, 50, 50]\\\\nclass = setosa\", fillcolor=\"#ffffff\"] ;\\n1 [label=\"gini = 0.0\\\\nsamples = 50\\\\nvalue = [50, 0, 0]\\\\nclass = setosa\", fillcolor=\"#e58139\"] ;\\n0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"True\"] ;\\n2 [label=\"petal width (cm) <= 1.75\\\\ngini = 0.5\\\\nsamples = 100\\\\nvalue = [0, 50, 50]\\\\nclass = versicolor\", fillcolor=\"#ffffff\"] ;\\n0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel=\"False\"] ;\\n3 [label=\"gini = 0.168\\\\nsamples = 54\\\\nvalue = [0, 49, 5]\\\\nclass = versicolor\", fillcolor=\"#4de88e\"] ;\\n2 -> 3 ;\\n4 [label=\"gini = 0.043\\\\nsamples = 46\\\\nvalue = [0, 1, 45]\\\\nclass = virginica\", fillcolor=\"#843de6\"] ;\\n2 -> 4 ;\\n}'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=None,\n",
    "    # out_file=\"iris_tree.dot\",\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "source": [
    "## 6.2 Making Predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To classify a new flower,\n",
    "\n",
    "1. Start at the **root node** (depth 0).\n",
    "2. Is petal length smaller than 2.45cm?\n",
    "3. **Yes**, move down to root's left child node (depth 1, left).\n",
    "4. **No**, move down to the root's right child node (depth 1, right).\n",
    "5. Is child node a **leaf node** (does not have any more child nodes)?\n",
    "6. **Yes**, do not ask any more questions and look at the predicted class for that node.\n",
    "7. **No**, ask more questions and repeat.\n",
    "\n",
    "Node attributes:\n",
    "- `samples`: Counts how many training instances it applies to.\n",
    "- `value`: Tells you how many training instances of each class this node applies to.\n",
    "- `gini`: Measures its impurity.\n",
    "\n",
    "> Note: Decision Trees do not require feature scaling or centering at all.\n",
    "\n",
    "> Note: Scikit-Learn uses CART algorithm - only **binary trees** -> nonleaf nodes always have two children \"yes/no answers\".\n",
    "\n",
    "> #### Model Interpretation: White Box Versus Black Box\n",
    ">> - **White box models** are intuitive and easy to interpret (eg. Decision Trees).\n",
    ">>\n",
    ">> - **Black box models** are usually hard to explain in simple terms why the predictions were made (eg. Random Forests, neural networks)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.3 Estimating Class Probabilities"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "A Decision Tree estimates the probability that an instance belongs to a particular class $k$ by:\n",
    "\n",
    "1. Traverses the tree to find the leaf node for this instance\n",
    "2. Returns the ratio of training instances of class $k$ in this node\n",
    "\n",
    "For examples, a flower with (length=5cm, width=1.5cm) corresponds to depth-2 left leaf node. It should output the following probabilities:\n",
    "\n",
    "- $0\\%$ $(0/54)$ for *Iris setosa*\n",
    "- $90.7\\%$ $(49/54)$ for *Iris versicolor*\n",
    "- $9.3\\%$ $(5/54)$ for *Iris virginica*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "source": [
    "## 6.4 The CART Training Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Scikit-Learn uses the **Classification and Regression Tree (CART)** to train Decision Trees (also called \"growing trees\").\n",
    "\n",
    "It first splits the training set into two subsets using a single feature $k$ and a threshold $t_k$ (eg. \"petal length <= 2.45 cm\").\n",
    "\n",
    "It chooses a pair $(k, t_k)$ such that it produces the purest subsets (minimizes the CART cost function).\n",
    "\n",
    "And once it splits, it splits the subsets using the same logic, stopping once it reaches the maximum depth (`max_depth` hyperparameter) or cannot find a split that will reduce impurity.\n",
    "\n",
    "These hyperparameters are additional stopping conditions:\n",
    "\n",
    "- `min_samples_split`\n",
    "- `min_samples_leaf`\n",
    "- `min_weight_fraction_leaf`\n",
    "- `max_leaf_nodes`\n",
    "\n",
    "> Note: CART algorithm is a **greedy algorithm**. It greedily searches for an optimum split at the top level, then repeats at each subsequent level. It does not check whether or not the split will lead to the lowest possible impurity several levels down.\n",
    "\n",
    "> Note: Greedy algorithms produce a solution that's reasonably good but not guaranteed to be optimal."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.5 Computational Complexity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Since each node only requires checking the value of one feature, the overall prediction complexity is $O(log_2(m))$, independent of the number of features.  \n",
    "=> Predictions are fast, even when dealing with large training sets.\n",
    "\n",
    "Comparing all features on all samples at each node results in a training complexity of $ O(n \\times m log_2 (m)) $."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.6 Gini Impurity or Entropy?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Gini impurity measure is used by default, but **entropy impurity measure** can be used by setting `criterion=\"entropy\"`.\n",
    "\n",
    "In Machine Learning, entropy is frequently used as an impurity measure: a set's entropy is zero when it contains instances of only one class.\n",
    "\n",
    "Most of the time it does not make a difference: they lead to similar trees. Gini impurity is slightly faster (good as default).\n",
    "\n",
    "But Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.7 Regularization Hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.8 Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.9 Instability"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}