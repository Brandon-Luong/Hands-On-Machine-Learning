{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10-Introduction_Artificial_Neural_Networks_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit ('venv')",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoOmmGY_fO2x"
      },
      "source": [
        "# Chapter 10: Introduction to Artificial Neural Networks with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3FGSNiJfO27"
      },
      "source": [
        "**Artificial neural networks (ANNs)**: A Machine Learning model inspired by the networks of biological neurons found in our brains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtiML3jRfO28"
      },
      "source": [
        "## 10.1 From Biological to Artificial Neurons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQTYdLcVfO29"
      },
      "source": [
        "### 10.1.1 Biological Neurons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeODW2B3fO29"
      },
      "source": [
        "Cell body -> Axon -> Telondendria -> Synaptic terminals (synapses) => Next cell's dendrites/body\n",
        "\n",
        "> Note: See Figure 10-1 in book\n",
        "\n",
        "Biological neurons produce short electrical impulses called action potentials (APs, or just signals) which travel along the axons and make the synapses release chemical signals called neurotransmitters.\n",
        "\n",
        "When a neuron receives a sufficient amount of these neurotransmitters, it fires its own electrical impulses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQFQgWGVfO2-"
      },
      "source": [
        "### 10.1.2 Logical Computations with Neurons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWYBGvsufO2_"
      },
      "source": [
        "Artificial neuron - It has one or more binary (on/off) inputs and one binary output. The artificial neuron activates its output when more than a certain number of its inputs are active.\n",
        "\n",
        "> Note: See Figure 10-3 in book.\n",
        "\n",
        "> Note: 2 input signals are needed to activate neuron C.\n",
        "\n",
        "1. Identity function: if neuron A is activated, neuron C gets activated as well. But if neuron A is off, then neuron C is off as well.\n",
        "\n",
        "2. Logical AND: neuron C is activated only when both neurons A and B are activated (a single input signal is not enough to activate neuron C).\n",
        "\n",
        "3. Logical OR: neuron C gets activated if either neuron A or B is activated (or both).\n",
        "\n",
        "4. Logicial NOT: neuron C is activated only if neuron A is active and neuron B is off."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A59tpup4fO3A"
      },
      "source": [
        "### 10.1.3 The Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFYW5iPRfO3A"
      },
      "source": [
        "**Perceptron** - Simplest ANN architecture based on a *threshold logic unit (TLU)* or sometimes a *linear threshold unit (LTU)*. The inputs and output are numbers (instead of binary on/off values), and each input connection is associated with a weight. \n",
        "\n",
        "The TLU computes a weighted sum of its inputs $(z=w_1x_1 + w_2x_2 + ... + w_nx_n = \\mathbf{x}^T \\mathbf{w})$, then applies a **step function** to that sum and outputs the result: $h_w(\\mathbf{x}) = \\text{step}(z), \\text{where } z=\\mathbf{x}^T \\mathbf{w}$.\n",
        "\n",
        "The most common step function used in Perceptrons is the **Heaviside step function** and sometimes the sign function is used instead.\n",
        "\n",
        "A single TLU can be used for simple linear binary classification. It computes a linear combination of the inputs and if the result exceeds a threshold, it outputs the positive class, else negative.\n",
        "\n",
        "Hebb's rule (Hebbian learning) - \"Cells that fire together, wire together.\" The connection weight between two neurons tends to increase when they fire simultaneously.\n",
        "\n",
        "> Note: The decision boundary of each output neuron is linear, so Perceptrons are incapable of learning complex patterns, **unless the training instances are linearly separable** and would then converge to a solution (called the *Perceptron convergence theorem)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lhprD9GfO3B"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5dtS7I_fO3C",
        "outputId": "edc53105-4470-4479-e873-6cc1cea46486"
      },
      "source": [
        "iris = load_iris()\n",
        "X = iris.data[:, (2,3)] # petal length, petal width\n",
        "y = (iris.target == 0).astype(np.int) # Iris setosa?\n",
        "\n",
        "per_clf = Perceptron()\n",
        "per_clf.fit(X, y)\n",
        "\n",
        "y_pred = per_clf.predict([[2, 0.5]])\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxY1kr8zfO3F"
      },
      "source": [
        "> Note: Scikit-Learn's `Perceptron` is equivalent to using `SGDClassifier` with following hyperparameters: \n",
        "\n",
        "> - `loss=\"perceptron\"`\n",
        "> - `learning_rate=\"constant\"`\n",
        "> - `eta0=1` (the learning rate)\n",
        "> - `penalty=None` (no regularization)\n",
        "\n",
        "> Note: Perceptrons do not output a class probability; they make predictions based on a hard threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J97qme4rfO3G"
      },
      "source": [
        "### 10.1.4 The Multilayer Perceptron and Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qGFX5yafO3G"
      },
      "source": [
        "An MLP is composed of: \n",
        "- One (passthrough) **input layer**\n",
        "- One or more layers of TLUs called **hidden layers**\n",
        "- One final layer of TLUs called the **output layer**\n",
        "- Every layer except output layer includes a bias neuron and is fully connected to the next layer (implicit, always true)\n",
        "\n",
        "> Note: The signal flows only in one direction (from the inputs to the outputs), so this architecture is an example of a **feedforward neural network (FNN)**.\n",
        "\n",
        "When an ANN contains a deep stack of hidden layers, it is called a **deep neural network (DNN)**, \"10s, 100s+ layers => Deep Learning\".\n",
        "\n",
        "**Backpropagation** - It is Gradient Descent using an efficient technique for computing the gradients automatically. In other words, it can find out how each connection weight and each bias term should be tweaked in order to reduce the error. Once it has these gradients, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution.\n",
        "\n",
        "In detail:\n",
        "\n",
        "1. It handles one mini-batch at a time and goes through the full training set multiple times. Each pass is called an **epoch**.\n",
        "\n",
        "2. **Forward pass**: Each mini-batch is passed to the network's input layer and into the first hidden layer. It computs the output and passed to next layer. All intermediate results are preserved.\n",
        "\n",
        "3. Algorithm measures network's output error (ie. loss function that compares the desired output and the actual output of the network, and returns some measure of the error).\n",
        "\n",
        "4. Computes how much each output contributed to the error. Done analytically by applying the **chain rule** from calculus.\n",
        "\n",
        "5. Measures how much of these error contributions (**error gradient**) came from each connection in the layer below, using the chain rule, working backward until reaching the input layer.\n",
        "\n",
        "6. Performs a Gradient Descent step to tweak all connection weights in the network, using the error gradients it just computed.\n",
        "\n",
        "> Note: It is important to **initialize all the hidden layers' connection weights randomly**, or else training will fail.\n",
        "\n",
        "> For example, if you initialize all weights and biases to zero, then all neurons in a given layer will be perfectly identical, and backpropagation will affect them in the exactly the same way, so they will remain identical.\n",
        "\n",
        "> If instead you randomly initialize the weights, you **break the symmetry** and allow backpropagation to train a diverse team of neurons.\n",
        "\n",
        "Key change is to **replace the step function with the logistic (sigmoid) function**: $ \\sigma(z) = 1/ (1 + \\text{exp}(-z)) $, since step function (Heaviside/sign) only has flat segments and Gradient Descent cannot move on flat segments.\n",
        "\n",
        "Other popular choices are:\n",
        "- Hyperbolic tangent function: $ \\tanh (z) = 2\\sigma(2z) -1 $:\n",
        "    - S-shaped, continuous, differentiable\n",
        "    - Output value ranges from -1 to 1\n",
        "    - Tends to make each layer's output centered around 0 at beginning of training\n",
        "    - Often helps speed up convergence\n",
        "\n",
        "- Rectified Linear Unit function: $ \\text{ReLU}(z) = \\max(0, z) $:\n",
        "    - Continuous\n",
        "    - Not differentiable at $z=0$\n",
        "    - Derivative is 0 for $z<0$\n",
        "    - In practice, it works well and fast to compute\n",
        "    - Become the default\n",
        "    - Does not have a maximum output value, helps reduce issues during GD\n",
        "\n",
        "> Note: If you chain several linear transformations, all you get is a linear transformation.\n",
        "\n",
        "> If $f(x) = 2x + 3$ and $g(x)= 5x - 1$, then $f(g(x)) = 2(5x - 1) + 3 = 10x + 1$.\n",
        "\n",
        "> So if you don't have some nonlinearity between layers, then even a deep stack of layers is equivalent to a single layer, and you can't solve very complex problems with that.\n",
        "\n",
        "> Conversely, a large enough DNN with nonlinear activations can theoretically approximate any continuous function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUx7y3XVfO3H"
      },
      "source": [
        "### 10.1.5 Regression MLPs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSge83QafO3J"
      },
      "source": [
        "If you want to predict a single value (eg. price of a house), then you just need a single output neuron. \n",
        "\n",
        "For multivariate regression, you need one output neuron per output dimension (eg. to locate the center of an object in an image, you need to predict a 2D coordinate => 2 output neurons).\n",
        "\n",
        "In general, when building an MLP for regression, **you do not want to use any activation function for the output neurons**, so they are free to output any range of values.\n",
        "\n",
        "The loss function to use during training is typically the mean squared error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU-AxmZhfO3J"
      },
      "source": [
        "### 10.1.6 Classification MLPs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lgr880ZfO3K"
      },
      "source": [
        "For binary classification problems, you just need a single output neuron using the logistic activation function: the output will be a number between 0 and 1, which you can interpret as the estimated probability of the positive class (negative class would be $ 1 - \\text{prob of positive class})$.\n",
        "\n",
        "For multilabel binary classification problems, dedicate one output neuron for each positive class.\n",
        "\n",
        "> Note: Output probabilities do not necessarily add up to 1. This lets the model output any combination of labels.\n",
        "\n",
        "If each instance can belong only to a single class, then **you need to have one output neuron per class and use softmax activation function for the whole output layer** (all estimated probabilities are between 0 and 1 and add up to 1, required if classes are exclusive).\n",
        "\n",
        "Since we are predicting probability distributions, cross-entropy loss (also called log loss) is generally a good choice for loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xCP9zRMfO3K"
      },
      "source": [
        "## 10.2 Implementing MLPs with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VORKEWQbfO3L"
      },
      "source": [
        "### 10.2.1 Installing TensorFlow 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8krsNUBfXZ7"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n6LwRg7efgiK",
        "outputId": "7753cddd-3b26-4933-cf7b-d0e20607cea8"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vt3eQzB9fk_T",
        "outputId": "b84ccc80-0fdc-4fa8-e1e9-c0b6ace78d11"
      },
      "source": [
        "keras.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIfUJqw7fO3L"
      },
      "source": [
        "### 10.2.2 Building an Image Classifier Using the Sequential API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq07PyMoiSit"
      },
      "source": [
        "This chapter will be tackling the Fashion MNIST. It has the exact same format as MNIST but the images represent fashion items rather than handwritten digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA4PDwOsiEvY"
      },
      "source": [
        "#### Using Keras to load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wpt010Fil7Z"
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\r\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWiL8OC4i_0y",
        "outputId": "ee6ebc72-ce12-4327-93cc-6e480dde95db"
      },
      "source": [
        "X_train_full.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq-C_xasjG7t",
        "outputId": "c75c4397-146f-425f-aaf1-cb2a5866ce7b"
      },
      "source": [
        "X_train_full.dtype"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('uint8')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV9jkT86jWUy"
      },
      "source": [
        "> Note: Every image is represented as a 28x28 array rather than a 1D array of size 784. The pixel intensities are represented as integers from 0 to 255 rather than floats from 0.0 to 255.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGPkjFD8jsMx"
      },
      "source": [
        "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\r\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\r\n",
        "\r\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\r\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UxaUuwYikdxh",
        "outputId": "01b44515-77d4-465c-e613-07bf1c13b0ad"
      },
      "source": [
        "class_names[y_train[0]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Coat'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcSqZplPk-MA"
      },
      "source": [
        "#### Creating the model using the Sequential API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSLNwfdPlvej"
      },
      "source": [
        "Here's a classification MLP with two hidden layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjgbSqjKlC9j"
      },
      "source": [
        "model = keras.models.Sequential()\r\n",
        "model.add(keras.layers.Flatten(input_shape=[28, 28]))\r\n",
        "model.add(keras.layers.Dense(300, activation=\"relu\"))\r\n",
        "model.add(keras.layers.Dense(100, activation=\"relu\"))\r\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE49vxeumGdU"
      },
      "source": [
        "Code explanation line by line:\r\n",
        "\r\n",
        "1. Creates a `Sequential` model.\r\n",
        "  - The simplest kind of Keras model for neural networks.\r\n",
        "  - Composed of a single stack of layers connected sequentially.\r\n",
        "\r\n",
        "2. Build the 1st layer and add it to the model. \r\n",
        "  - `Flatten` converts each input image into a 1D array.\r\n",
        "  - If it receives input data `X` it computes `X.reshape(-1, 1)`.\r\n",
        "  - Since it is the first layer in the model, specify the `input_shape`.\r\n",
        "\r\n",
        "3. Add a `Dense` hidden layer with 300 neurons.\r\n",
        "  - Uses the ReLU activation function.\r\n",
        "  - Each `Dense` layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs.\r\n",
        "  - Manages a vector of bias terms (one per neuron).\r\n",
        "  - When it receives input data, it computes $ h_{\\mathbf{W}, \\mathbf{b}}(\\mathbf{X}) = \\phi(\\mathbf{XW} + \\mathbf{b})$.\r\n",
        "\r\n",
        "4. Add a 2nd `Dense` hidden layer with 100 neurons.\r\n",
        "  - Uses ReLU activation function.\r\n",
        "\r\n",
        "5. Add a `Dense` output layer with 10 neurons (one per class).\r\n",
        "  - Uses softmax activation function (classes are exclusive).\r\n",
        "\r\n",
        "Alternatively, we can pass a list of layers when creating the `Sequential` model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba011CCgpmaW"
      },
      "source": [
        "model = keras.models.Sequential([\r\n",
        "  keras.layers.Flatten(input_shape=[28, 28]),\r\n",
        "  keras.layers.Dense(300, activation=\"relu\"),\r\n",
        "  keras.layers.Dense(100, activation=\"relu\"),\r\n",
        "  keras.layers.Dense(10, activation=\"softmax\")                           \r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD-IxNLYq3Qp",
        "outputId": "461c2e9f-030e-49d3-ce9b-3af04e816fe7"
      },
      "source": [
        "model.summary() # Displays all the model's layers\r\n",
        "                # None = batch size can be anything"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_6 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "nfmfEtUhqp3v",
        "outputId": "2ddb25e3-c289-4b26-b5ff-4cc29ee19708"
      },
      "source": [
        "keras.utils.plot_model(model) # Generates an image of your model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYLjSv51rZuj"
      },
      "source": [
        "> Note: `Dense` layers often have a lot of parameters (235,500 for first hidden layer). This gives the model quite a lot of flexibility to fit the training data, but the model runs the risk of overfitting, especially when there isn't a lot of training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADOJeMNHrxo1",
        "outputId": "34d3fb8b-027d-42df-c412-d5b626b17e7e"
      },
      "source": [
        "model.layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.layers.core.Flatten at 0x7fb89f2b1cd0>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7fb89f2b8a10>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7fb89f25b910>,\n",
              " <tensorflow.python.keras.layers.core.Dense at 0x7fb89f25b810>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qt4ngNTkr1YH",
        "outputId": "23f4cd83-7f5e-40a2-853e-f5cba4cdd115"
      },
      "source": [
        "hidden1 = model.layers[1]\r\n",
        "hidden1.name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'dense_18'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgLJwfjWsDMV",
        "outputId": "fd9944e6-d7ad-4692-d6a3-0b95ab2dbf2b"
      },
      "source": [
        "model.get_layer('dense_18') is hidden1 # 'dense_18' instead of book's 'dense'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InxMI-t5sR4A",
        "outputId": "5fff8b24-ccfe-4c2c-b2f0-2fa017da9d64"
      },
      "source": [
        "weights, biases = hidden1.get_weights()\r\n",
        "weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.02201853,  0.0059806 , -0.07226824, ..., -0.02784478,\n",
              "         0.0561084 , -0.05503958],\n",
              "       [-0.02628014, -0.04710997, -0.0044467 , ..., -0.04245372,\n",
              "         0.04183065,  0.06866759],\n",
              "       [-0.05730478, -0.00558148, -0.01771047, ..., -0.0012911 ,\n",
              "         0.03570967,  0.05916241],\n",
              "       ...,\n",
              "       [-0.02621885,  0.02221927, -0.04146211, ..., -0.02160297,\n",
              "        -0.00219274, -0.04045064],\n",
              "       [-0.02129816,  0.01027724, -0.05901189, ...,  0.04627035,\n",
              "        -0.04596421,  0.00605425],\n",
              "       [ 0.07126527, -0.04009555,  0.00080884, ...,  0.01991408,\n",
              "        -0.06352869,  0.0657769 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBj80IyusXcj",
        "outputId": "8106ccbd-9740-4178-c0ea-bab119bc203d"
      },
      "source": [
        "weights.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkH3JtFzsZ8d",
        "outputId": "7fa68461-bdf8-4cb0-a455-14ab0c5ea65c"
      },
      "source": [
        "biases"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ticgz_Csc4g",
        "outputId": "7a2ee286-b1ba-417a-e3f6-172f29414812"
      },
      "source": [
        "biases.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6XH6UoasnL8"
      },
      "source": [
        "> Note: The `Dense` layer initialized the connection weights randomly (to break symmetry) and the biases were initialized to 0s (which is fine).\r\n",
        "\r\n",
        "> Note: It is recommended to specify the `input_shape` when creating the first layer in a `Sequential` model. Until the model is really built, the layers will not have any weights, and you will not be able to do certain things (such as print the model summary or save the model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVGT8SzslFrE"
      },
      "source": [
        "#### Compiling the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIzLQs5DtlcF"
      },
      "source": [
        "After a model is created, call `compile()` to specify the loss function, optimizer, and/or a list of extra metrics to compute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp9RCp13twRc"
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\r\n",
        "              optimizer=\"sgd\",\r\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQbqX1z5uEop"
      },
      "source": [
        "Code explanation:\r\n",
        "\r\n",
        "- `loss=\"sparse_categorial_crossentropy\"`:\r\n",
        "  - We have sparse labels (ie. for each instance, there is just a target class index, from 0-9).\r\n",
        "  - Classes are exclusive.\r\n",
        "\r\n",
        "- `optimizer=\"sgd\"`:\r\n",
        "  - We will train the model using simple Stochastic Gradient Descent.\r\n",
        "  - Keras will perform the backpropagation (ie. reverse-mode autodiff + Gradient Descent).\r\n",
        "  - Note: When using the SGD optimizer, it is important to tune the learning rate. So generally use `optimizer=keras.optimizers.SGD(lr=???)` to set the learning rate, rather than `optimizer=\"sgd\"` which defaults to `lr=0.01`.\r\n",
        "\r\n",
        "- `metrics=[\"accuracy\"]`:\r\n",
        "  - It is a classifier so it's useful to measure it's \"accuracy.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOvGSrvxlIyH"
      },
      "source": [
        "#### Training and evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0xXUyUCvuQU",
        "outputId": "cec4da8f-e5f6-4cad-b263-96881c3fd798"
      },
      "source": [
        "history = model.fit(X_train, y_train, epochs=30,\r\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "1719/1719 [==============================] - 6s 3ms/step - loss: 0.9839 - accuracy: 0.6913 - val_loss: 0.5234 - val_accuracy: 0.8220\n",
            "Epoch 2/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4967 - accuracy: 0.8299 - val_loss: 0.4568 - val_accuracy: 0.8412\n",
            "Epoch 3/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4520 - accuracy: 0.8431 - val_loss: 0.4313 - val_accuracy: 0.8496\n",
            "Epoch 4/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4195 - accuracy: 0.8550 - val_loss: 0.4083 - val_accuracy: 0.8558\n",
            "Epoch 5/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3986 - accuracy: 0.8597 - val_loss: 0.4061 - val_accuracy: 0.8596\n",
            "Epoch 6/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3847 - accuracy: 0.8663 - val_loss: 0.3737 - val_accuracy: 0.8700\n",
            "Epoch 7/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3699 - accuracy: 0.8693 - val_loss: 0.3605 - val_accuracy: 0.8762\n",
            "Epoch 8/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3587 - accuracy: 0.8735 - val_loss: 0.3465 - val_accuracy: 0.8792\n",
            "Epoch 9/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3423 - accuracy: 0.8791 - val_loss: 0.3531 - val_accuracy: 0.8708\n",
            "Epoch 10/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3349 - accuracy: 0.8797 - val_loss: 0.3391 - val_accuracy: 0.8792\n",
            "Epoch 11/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3288 - accuracy: 0.8805 - val_loss: 0.3364 - val_accuracy: 0.8792\n",
            "Epoch 12/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3197 - accuracy: 0.8862 - val_loss: 0.3540 - val_accuracy: 0.8744\n",
            "Epoch 13/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3090 - accuracy: 0.8901 - val_loss: 0.3324 - val_accuracy: 0.8828\n",
            "Epoch 14/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3026 - accuracy: 0.8906 - val_loss: 0.3178 - val_accuracy: 0.8850\n",
            "Epoch 15/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3014 - accuracy: 0.8917 - val_loss: 0.3260 - val_accuracy: 0.8820\n",
            "Epoch 16/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2879 - accuracy: 0.8968 - val_loss: 0.3255 - val_accuracy: 0.8806\n",
            "Epoch 17/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2827 - accuracy: 0.8970 - val_loss: 0.3156 - val_accuracy: 0.8852\n",
            "Epoch 18/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2773 - accuracy: 0.9008 - val_loss: 0.3067 - val_accuracy: 0.8920\n",
            "Epoch 19/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2751 - accuracy: 0.9002 - val_loss: 0.3109 - val_accuracy: 0.8890\n",
            "Epoch 20/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2643 - accuracy: 0.9048 - val_loss: 0.3082 - val_accuracy: 0.8860\n",
            "Epoch 21/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2585 - accuracy: 0.9072 - val_loss: 0.3145 - val_accuracy: 0.8880\n",
            "Epoch 22/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2510 - accuracy: 0.9085 - val_loss: 0.3060 - val_accuracy: 0.8906\n",
            "Epoch 23/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2470 - accuracy: 0.9103 - val_loss: 0.3162 - val_accuracy: 0.8824\n",
            "Epoch 24/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2471 - accuracy: 0.9123 - val_loss: 0.2979 - val_accuracy: 0.8922\n",
            "Epoch 25/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2378 - accuracy: 0.9153 - val_loss: 0.3176 - val_accuracy: 0.8860\n",
            "Epoch 26/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2423 - accuracy: 0.9145 - val_loss: 0.2971 - val_accuracy: 0.8906\n",
            "Epoch 27/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2327 - accuracy: 0.9152 - val_loss: 0.2962 - val_accuracy: 0.8884\n",
            "Epoch 28/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2317 - accuracy: 0.9149 - val_loss: 0.3161 - val_accuracy: 0.8792\n",
            "Epoch 29/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2290 - accuracy: 0.9182 - val_loss: 0.2930 - val_accuracy: 0.8920\n",
            "Epoch 30/30\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2255 - accuracy: 0.9195 - val_loss: 0.3078 - val_accuracy: 0.8852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjGTWDoL0Lfr"
      },
      "source": [
        "> Note: Instead of passing a validation set using `validation_data` argument, set `validation_split` to the ratio of the training set you want Keras to use for validation.\r\n",
        "\r\n",
        "> For example, `validation_split=0.1` tells Keras to use the last 10% of the data (before shuffling) for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjwEdMqx0wzx"
      },
      "source": [
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "XzcQ09lE01h4",
        "outputId": "8fe29999-247d-4f82-fecd-caa64f8df1fb"
      },
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\r\n",
        "plt.grid(True)\r\n",
        "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi81MlK318ev"
      },
      "source": [
        "Notes on the plot:\r\n",
        "\r\n",
        "- Both training and validation accuracy steadily increase.\r\n",
        "- Both traing and validation loss decrease.\r\n",
        "- Validation curves are close to training curves => not much overfitting.\r\n",
        "- Training curve should be shifted by half an epoch to the left (validation is recorded at end of each epoch).\r\n",
        "- Training set performance beats validation performance (generally the case).\r\n",
        "- Model hasn't converged yet since validation loss is still going down.\r\n",
        "- Can continue training by calling `fit()` again and Keras continues where it left off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2QIuvgI3BL3",
        "outputId": "76c4eefa-9969-4ffc-a79b-9829f172c5f5"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 59.2250 - accuracy: 0.8543\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[59.22496795654297, 0.8543000221252441]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqCP_ZOylNqa"
      },
      "source": [
        "#### Using the model to make predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7njySUoE3WcW",
        "outputId": "3ac3d462-d4d8-4cab-8455-344752639443"
      },
      "source": [
        "X_new = X_test[:3] # Use first 3 instances of test set as new instances\r\n",
        "y_proba = model.predict(X_new)\r\n",
        "y_proba.round(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8Pxxem131PG",
        "outputId": "fcdffa66-1976-447d-80eb-d8d9bd4d2d18"
      },
      "source": [
        "y_pred = np.argmax(model.predict(X_new), axis=-1) # .predict_classes() is depreciated, use the following code\r\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 2, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15AvHfGu48Xh",
        "outputId": "464baf61-bda5-4d0e-8de3-26238e81bab0"
      },
      "source": [
        "np.array(class_names)[y_pred]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtg9JjSG5C2O",
        "outputId": "a39cbe76-9abc-4687-871a-c2000a819578"
      },
      "source": [
        "y_new = y_test[:3]\r\n",
        "y_new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 2, 1], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f4jXSZAfO3L"
      },
      "source": [
        "### 10.2.3 Building a Regression MLP Using the Sequential API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwLcdJCdgZHW"
      },
      "source": [
        "Let's tackle a regression neural network using the California housing problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrlImF7pgfxp"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nTNp9c2gsxD",
        "outputId": "4e761e19-2c45-4fdd-fbae-c0113dfa4e02"
      },
      "source": [
        "housing = fetch_california_housing()\r\n",
        "\r\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\r\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\r\n",
        "\r\n",
        "scaler = StandardScaler()\r\n",
        "X_train = scaler.fit_transform(X_train)\r\n",
        "X_valid = scaler.transform(X_valid)\r\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJz1zESKhWuh"
      },
      "source": [
        "Since the dataset is quite noisy, we just use a single hidden layer with fewer neurons than before, to avoid overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WUBe9oyhM58",
        "outputId": "2762ebe4-7628-4c2e-d22f-9039ca8b29f2"
      },
      "source": [
        "model = keras.models.Sequential([\r\n",
        "  keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\r\n",
        "  keras.layers.Dense(1)\r\n",
        "])\r\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\r\n",
        "history = model.fit(X_train, y_train, epochs=20,\r\n",
        "                    validation_data=(X_valid, y_valid))\r\n",
        "mse_test = model.evaluate(X_test, y_test)\r\n",
        "X_new = X_test[:3] # pretend these are new instances\r\n",
        "y_pred = model.predict(X_new)\r\n",
        "y_pred"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 2.6731 - val_loss: 0.5747\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5157 - val_loss: 0.4894\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4352 - val_loss: 0.4520\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3983 - val_loss: 0.4377\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4082 - val_loss: 0.4317\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3895 - val_loss: 0.4218\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3831 - val_loss: 0.4235\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3754 - val_loss: 0.4143\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3870 - val_loss: 0.4092\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3716 - val_loss: 0.4089\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3813 - val_loss: 0.4071\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3572 - val_loss: 0.3984\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3752 - val_loss: 0.4011\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3632 - val_loss: 0.3941\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3525 - val_loss: 0.4015\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3638 - val_loss: 0.3879\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3698 - val_loss: 0.3905\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3581 - val_loss: 0.3877\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3575 - val_loss: 0.3875\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3688 - val_loss: 0.3852\n",
            "162/162 [==============================] - 0s 831us/step - loss: 0.3688\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.9992337],\n",
              "       [1.5287693],\n",
              "       [1.7307444]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-NMb3cafO3M"
      },
      "source": [
        "### 10.2.4 Building Complex Models Using the Functional API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_7pT4pOjB79"
      },
      "source": [
        "One example of a nonsequential neural network is a Wide & Deep neural network. It connects all or part of the inputs directly to the output layer. This architecture makes it possible for the neural network to learn both deep patterns (using the deep path) and simple rules (through the short path)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXMUDFxnjZgC"
      },
      "source": [
        "input_ = keras.layers.Input(shape=X_train.shape[1:])\r\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\r\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\r\n",
        "concat = keras.layers.Concatenate()([input_, hidden2])\r\n",
        "output = keras.layers.Dense(1)(concat)\r\n",
        "model = keras.Model(inputs=[input_], outputs=[output])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmHFsO_aj-Cm"
      },
      "source": [
        "Code explanation line by line:\r\n",
        "\r\n",
        "1. Create an `Input` object.\r\n",
        "  - The name `input_` is used to avoid overshadowing Python's built in `input()` function.\r\n",
        "  - Specify `shape` and `dtype`.\r\n",
        "\r\n",
        "2. Create a `Dense` layer with 30 neurons.\r\n",
        "  - Uses ReLU activation function.\r\n",
        "  - Call it like a function, passing it the input.\r\n",
        "  - This is why this is called **Functional API**.\r\n",
        "  - Tells Keras how to connect the layers together, no data is processed.\r\n",
        "\r\n",
        "3. Create a second hidden layer.\r\n",
        "  - Use it as a function.\r\n",
        "  - Pass the output of the first hidden layer.\r\n",
        "\r\n",
        "4. Create a `Concatenate` layer.\r\n",
        "  - Use it as a function.\r\n",
        "  - Concatenates the input and output of the second hidden layer.\r\n",
        "  - Alternatively, `keras.layers.concatenate()` creates a `Concatenate` layer and immediately calls it with the given inputs.\r\n",
        "\r\n",
        "5. Create the output layer.\r\n",
        "  - Single neuron (we want a single value).\r\n",
        "  - No activation function (we want the value to be any range, no constraints).\r\n",
        "  - Call it like a function, passing the result of concatenation.\r\n",
        "\r\n",
        "6. Create a Keras `Model`.\r\n",
        "  - Specify inputs and outputs to use.\r\n",
        "\r\n",
        "What if you want to send a subset of the features through the wide path and a different subset (possibly overlapping) through the deep path?\r\n",
        "\r\n",
        "One solution is to use multiple inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP-ETbaanVmz"
      },
      "source": [
        "input_A = keras.layers.Input(shape=[5], name=\"wide_input\") # 5 features through wide path (features 0-4)\r\n",
        "input_B = keras.layers.Input(shape=[6], name=\"deep_input\") # 6 features through deep path (features 2-7)\r\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\r\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\r\n",
        "concat = keras.layers.concatenate([input_A, hidden2])\r\n",
        "output = keras.layers.Dense(1, name=\"output\")(concat)\r\n",
        "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoDHDw4bok0e"
      },
      "source": [
        "Because we specified, `inputs=[input_A, input_B]`, we must pass a pair of matrices `(X_train_A, X_train_B)`: one per input - same for `X_valid`, `X_test`, `X_new` when calling `evaluate()` or `predict()`.\r\n",
        "\r\n",
        "> Note: Alternatively, you can pass a dictionary mapping the input names to the input values like `{\"wide_input\": X_train_A, \"deep_input\": X_train_B}`. This is especially useful when there are many inputs, to avoid getting the order wrong."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK8nEkwfpTGN",
        "outputId": "339bfc8e-2fce-4b40-d8bb-4ff351856154"
      },
      "source": [
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\r\n",
        "\r\n",
        "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\r\n",
        "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\r\n",
        "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\r\n",
        "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\r\n",
        "\r\n",
        "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\r\n",
        "                    validation_data=((X_valid_A, X_valid_B), y_valid))\r\n",
        "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\r\n",
        "y_pred = model.predict((X_new_A, X_new_B))\r\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 2.8056 - val_loss: 0.8478\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.7225 - val_loss: 0.6446\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.6182 - val_loss: 0.5956\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5733 - val_loss: 0.5670\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5726 - val_loss: 0.5452\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5384 - val_loss: 0.5304\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5222 - val_loss: 0.5174\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5121 - val_loss: 0.5060\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5049 - val_loss: 0.4973\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5065 - val_loss: 0.4922\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4901 - val_loss: 0.4876\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4884 - val_loss: 0.4826\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4947 - val_loss: 0.4780\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4836 - val_loss: 0.4747\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4865 - val_loss: 0.4670\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4794 - val_loss: 0.4633\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4766 - val_loss: 0.4595\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4607 - val_loss: 0.4567\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4497 - val_loss: 0.4570\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4497 - val_loss: 0.4524\n",
            "162/162 [==============================] - 0s 847us/step - loss: 0.4996\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.636394 ],\n",
              "       [3.2428286],\n",
              "       [3.1800995]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uR72TE4rj2v"
      },
      "source": [
        "Cases to have multiple outputs:\r\n",
        "\r\n",
        "- Task may demand it.\r\n",
        "  - For example, locate and classify the main object in a picture.\r\n",
        "  - This is both a regression and classification task.\r\n",
        "\r\n",
        "- Multiple independent tasks based on the same data.\r\n",
        "  - Neural networks can learn features in the data that are useful across tasks.\r\n",
        "  - For example, multitask classification on pictures of faces.\r\n",
        "  - 1 output to classify the person's facial expression (smiling, surprised, etc.)\r\n",
        "  - 1 output to identify whether they are wearing glasses or not.\r\n",
        "\r\n",
        "- Regularization technique (ie. a training constraint whose objective is to reduce overfitting and improve the model's ability to generalize).\r\n",
        "  - For example, add auxiliary outputs to ensure that the underlying part of the network learns something useful on its own, without relying on the rest of the network.\r\n",
        "\r\n",
        "The following code builds the network represented in Figure 10-16 - Aux. Output that branches out from Hidden 2 layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNBo5PHrteEF"
      },
      "source": [
        "input_A = keras.layers.Input(shape=[5], name=\"wide_input\") # 5 features through wide path (features 0-4)\r\n",
        "input_B = keras.layers.Input(shape=[6], name=\"deep_input\") # 6 features through deep path (features 2-7)\r\n",
        "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\r\n",
        "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\r\n",
        "concat = keras.layers.concatenate([input_A, hidden2])\r\n",
        "# Same as above, up to the main output layer\r\n",
        "output = keras.layers.Dense(1, name=\"main_output\")(concat)\r\n",
        "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\r\n",
        "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7pqqzEKuJ-U"
      },
      "source": [
        "Each output will need its own loss function.\r\n",
        "\r\n",
        "We care much more about the main output than about the auxiliary output (just used for regularization), so we want to give the main output's loss a much greater weight."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpKPEjLKuei0",
        "outputId": "2c8b09e3-762d-4889-9efc-d70d0552f804"
      },
      "source": [
        "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")\r\n",
        "\r\n",
        "history = model.fit(\r\n",
        "    [X_train_A, X_train_B], [y_train, y_train], epochs=20, # Main & Auxiliary output same predictions => same labels\r\n",
        "    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\r\n",
        "\r\n",
        "total_loss, main_loss, aux_loss = model.evaluate(\r\n",
        "    [X_test_A, X_test_B], [y_test, y_test])\r\n",
        "\r\n",
        "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\r\n",
        "\r\n",
        "y_pred_main, y_pred_aux"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 1.6885 - main_output_loss: 1.6079 - aux_output_loss: 2.4138 - val_loss: 1.2296 - val_main_output_loss: 1.2167 - val_aux_output_loss: 1.3458\n",
            "Epoch 2/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.7378 - main_output_loss: 0.6827 - aux_output_loss: 1.2329 - val_loss: 0.6301 - val_main_output_loss: 0.5818 - val_aux_output_loss: 1.0641\n",
            "Epoch 3/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.5698 - main_output_loss: 0.5182 - aux_output_loss: 1.0337 - val_loss: 0.4929 - val_main_output_loss: 0.4533 - val_aux_output_loss: 0.8494\n",
            "Epoch 4/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4810 - main_output_loss: 0.4413 - aux_output_loss: 0.8387 - val_loss: 0.4661 - val_main_output_loss: 0.4347 - val_aux_output_loss: 0.7485\n",
            "Epoch 5/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4682 - main_output_loss: 0.4382 - aux_output_loss: 0.7383 - val_loss: 0.4464 - val_main_output_loss: 0.4209 - val_aux_output_loss: 0.6759\n",
            "Epoch 6/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4300 - main_output_loss: 0.4052 - aux_output_loss: 0.6533 - val_loss: 0.4480 - val_main_output_loss: 0.4268 - val_aux_output_loss: 0.6382\n",
            "Epoch 7/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4506 - main_output_loss: 0.4285 - aux_output_loss: 0.6493 - val_loss: 0.4270 - val_main_output_loss: 0.4074 - val_aux_output_loss: 0.6037\n",
            "Epoch 8/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4165 - main_output_loss: 0.3960 - aux_output_loss: 0.6009 - val_loss: 0.4314 - val_main_output_loss: 0.4131 - val_aux_output_loss: 0.5958\n",
            "Epoch 9/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4312 - main_output_loss: 0.4124 - aux_output_loss: 0.6006 - val_loss: 0.4247 - val_main_output_loss: 0.4074 - val_aux_output_loss: 0.5799\n",
            "Epoch 10/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4231 - main_output_loss: 0.4045 - aux_output_loss: 0.5906 - val_loss: 0.4124 - val_main_output_loss: 0.3953 - val_aux_output_loss: 0.5665\n",
            "Epoch 11/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4208 - main_output_loss: 0.4033 - aux_output_loss: 0.5784 - val_loss: 0.4178 - val_main_output_loss: 0.4021 - val_aux_output_loss: 0.5589\n",
            "Epoch 12/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4057 - main_output_loss: 0.3888 - aux_output_loss: 0.5571 - val_loss: 0.4160 - val_main_output_loss: 0.4004 - val_aux_output_loss: 0.5561\n",
            "Epoch 13/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4048 - main_output_loss: 0.3877 - aux_output_loss: 0.5590 - val_loss: 0.4020 - val_main_output_loss: 0.3862 - val_aux_output_loss: 0.5443\n",
            "Epoch 14/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4022 - main_output_loss: 0.3873 - aux_output_loss: 0.5364 - val_loss: 0.3965 - val_main_output_loss: 0.3809 - val_aux_output_loss: 0.5370\n",
            "Epoch 15/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3819 - main_output_loss: 0.3665 - aux_output_loss: 0.5207 - val_loss: 0.3944 - val_main_output_loss: 0.3792 - val_aux_output_loss: 0.5311\n",
            "Epoch 16/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3877 - main_output_loss: 0.3716 - aux_output_loss: 0.5330 - val_loss: 0.3895 - val_main_output_loss: 0.3743 - val_aux_output_loss: 0.5259\n",
            "Epoch 17/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3868 - main_output_loss: 0.3717 - aux_output_loss: 0.5223 - val_loss: 0.4063 - val_main_output_loss: 0.3934 - val_aux_output_loss: 0.5229\n",
            "Epoch 18/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3967 - main_output_loss: 0.3827 - aux_output_loss: 0.5222 - val_loss: 0.3894 - val_main_output_loss: 0.3757 - val_aux_output_loss: 0.5125\n",
            "Epoch 19/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3815 - main_output_loss: 0.3663 - aux_output_loss: 0.5188 - val_loss: 0.3799 - val_main_output_loss: 0.3662 - val_aux_output_loss: 0.5030\n",
            "Epoch 20/20\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3674 - main_output_loss: 0.3529 - aux_output_loss: 0.4981 - val_loss: 0.3777 - val_main_output_loss: 0.3643 - val_aux_output_loss: 0.4987\n",
            "162/162 [==============================] - 0s 1ms/step - loss: 0.4028 - main_output_loss: 0.3895 - aux_output_loss: 0.5227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[3.6975222],\n",
              "        [3.1889672],\n",
              "        [3.41285  ]], dtype=float32), array([[3.4704459],\n",
              "        [3.1441848],\n",
              "        [3.3152936]], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVE1q_DNfO3N"
      },
      "source": [
        "### 10.2.5 Using the Subclassing API to Build Dynamic Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUrmAvykyTHf"
      },
      "source": [
        "Sequential and Functional API are declarative: you start by declaring which layers you want to use and how they should be connected, and only then can you start feeding the model some data for training.\r\n",
        "\r\n",
        "Advantages:\r\n",
        "- Easily saved, cloned, and shared\r\n",
        "- Structure can be displayed and analyzed\r\n",
        "- Framework can infer shapes and check types\r\n",
        "- Errors can be caught early (ie. before any data ever goes through model)\r\n",
        "- Easy to debug (static graph of layers)\r\n",
        "\r\n",
        "Disadvantages:\r\n",
        "- Static\r\n",
        "- Cannot support models involving loops, varying shapes, conditional branching, other dynamic behaviors\r\n",
        "\r\n",
        "Subclassing API is a more imperative programming style.\r\n",
        "1. Subclass the `Model` class\r\n",
        "2. Create the layers you need in the constructor\r\n",
        "3. Use `call()` to perform the computations\r\n",
        "4. Then compile, evaluate, make predictions, exactly like before\r\n",
        "\r\n",
        "For example, the following `WideAndDeepModel` class gives us an equivalent model to the one built with the Functional API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abkNpv7zz8Uv"
      },
      "source": [
        "class WideAndDeepModel(keras.Model):\r\n",
        "  def __init__(self, units=30, activation=\"relu\", **kwargs):\r\n",
        "    super().__init__(**kwargs) # handles standard args (eg. name)\r\n",
        "    self.hidden1 = keras.layers.Dense(units, activation=activation)\r\n",
        "    self.hidden2 = keras.layers.Dense(units, activation=activation)\r\n",
        "    self.main_output = keras.layers.Dense(1)\r\n",
        "    self.aux_output = keras.layers.Dense(1)\r\n",
        "\r\n",
        "  def call(self, inputs):\r\n",
        "    input_A, input_B = inputs\r\n",
        "    hidden1 = self.hidden1(input_B)\r\n",
        "    hidden2 = self.hidden2(hidden1)\r\n",
        "    concat = keras.layers.concatenate([input_A, hidden2])\r\n",
        "    main_output = self.main_output(concat)\r\n",
        "    aux_output = self.aux_output(hidden2)\r\n",
        "    return main_output, aux_output\r\n",
        "\r\n",
        "model = WideAndDeepModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLod4Jc81N9r"
      },
      "source": [
        "We do not need to create the inputs; we just use the `input` argument to the `call()` method and we separate the creation of the layers in the constructor from their usage in the `call()` method.\r\n",
        "\r\n",
        "Advantages:\r\n",
        "- You can do pretty much anything in the `call()` method:\r\n",
        "  - `for` loops\r\n",
        "  - `if` statements\r\n",
        "  - Low-level Tensorflow operations\r\n",
        "\r\n",
        "Disadvantages:\r\n",
        "- Model's architecture is hidden within the `call()` method, so Keras cannot easily inspect it.\r\n",
        "  - Cannot save or clone it\r\n",
        "  - When calling `summary()`, you only get a list of layers, without any information on their connections\r\n",
        "  - Keras cannot check types and shapes ahead of time\r\n",
        "  - Easier to make mistakes\r\n",
        "\r\n",
        "> Note: So unless you really need that extra flexibility, you should probably stick to the Sequential or Functional API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-xKysnGfO3N"
      },
      "source": [
        "### 10.2.6 Saving and Restoring a Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-e7zlDWE48ix",
        "outputId": "2859ec52-eb57-4aae-b1e0-0d07bfdae8a1"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\r\n",
        "\r\n",
        "model = keras.models.Sequential([ # or keras.Model([...])\r\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\r\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\r\n",
        "    keras.layers.Dense(1)\r\n",
        "])\r\n",
        "\r\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\r\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\r\n",
        "mse_test = model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 2.7403 - val_loss: 0.8034\n",
            "Epoch 2/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.7512 - val_loss: 0.6752\n",
            "Epoch 3/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.6632 - val_loss: 0.6161\n",
            "Epoch 4/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.6276 - val_loss: 0.5735\n",
            "Epoch 5/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5862 - val_loss: 0.5398\n",
            "Epoch 6/10\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5410 - val_loss: 0.5127\n",
            "Epoch 7/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5239 - val_loss: 0.4924\n",
            "Epoch 8/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4789 - val_loss: 0.4772\n",
            "Epoch 9/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5009 - val_loss: 0.4668\n",
            "Epoch 10/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4818 - val_loss: 0.4569\n",
            "162/162 [==============================] - 0s 976us/step - loss: 0.5059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnU-cSlR5YSK"
      },
      "source": [
        "model.save(\"my_keras_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5Lvz6NH5iTF"
      },
      "source": [
        "Keras uses the HDF5 format to save:\r\n",
        "- The model's architecture (every layer's hyperparmeters)\r\n",
        "- The values of all the model parameters for every layer (eg. connection weights and biases)\r\n",
        "- The optimizer and its hyperparameters and any state it may have"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxQ0wgGN5_rR"
      },
      "source": [
        "model = keras.models.load_model(\"my_keras_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmEsUKE26MQp"
      },
      "source": [
        "> Note: This will work when using the Sequential or Functional API, but not when using model subclassing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGSa4ECgfO3O"
      },
      "source": [
        "### 10.2.7 Using Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXgfieuP6wnN"
      },
      "source": [
        "The `fit()` method accepts a `callbacks` argument that lets you specify a list of objects that Keras will call at the start and end of training, at the start and end of each epoch, and even before and after processing each batch.\r\n",
        "\r\n",
        "`ModelCheckpoint` callback saves checkpoints of your model at regular intervals during training (default at the end of each epoch)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReaWwR7L6wAY"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\r\n",
        "\r\n",
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\r\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\r\n",
        "    keras.layers.Dense(1)\r\n",
        "])    \r\n",
        "\r\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb_RERPx7WbN",
        "outputId": "eeee668f-2c30-4460-e011-ca3538a49a87"
      },
      "source": [
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\r\n",
        "history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 3.3255\n",
            "Epoch 2/10\n",
            "363/363 [==============================] - 0s 960us/step - loss: 1.0364\n",
            "Epoch 3/10\n",
            "363/363 [==============================] - 0s 945us/step - loss: 0.7435\n",
            "Epoch 4/10\n",
            "363/363 [==============================] - 0s 941us/step - loss: 0.6265\n",
            "Epoch 5/10\n",
            "363/363 [==============================] - 0s 988us/step - loss: 0.5925\n",
            "Epoch 6/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5530\n",
            "Epoch 7/10\n",
            "363/363 [==============================] - 0s 944us/step - loss: 0.5165\n",
            "Epoch 8/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5047\n",
            "Epoch 9/10\n",
            "363/363 [==============================] - 0s 997us/step - loss: 0.5046\n",
            "Epoch 10/10\n",
            "363/363 [==============================] - 0s 948us/step - loss: 0.4778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcY3Jd2A7qTM",
        "outputId": "0ca8d0cc-5e64-4334-c851-363d8cb7bf16"
      },
      "source": [
        "# Early stopping\r\n",
        "\r\n",
        "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",\r\n",
        "                                                save_best_only=True) # Only save when its performance on valid set is best so far\r\n",
        "history = model.fit(X_train, y_train, epochs=10,\r\n",
        "                    validation_data=(X_valid, y_valid),\r\n",
        "                    callbacks=[checkpoint_cb])\r\n",
        "model = keras.models.load_model(\"my_keras_model.h5\") # roll back to best model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4792 - val_loss: 0.4649\n",
            "Epoch 2/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4704 - val_loss: 0.4570\n",
            "Epoch 3/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4628 - val_loss: 0.4500\n",
            "Epoch 4/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4555 - val_loss: 0.4444\n",
            "Epoch 5/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4493 - val_loss: 0.4397\n",
            "Epoch 6/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4440 - val_loss: 0.4349\n",
            "Epoch 7/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4390 - val_loss: 0.4302\n",
            "Epoch 8/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4343 - val_loss: 0.4269\n",
            "Epoch 9/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4299 - val_loss: 0.4240\n",
            "Epoch 10/10\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4259 - val_loss: 0.4206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOtAviaL8Whb",
        "outputId": "c2ab6618-25d4-484b-d196-71faec5e89c3"
      },
      "source": [
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,                # No progress on valid set for patience number of epochs\r\n",
        "                                                  restore_best_weights=True)\r\n",
        "history = model.fit(X_train, y_train, epochs=100,\r\n",
        "                    validation_data=(X_valid, y_valid),\r\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4223 - val_loss: 0.4171\n",
            "Epoch 2/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4184 - val_loss: 0.4142\n",
            "Epoch 3/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4153 - val_loss: 0.4123\n",
            "Epoch 4/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4124 - val_loss: 0.4095\n",
            "Epoch 5/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4094 - val_loss: 0.4072\n",
            "Epoch 6/100\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4064 - val_loss: 0.4047\n",
            "Epoch 7/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4036 - val_loss: 0.4032\n",
            "Epoch 8/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4012 - val_loss: 0.4012\n",
            "Epoch 9/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3985 - val_loss: 0.3988\n",
            "Epoch 10/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3963 - val_loss: 0.3969\n",
            "Epoch 11/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3942 - val_loss: 0.3956\n",
            "Epoch 12/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3920 - val_loss: 0.3936\n",
            "Epoch 13/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3899 - val_loss: 0.3925\n",
            "Epoch 14/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3877 - val_loss: 0.3898\n",
            "Epoch 15/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3857 - val_loss: 0.3898\n",
            "Epoch 16/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3838 - val_loss: 0.3874\n",
            "Epoch 17/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3822 - val_loss: 0.3856\n",
            "Epoch 18/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3807 - val_loss: 0.3838\n",
            "Epoch 19/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3789 - val_loss: 0.3821\n",
            "Epoch 20/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3776 - val_loss: 0.3815\n",
            "Epoch 21/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3757 - val_loss: 0.3804\n",
            "Epoch 22/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3743 - val_loss: 0.3797\n",
            "Epoch 23/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3725 - val_loss: 0.3789\n",
            "Epoch 24/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3712 - val_loss: 0.3764\n",
            "Epoch 25/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3702 - val_loss: 0.3752\n",
            "Epoch 26/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3685 - val_loss: 0.3755\n",
            "Epoch 27/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3673 - val_loss: 0.3738\n",
            "Epoch 28/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3660 - val_loss: 0.3723\n",
            "Epoch 29/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3649 - val_loss: 0.3717\n",
            "Epoch 30/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3637 - val_loss: 0.3705\n",
            "Epoch 31/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3626 - val_loss: 0.3693\n",
            "Epoch 32/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3615 - val_loss: 0.3693\n",
            "Epoch 33/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3607 - val_loss: 0.3688\n",
            "Epoch 34/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3592 - val_loss: 0.3682\n",
            "Epoch 35/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3588 - val_loss: 0.3667\n",
            "Epoch 36/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3575 - val_loss: 0.3658\n",
            "Epoch 37/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3564 - val_loss: 0.3652\n",
            "Epoch 38/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3558 - val_loss: 0.3651\n",
            "Epoch 39/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3550 - val_loss: 0.3640\n",
            "Epoch 40/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3539 - val_loss: 0.3643\n",
            "Epoch 41/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3528 - val_loss: 0.3623\n",
            "Epoch 42/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3523 - val_loss: 0.3609\n",
            "Epoch 43/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3515 - val_loss: 0.3609\n",
            "Epoch 44/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3504 - val_loss: 0.3607\n",
            "Epoch 45/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3497 - val_loss: 0.3603\n",
            "Epoch 46/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3491 - val_loss: 0.3604\n",
            "Epoch 47/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3486 - val_loss: 0.3599\n",
            "Epoch 48/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3478 - val_loss: 0.3575\n",
            "Epoch 49/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.3574\n",
            "Epoch 50/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3464 - val_loss: 0.3575\n",
            "Epoch 51/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3565\n",
            "Epoch 52/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3560\n",
            "Epoch 53/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3447 - val_loss: 0.3555\n",
            "Epoch 54/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3440 - val_loss: 0.3544\n",
            "Epoch 55/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3435 - val_loss: 0.3550\n",
            "Epoch 56/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3427 - val_loss: 0.3541\n",
            "Epoch 57/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3421 - val_loss: 0.3534\n",
            "Epoch 58/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3416 - val_loss: 0.3528\n",
            "Epoch 59/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3413 - val_loss: 0.3533\n",
            "Epoch 60/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3403 - val_loss: 0.3523\n",
            "Epoch 61/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3401 - val_loss: 0.3511\n",
            "Epoch 62/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3392 - val_loss: 0.3521\n",
            "Epoch 63/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3390 - val_loss: 0.3505\n",
            "Epoch 64/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3383 - val_loss: 0.3504\n",
            "Epoch 65/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3378 - val_loss: 0.3494\n",
            "Epoch 66/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3372 - val_loss: 0.3489\n",
            "Epoch 67/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3373 - val_loss: 0.3484\n",
            "Epoch 68/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3363 - val_loss: 0.3483\n",
            "Epoch 69/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3357 - val_loss: 0.3493\n",
            "Epoch 70/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3358 - val_loss: 0.3470\n",
            "Epoch 71/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3349 - val_loss: 0.3463\n",
            "Epoch 72/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3347 - val_loss: 0.3469\n",
            "Epoch 73/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3337 - val_loss: 0.3459\n",
            "Epoch 74/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3331 - val_loss: 0.3461\n",
            "Epoch 75/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3330 - val_loss: 0.3455\n",
            "Epoch 76/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3326 - val_loss: 0.3450\n",
            "Epoch 77/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3323 - val_loss: 0.3442\n",
            "Epoch 78/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3315 - val_loss: 0.3451\n",
            "Epoch 79/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3315 - val_loss: 0.3430\n",
            "Epoch 80/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3305 - val_loss: 0.3441\n",
            "Epoch 81/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3303 - val_loss: 0.3432\n",
            "Epoch 82/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3294 - val_loss: 0.3421\n",
            "Epoch 83/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3294 - val_loss: 0.3418\n",
            "Epoch 84/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3289 - val_loss: 0.3428\n",
            "Epoch 85/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3284 - val_loss: 0.3419\n",
            "Epoch 86/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3277 - val_loss: 0.3420\n",
            "Epoch 87/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3276 - val_loss: 0.3403\n",
            "Epoch 88/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3269 - val_loss: 0.3404\n",
            "Epoch 89/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3266 - val_loss: 0.3394\n",
            "Epoch 90/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3261 - val_loss: 0.3395\n",
            "Epoch 91/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3258 - val_loss: 0.3383\n",
            "Epoch 92/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3249 - val_loss: 0.3388\n",
            "Epoch 93/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3247 - val_loss: 0.3374\n",
            "Epoch 94/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3240 - val_loss: 0.3384\n",
            "Epoch 95/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3240 - val_loss: 0.3377\n",
            "Epoch 96/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3235 - val_loss: 0.3369\n",
            "Epoch 97/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3232 - val_loss: 0.3365\n",
            "Epoch 98/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3227 - val_loss: 0.3355\n",
            "Epoch 99/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3217 - val_loss: 0.3374\n",
            "Epoch 100/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3219 - val_loss: 0.3350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbO9Nd_Z9R04"
      },
      "source": [
        "Can write custom callbacks.\r\n",
        "\r\n",
        "For example, this will display the ratio between the validation and training loss during training (eg. to detect overfitting)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7-2VX-39e06"
      },
      "source": [
        "class PrintValTrainRatioCallback(keras.callbacks.Callback):\r\n",
        "  def on_epoch_end(self, epoch, logs):\r\n",
        "    print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeLLgbrWfO3O"
      },
      "source": [
        "### 10.2.8 Using TensorBoard for Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vb5asjr_IWj"
      },
      "source": [
        "TensorBoard is a great interactive visualization tool that you can use to:\r\n",
        "- View the learning curves during training\r\n",
        "- Compare learning curves between multiple runs\r\n",
        "- Visualize the computation graph\r\n",
        "- Analyze training statistics\r\n",
        "- View images generated by your model\r\n",
        "- Visualize complex multidimensional data projected down to 3D and automatically clustered for you\r\n",
        "\r\n",
        "To use it, you must modify your program so that it outputs the data you want to visualize to special binary log files called event files. In general, you want to point the TensorBoard server to a root log directory and configure you program so that it writes to a different subdirectory every time it runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2q7ENUDAFO7"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VscN4G5AInT"
      },
      "source": [
        "root_logdir = os.path.join(os.curdir, \"my_logs\")\r\n",
        "\r\n",
        "def get_run_logdir():\r\n",
        "    import time\r\n",
        "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\r\n",
        "    return os.path.join(root_logdir, run_id)\r\n",
        "\r\n",
        "run_logdir = get_run_logdir() # eg. './my_logs/run_2019_06_07-15_15_22'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CQzefnpA7rd"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\r\n",
        "\r\n",
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\r\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\r\n",
        "    keras.layers.Dense(1)\r\n",
        "])    \r\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm_YsBrqBHcN",
        "outputId": "e6589f1b-d45c-4bff-bec7-1d4075d1d056"
      },
      "source": [
        "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\r\n",
        "history = model.fit(X_train, y_train, epochs=30,\r\n",
        "                    validation_data=(X_valid, y_valid),\r\n",
        "                    callbacks=[checkpoint_cb, tensorboard_cb])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 3.7293 - val_loss: 1.1000\n",
            "Epoch 2/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.9233 - val_loss: 0.7208\n",
            "Epoch 3/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.6920 - val_loss: 0.6516\n",
            "Epoch 4/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.6287 - val_loss: 0.6212\n",
            "Epoch 5/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.6107 - val_loss: 0.5999\n",
            "Epoch 6/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.6001 - val_loss: 0.5803\n",
            "Epoch 7/30\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.5703 - val_loss: 0.5631\n",
            "Epoch 8/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5568 - val_loss: 0.5481\n",
            "Epoch 9/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5369 - val_loss: 0.5335\n",
            "Epoch 10/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5241 - val_loss: 0.5203\n",
            "Epoch 11/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5149 - val_loss: 0.5087\n",
            "Epoch 12/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5120 - val_loss: 0.4991\n",
            "Epoch 13/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4996 - val_loss: 0.4884\n",
            "Epoch 14/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5036 - val_loss: 0.4811\n",
            "Epoch 15/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4937 - val_loss: 0.4733\n",
            "Epoch 16/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4669 - val_loss: 0.4654\n",
            "Epoch 17/30\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4836 - val_loss: 0.4599\n",
            "Epoch 18/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4650 - val_loss: 0.4550\n",
            "Epoch 19/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4572 - val_loss: 0.4493\n",
            "Epoch 20/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4591 - val_loss: 0.4452\n",
            "Epoch 21/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4384 - val_loss: 0.4416\n",
            "Epoch 22/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4417 - val_loss: 0.4383\n",
            "Epoch 23/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4446 - val_loss: 0.4341\n",
            "Epoch 24/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4215 - val_loss: 0.4312\n",
            "Epoch 25/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4370 - val_loss: 0.4280\n",
            "Epoch 26/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4362 - val_loss: 0.4253\n",
            "Epoch 27/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4355 - val_loss: 0.4210\n",
            "Epoch 28/30\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4259 - val_loss: 0.4184\n",
            "Epoch 29/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4251 - val_loss: 0.4157\n",
            "Epoch 30/30\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.4076 - val_loss: 0.4135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC5PUXLuGiS0"
      },
      "source": [
        "run_logdir2 = get_run_logdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-JgJjo6GnD1"
      },
      "source": [
        "# FROM TEXTBOOK NOTEBOOK\r\n",
        "\r\n",
        "model = keras.models.Sequential([\r\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\r\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\r\n",
        "    keras.layers.Dense(1)\r\n",
        "])    \r\n",
        "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=0.05))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI1qNwMgGxBh",
        "outputId": "541bb4c6-0c33-4e9b-8bb5-4030f47dbf08"
      },
      "source": [
        "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir2)\r\n",
        "history = model.fit(X_train, y_train, epochs=30,\r\n",
        "                    validation_data=(X_valid, y_valid),\r\n",
        "                    callbacks=[checkpoint_cb, tensorboard_cb])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.8424 - val_loss: 1.0077\n",
            "Epoch 2/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4451 - val_loss: 0.4464\n",
            "Epoch 3/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3933 - val_loss: 0.3772\n",
            "Epoch 4/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3748 - val_loss: 0.3565\n",
            "Epoch 5/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3534 - val_loss: 0.3760\n",
            "Epoch 6/30\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3562 - val_loss: 0.3492\n",
            "Epoch 7/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3296 - val_loss: 0.4262\n",
            "Epoch 8/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3424 - val_loss: 0.3208\n",
            "Epoch 9/30\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3320 - val_loss: 0.3390\n",
            "Epoch 10/30\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3251 - val_loss: 0.3279\n",
            "Epoch 11/30\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.3154 - val_loss: 0.3197\n",
            "Epoch 12/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.2973 - val_loss: 0.3160\n",
            "Epoch 13/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3090 - val_loss: 0.3342\n",
            "Epoch 14/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3183 - val_loss: 0.3020\n",
            "Epoch 15/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.2937 - val_loss: 0.3067\n",
            "Epoch 16/30\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.2936 - val_loss: 0.3050\n",
            "Epoch 17/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.2925 - val_loss: 0.3071\n",
            "Epoch 18/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.2918 - val_loss: 0.3026\n",
            "Epoch 19/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.2837 - val_loss: 0.3019\n",
            "Epoch 20/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.2872 - val_loss: 0.2971\n",
            "Epoch 21/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.2865 - val_loss: 0.2945\n",
            "Epoch 22/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3008 - val_loss: 0.2962\n",
            "Epoch 23/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.2927 - val_loss: 0.2962\n",
            "Epoch 24/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.2786 - val_loss: 0.2946\n",
            "Epoch 25/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.2853 - val_loss: 0.3801\n",
            "Epoch 26/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3094 - val_loss: 0.3023\n",
            "Epoch 27/30\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 0.2827 - val_loss: 0.2903\n",
            "Epoch 28/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3010 - val_loss: 0.2861\n",
            "Epoch 29/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.2945 - val_loss: 0.2895\n",
            "Epoch 30/30\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.2834 - val_loss: 0.3024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXjZLPQHCLrM"
      },
      "source": [
        "%load_ext tensorboard\r\n",
        "%tensorboard --logdir=./my_logs --port=6006"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYP0oZreJA2z"
      },
      "source": [
        "The following code creates a `SummaryWriter` using the `create_file_writer()` function, and it uses this writer as a context to log scalars, histograms, images, audio, and text, all of which can then be visualized using TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxbfs-wsJYpl"
      },
      "source": [
        "test_logdir = get_run_logdir()\r\n",
        "writer = tf.summary.create_file_writer(test_logdir)\r\n",
        "with writer.as_default():\r\n",
        "    for step in range(1, 1000 + 1):\r\n",
        "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\r\n",
        "        data = (np.random.randn(100) + 2) * step / 100 # some random data\r\n",
        "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\r\n",
        "        images = np.random.randn(2, 32, 32, 3) # random 32x32 RGB images\r\n",
        "        tf.summary.image(\"my_images\", images * step / 1000, step=step)\r\n",
        "        texts = [\"The step is \" + str(step), \"Its square is \" + str(step**2)]\r\n",
        "        tf.summary.text(\"my_text\", texts, step=step)\r\n",
        "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\r\n",
        "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\r\n",
        "        tf.summary.audio(\"my_audio\", audio, sample_rate = 48000, step=step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyWnKeK1fO3O"
      },
      "source": [
        "## 10.3 Fine-Tuning Neural Networks Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gwf7NYcBAcl"
      },
      "source": [
        "The flexibility of neural networks is also one of their main drawbacks: there are many hyperparameters to tweak. One option is to simply try many combinations of hyperparameters and see which one works best on the validation set (eg. use `GridSearchCV` or `RandomizedSearchCV`).\r\n",
        "\r\n",
        "But first, we need to wrap our Keras models in objects that mimic regular Scikit-Learn regressors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0onPG84BeY_"
      },
      "source": [
        "# Create a function that will build and compile a Keras model\r\n",
        "# Given a set of hyperparameters\r\n",
        "\r\n",
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\r\n",
        "    model = keras.models.Sequential()\r\n",
        "    model.add(keras.layers.InputLayer(input_shape=input_shape))         # Input layer\r\n",
        "    for layer in range(n_hidden):                                       # Hidden layers\r\n",
        "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\r\n",
        "    model.add(keras.layers.Dense(1))                                    # Output layer\r\n",
        "    optimizer = keras.optimizers.SGD(lr=learning_rate)\r\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer)\r\n",
        "    return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NGgVsCtCjL4"
      },
      "source": [
        "# Next create a KerasRegressor model\r\n",
        "# Based on this build_model() function\r\n",
        "\r\n",
        "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpKss9C2DEJf"
      },
      "source": [
        "The `KerasRegressor` object is a thin wrapper around the Keras model built using `build_model()`. Now we can use this object like a regular Scikit-Learn regressor:\r\n",
        "- Train using `fit()`\r\n",
        "- Evaluate using `score()`\r\n",
        "- Make predictions using `predict()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAoWFQqJDYf2",
        "outputId": "97745f99-e249-4cd8-f8aa-2f4474ee4169"
      },
      "source": [
        "keras_reg.fit(X_train, y_train, epochs=100,\r\n",
        "              validation_data=(X_valid, y_valid),\r\n",
        "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\r\n",
        "mse_test = keras_reg.score(X_test, y_test)\r\n",
        "y_pred = keras_reg.predict(X_new)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "363/363 [==============================] - 1s 1ms/step - loss: 2.2255 - val_loss: 0.7189\n",
            "Epoch 2/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.6816 - val_loss: 0.6470\n",
            "Epoch 3/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.6098 - val_loss: 0.5912\n",
            "Epoch 4/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5365 - val_loss: 0.5616\n",
            "Epoch 5/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.5231 - val_loss: 0.5397\n",
            "Epoch 6/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4820 - val_loss: 0.5245\n",
            "Epoch 7/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4877 - val_loss: 0.5108\n",
            "Epoch 8/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4939 - val_loss: 0.5021\n",
            "Epoch 9/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4698 - val_loss: 0.4937\n",
            "Epoch 10/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4732 - val_loss: 0.4871\n",
            "Epoch 11/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4637 - val_loss: 0.4829\n",
            "Epoch 12/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4287 - val_loss: 0.4769\n",
            "Epoch 13/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4289 - val_loss: 0.4738\n",
            "Epoch 14/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4227 - val_loss: 0.4686\n",
            "Epoch 15/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4299 - val_loss: 0.4661\n",
            "Epoch 16/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4334 - val_loss: 0.4643\n",
            "Epoch 17/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4297 - val_loss: 0.4605\n",
            "Epoch 18/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4337 - val_loss: 0.4585\n",
            "Epoch 19/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.4558\n",
            "Epoch 20/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4165 - val_loss: 0.4527\n",
            "Epoch 21/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4209 - val_loss: 0.4522\n",
            "Epoch 22/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4135 - val_loss: 0.4514\n",
            "Epoch 23/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4105 - val_loss: 0.4478\n",
            "Epoch 24/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4227 - val_loss: 0.4463\n",
            "Epoch 25/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4046 - val_loss: 0.4464\n",
            "Epoch 26/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4119 - val_loss: 0.4426\n",
            "Epoch 27/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4016 - val_loss: 0.4419\n",
            "Epoch 28/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4075 - val_loss: 0.4413\n",
            "Epoch 29/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3988 - val_loss: 0.4381\n",
            "Epoch 30/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3989 - val_loss: 0.4390\n",
            "Epoch 31/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4056 - val_loss: 0.4363\n",
            "Epoch 32/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4019 - val_loss: 0.4344\n",
            "Epoch 33/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3916 - val_loss: 0.4342\n",
            "Epoch 34/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3944 - val_loss: 0.4315\n",
            "Epoch 35/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3841 - val_loss: 0.4299\n",
            "Epoch 36/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.4033 - val_loss: 0.4301\n",
            "Epoch 37/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3899 - val_loss: 0.4272\n",
            "Epoch 38/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3899 - val_loss: 0.4298\n",
            "Epoch 39/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3797 - val_loss: 0.4254\n",
            "Epoch 40/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3960 - val_loss: 0.4252\n",
            "Epoch 41/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3942 - val_loss: 0.4220\n",
            "Epoch 42/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3896 - val_loss: 0.4216\n",
            "Epoch 43/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3898 - val_loss: 0.4206\n",
            "Epoch 44/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3851 - val_loss: 0.4217\n",
            "Epoch 45/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.4179\n",
            "Epoch 46/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3846 - val_loss: 0.4154\n",
            "Epoch 47/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3762 - val_loss: 0.4163\n",
            "Epoch 48/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3779 - val_loss: 0.4139\n",
            "Epoch 49/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3796 - val_loss: 0.4145\n",
            "Epoch 50/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3866 - val_loss: 0.4127\n",
            "Epoch 51/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3714 - val_loss: 0.4132\n",
            "Epoch 52/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3708 - val_loss: 0.4131\n",
            "Epoch 53/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3812 - val_loss: 0.4100\n",
            "Epoch 54/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3773 - val_loss: 0.4087\n",
            "Epoch 55/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3767 - val_loss: 0.4082\n",
            "Epoch 56/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3730 - val_loss: 0.4069\n",
            "Epoch 57/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3861 - val_loss: 0.4062\n",
            "Epoch 58/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3736 - val_loss: 0.4066\n",
            "Epoch 59/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3570 - val_loss: 0.4062\n",
            "Epoch 60/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3723 - val_loss: 0.4040\n",
            "Epoch 61/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3746 - val_loss: 0.4046\n",
            "Epoch 62/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3858 - val_loss: 0.4028\n",
            "Epoch 63/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3734 - val_loss: 0.4010\n",
            "Epoch 64/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3673 - val_loss: 0.4006\n",
            "Epoch 65/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3633 - val_loss: 0.4000\n",
            "Epoch 66/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3652 - val_loss: 0.3989\n",
            "Epoch 67/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3605 - val_loss: 0.3979\n",
            "Epoch 68/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3812 - val_loss: 0.3974\n",
            "Epoch 69/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3736 - val_loss: 0.3975\n",
            "Epoch 70/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3663 - val_loss: 0.3963\n",
            "Epoch 71/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3466 - val_loss: 0.3960\n",
            "Epoch 72/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3727 - val_loss: 0.3949\n",
            "Epoch 73/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3699 - val_loss: 0.3948\n",
            "Epoch 74/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3546 - val_loss: 0.3939\n",
            "Epoch 75/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3523 - val_loss: 0.3923\n",
            "Epoch 76/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3595 - val_loss: 0.3922\n",
            "Epoch 77/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3659 - val_loss: 0.3909\n",
            "Epoch 78/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3729 - val_loss: 0.3921\n",
            "Epoch 79/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3711 - val_loss: 0.3892\n",
            "Epoch 80/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3572 - val_loss: 0.3914\n",
            "Epoch 81/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3599 - val_loss: 0.3900\n",
            "Epoch 82/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3692 - val_loss: 0.3885\n",
            "Epoch 83/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3681 - val_loss: 0.3861\n",
            "Epoch 84/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3521 - val_loss: 0.3883\n",
            "Epoch 85/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3666 - val_loss: 0.3851\n",
            "Epoch 86/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3512 - val_loss: 0.3877\n",
            "Epoch 87/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3557 - val_loss: 0.3839\n",
            "Epoch 88/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3598 - val_loss: 0.3836\n",
            "Epoch 89/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3493 - val_loss: 0.3830\n",
            "Epoch 90/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3673 - val_loss: 0.3833\n",
            "Epoch 91/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3420 - val_loss: 0.3814\n",
            "Epoch 92/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3517 - val_loss: 0.3820\n",
            "Epoch 93/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3596 - val_loss: 0.3827\n",
            "Epoch 94/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3388 - val_loss: 0.3832\n",
            "Epoch 95/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3609 - val_loss: 0.3807\n",
            "Epoch 96/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3417 - val_loss: 0.3812\n",
            "Epoch 97/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3452 - val_loss: 0.3801\n",
            "Epoch 98/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3603 - val_loss: 0.3788\n",
            "Epoch 99/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3463 - val_loss: 0.3795\n",
            "Epoch 100/100\n",
            "363/363 [==============================] - 0s 1ms/step - loss: 0.3460 - val_loss: 0.3808\n",
            "162/162 [==============================] - 0s 759us/step - loss: 0.3648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BONhaQgRFr4s"
      },
      "source": [
        "We don't want to train and evaluate just a single model; we want to train hundreds of variants and see which one performs best on validation set. Since there are many hyperparameters, it's preferable to use a randomized search rather than grid search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_1BYFHJFrDJ"
      },
      "source": [
        "from scipy.stats import reciprocal\r\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgW8-oBmGERo",
        "outputId": "605c3453-d6bf-4328-86ad-34505e356581"
      },
      "source": [
        "# Note: There's current a bug with Keras/Sklearn's \"KerasRegressor\"\r\n",
        "# Workaround: all param values have to be lists\r\n",
        "\r\n",
        "param_distribs = {\r\n",
        "    \"n_hidden\": [0, 1, 2, 3],\r\n",
        "    \"n_neurons\": np.arange(1, 100).tolist(),\r\n",
        "    \"learning_rate\": np.array(reciprocal.rvs(3e-4, 3e-2, size=10)).tolist()         # Extract 10 values out of PDF\r\n",
        "}\r\n",
        "\r\n",
        "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\r\n",
        "rnd_search_cv.fit(X_train, y_train, epochs=100, verbose=0,                          # Suppressed epoch output printing\r\n",
        "                  validation_data=(X_valid, y_valid),\r\n",
        "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])                \r\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "121/121 [==============================] - 0s 820us/step - loss: 0.3650\n",
            "121/121 [==============================] - 0s 807us/step - loss: 0.3521\n",
            "121/121 [==============================] - 0s 802us/step - loss: 0.3401\n",
            "121/121 [==============================] - 0s 752us/step - loss: 0.3370\n",
            "121/121 [==============================] - 0s 806us/step - loss: 0.3357\n",
            "121/121 [==============================] - 0s 799us/step - loss: 0.3285\n",
            "121/121 [==============================] - 0s 746us/step - loss: 0.5312\n",
            "121/121 [==============================] - 0s 732us/step - loss: 0.5322\n",
            "121/121 [==============================] - 0s 710us/step - loss: 0.5451\n",
            "121/121 [==============================] - 0s 839us/step - loss: nan\n",
            "121/121 [==============================] - 0s 828us/step - loss: 0.2844\n",
            "121/121 [==============================] - 0s 750us/step - loss: nan\n",
            "121/121 [==============================] - 0s 710us/step - loss: 0.4668\n",
            "121/121 [==============================] - 0s 739us/step - loss: 0.4647\n",
            "121/121 [==============================] - 0s 740us/step - loss: 0.5126\n",
            "121/121 [==============================] - 0s 789us/step - loss: 0.2966\n",
            "121/121 [==============================] - 0s 769us/step - loss: 0.3218\n",
            "121/121 [==============================] - 0s 790us/step - loss: 0.3294\n",
            "121/121 [==============================] - 0s 729us/step - loss: 0.4169\n",
            "121/121 [==============================] - 0s 722us/step - loss: 0.4088\n",
            "121/121 [==============================] - 0s 734us/step - loss: 0.3979\n",
            "121/121 [==============================] - 0s 737us/step - loss: 0.4481\n",
            "121/121 [==============================] - 0s 774us/step - loss: 0.4417\n",
            "121/121 [==============================] - 0s 773us/step - loss: 0.4206\n",
            "121/121 [==============================] - 0s 803us/step - loss: 0.3460\n",
            "121/121 [==============================] - 0s 729us/step - loss: 0.3562\n",
            "121/121 [==============================] - 0s 751us/step - loss: 0.3312\n",
            "121/121 [==============================] - 0s 746us/step - loss: 0.4079\n",
            "121/121 [==============================] - 0s 731us/step - loss: 0.4106\n",
            "121/121 [==============================] - 0s 707us/step - loss: 0.4022\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score=nan,\n",
              "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7ff336d22850>,\n",
              "                   iid='deprecated', n_iter=10, n_jobs=None,\n",
              "                   param_distributions={'learning_rate': [0.006668687734339515,\n",
              "                                                          0.0012262191828130119,\n",
              "                                                          0.0011155759815355039,\n",
              "                                                          0.009630728205057507,\n",
              "                                                          0.00030257500252921563,\n",
              "                                                          0.0009422646877394766,\n",
              "                                                          0.0031135269814296946,\n",
              "                                                          0.0025815956673095487,\n",
              "                                                          0.0016687473230493465,\n",
              "                                                          0.029843441636100025],\n",
              "                                        'n_hidden': [0, 1, 2, 3],\n",
              "                                        'n_neurons': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
              "                                                      10, 11, 12, 13, 14, 15,\n",
              "                                                      16, 17, 18, 19, 20, 21,\n",
              "                                                      22, 23, 24, 25, 26, 27,\n",
              "                                                      28, 29, 30, ...]},\n",
              "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e13r2wG7H8QO"
      },
      "source": [
        "> Note: `RandomizedSearchCV` uses K-fold cross-validation so it does not use `X_valid` and `y_valid`, which are only used for early stopping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFCkqcaJH6mH",
        "outputId": "41f86881-5df5-4dd6-b4d3-36d55aedbaf0"
      },
      "source": [
        "rnd_search_cv.best_params_"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.009630728205057507, 'n_hidden': 3, 'n_neurons': 31}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb2vIftrIOZl",
        "outputId": "41e0fc2e-b949-4ff3-c5fb-5f72d66943c7"
      },
      "source": [
        "rnd_search_cv.best_score_"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.31591692566871643"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pnd1Tp-IQm2",
        "outputId": "20190b36-adae-4de3-a8bf-0f6169c366d7"
      },
      "source": [
        "model = rnd_search_cv.best_estimator_.model\r\n",
        "model"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.engine.sequential.Sequential at 0x7ff32aad7a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZNrhOTQIm_Q"
      },
      "source": [
        "There are many techniques that explore a search space much more efficiently than randomly. Their core idea is simple: when a region of the space turns out to be good, it should be explored more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM24hpLWfO3O"
      },
      "source": [
        "### 10.3.1 Number of Hidden Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlexT7fOJ8Xr"
      },
      "source": [
        "For many problems, you can begin with a single hidden layer and get reasonable results.\r\n",
        "\r\n",
        "But for complex problems, deep networks have a much higher parameter efficiency than shallow ones: they can model complex functions using exponentially fewer neurons than shallow nets, allowing them to reach much better performance with the same amount of training data.  \r\n",
        "=> more layers, less neurons\r\n",
        "\r\n",
        "Example: imagine drawing a forest.\r\n",
        "\r\n",
        "It would take a very long time if you have to draw every tree individually, with every branch and every leaf.\r\n",
        "\r\n",
        "But if you can draw 1 leaf, copy & paste -> draw 1 branch, copy & paste -> draw 1 tree, copy & paste, you'll be done in no time.\r\n",
        "\r\n",
        "Neural networks take advantage of this structure:\r\n",
        "- Lower hidden layers model low-level structures (eg. line segments)\r\n",
        "- Intermediate hidden layers combine low-level structures to model intermediate-level structures (eg. squares, circles)\r\n",
        "- Highest hidden layers and outout layer combine these intermediate structures to model high-level structures (eg. faces)\r\n",
        "\r\n",
        "**Transfer learning** - Initialize the weights and biases of the first few layers from second network to the ones of the first network. The network will not have to learn from scratch all the low-level structures, only the higher-level ones.\r\n",
        "\r\n",
        "**Summary**:\r\n",
        "***\r\n",
        "- You can start with just 1 or 2 hidden layers and the neural network will work just fine.\r\n",
        "- For more complex problems, you can ramp up the number of hidden layers until you start overfitting the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZnqB61efO3P"
      },
      "source": [
        "### 10.3.2 Number of Neurons per Hidden Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFXr_UExNx7G"
      },
      "source": [
        "The number of neurons in the input and output layers is determined by the type of input and output your task requires.\r\n",
        "\r\n",
        "For example, the MNIST task requires 28 x 28 = 784 input neurons and 10 output neurons.\r\n",
        "\r\n",
        "For hidden layers, pick the same number of neurons for all layers, as it performs just as well or even better than pyramid (most at low-level, least at high-level) and there is only 1 hyperparameter to tune.\r\n",
        "\r\n",
        "For best practice, pick a model with more layers and neurons than you actually need, then use early stopping and other regularization techniques to prevent it from overfitting.\r\n",
        "\r\n",
        "> The \"stretch pants\" approch: instead of wasting time looking for pants that perfectly match your size, just use large stretch pants that will shrink down to the right size.\r\n",
        "\r\n",
        "> Note: In general you will get more bang for your buck by increasing the number of layers instead of number of neurons per layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iDxWERRfO3P"
      },
      "source": [
        "### 10.3.3 Learning Rate, Batch Size, and Other Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A16CiAiHQjZN"
      },
      "source": [
        "**Learning rate**:\r\n",
        "- In general, the optimal learning rate is about half the maximum learning rate (ie, the learning rate above which the algorithm diverges).\r\n",
        "- Train the model for a few hundred iterations, starting with a very low learning rate (eg. $10^{-5}$) and gradually increasing to a very large value (eg. $10$).\r\n",
        "- Plot the loss as a function of the learning rate (using a log scale for the learning rate).\r\n",
        "- You should see it dropping at first then shoot back up (learning rate too large).\r\n",
        "- Optimal learning rate will be a bit lower than that point.\r\n",
        "- Then reinitialize the model and train normally using this good learning rate.\r\n",
        "\r\n",
        "**Optimizer**:\r\n",
        "- Choosing a better optimizer than Mini-batch Gradient Descent is quite important.\r\n",
        "\r\n",
        "**Batch size**:\r\n",
        "- One strategy is to try to use a large batch size, using learning rate warmup.\r\n",
        "- If the training is unstable or the final performance is disappointing, then try using a small batch size instead.\r\n",
        "\r\n",
        "**Activation function**:\r\n",
        "- ReLU activation function will be a good default for all hidden layers.\r\n",
        "- For output layer, depends on the task.\r\n",
        "\r\n",
        "**Number of iterations**:\r\n",
        "- In most cases, the number of training iterations do not need to be tweaked.\r\n",
        "- Use early stopping instead."
      ]
    }
  ]
}