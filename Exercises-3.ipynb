{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "54b71c1b326a101c02c54fb24502b2663857f505f7b72a06051ccceba068a4ff"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Chapter 3: Classification Exercises"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "## 1.\n",
    "\n",
    "> Try to build a classifier for the MNIST dataset that achieves over 97% accuracy on the test set.  \n",
    "\n",
    "> Hint: The `KNeighborsClassifier` works quite well for this task; you just need to find good hyperparameter values (try a grid search on the `weights` and `n_neighbor` hyperparameters)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2.\n",
    "\n",
    "> 1. Write a function that can shift an MNIST image in any direction (left, right, up, or down) by one pixel.  \n",
    "\n",
    ">> You can use the `shift()` function from the `scipy.ndimage.interpolation` module. For example, `shift(image, [2, 1], cval=0)` shifts the image two pixels down and one pixel to the right.  \n",
    "\n",
    "> 2. Then, for each image in the training set, create four shifted copies (one per direction) and add them to the training set.  \n",
    "\n",
    "> 3. Finally, train your best model on this expanded training set and measure its accuracy on the test set.  \n",
    "\n",
    "> You should observe that your model performs even better now! This technique of artificially growing the training set is called *data augmentation* or *training set expansion*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.\n",
    "\n",
    "> Tackle the Titanic dataset. A great place to start is on Kaggle."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 4.\n",
    "\n",
    "> Build a spam classifier (a more challenging exercise):  \n",
    "\n",
    "> 1. Download examples of spam and ham from Apache SpamAssassin's public datasets.\n",
    "\n",
    "> 2. Unzip the datasets and familiarize yourself with the data format.\n",
    "\n",
    "> 3. Split the datasets into a training set and a test set.\n",
    "\n",
    "> 4. Write a data preparation pipeline to convert each email into a feature vector.  \n",
    "   - Your preparation pipeline should transform an email into a (sparse) vector that indicates the presence or absence of each possible word.  \n",
    ">> For example, if all emails only ever contain four words, \"Hello,\" \"how,\" \"are,\" \"you,\" then the email \"Hello you Hello Hello you\" would be converted into a vector \\[1, 0, 0, 1] (meaning \\[\"Hello\" is present, \"how\" is absent, \"are\" is absent, \"you\" is present]), or \\[3, 0, 0, 2] if you prefer to count the number of occurrences of each word.\n",
    "   - You may want to add hyperparameters to your preparation pipeline to control whether or not to:\n",
    "   >> strip off email headers, convert each email to lowercase, remove punctuation, replace all URLs with \"URL,\" replace all numbers with \"NUMBER,\" or even perform *stemming* (ie, trim off word endings; there are Python libraries available to do this).\n",
    "5. Finally, try out several classifiers and see if you can build a great spam classifier, with both high recall and high precision."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}